{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet_NLP_quora_kaggle.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3bea31bb078a485eb08b0074737043c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ff32d71ae94548aca61ff7f8e4b5327a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_09f2180111514472b403b5657a6cc50b",
              "IPY_MODEL_a853bf9042b84642b37b6604023bc77b"
            ]
          }
        },
        "ff32d71ae94548aca61ff7f8e4b5327a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09f2180111514472b403b5657a6cc50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8723b0ef292a43f9822fc373e62d3e06",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c5dd20967564e87af104490ad74f888"
          }
        },
        "a853bf9042b84642b37b6604023bc77b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_30704839d7484b7787452ad282f67ce5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [06:05&lt;00:00, 1.19B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b187ce138ae34618807d99e2f4539451"
          }
        },
        "8723b0ef292a43f9822fc373e62d3e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c5dd20967564e87af104490ad74f888": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "30704839d7484b7787452ad282f67ce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b187ce138ae34618807d99e2f4539451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "52288c16a8204513b8118eb143bc2a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_724ae28444bc4c03b79a9387e5183d69",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b30f34f70f914b8892023d6f2b718b45",
              "IPY_MODEL_2d2f13e83e2e463e9a2510ae76a7d41e"
            ]
          }
        },
        "724ae28444bc4c03b79a9387e5183d69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b30f34f70f914b8892023d6f2b718b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ff85d0ae23d74e0dabda0c01eca1965a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6854144dd0b848bda4e8f81968070549"
          }
        },
        "2d2f13e83e2e463e9a2510ae76a7d41e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6bd0ec4d43ff4932a78a85fd3fed8c4a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:23&lt;00:00, 19.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9afd75522b447719ac8a8d1d0c3c5a2"
          }
        },
        "ff85d0ae23d74e0dabda0c01eca1965a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6854144dd0b848bda4e8f81968070549": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6bd0ec4d43ff4932a78a85fd3fed8c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9afd75522b447719ac8a8d1d0c3c5a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQbfKP2OCKN1",
        "colab_type": "text"
      },
      "source": [
        "# **Insincere Question Classification**\n",
        "\n",
        "An existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world.\n",
        "Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n",
        "In this competition, Kagglers will develop models that identify and flag insincere questions. To date, Quora has employed both machine learning and manual review to address this problem. With your help, they can develop more scalable methods to detect toxic and misleading content.\n",
        "Here's your chance to combat online trolls at scale. Help Quora uphold their policy of “Be Nice, Be Respectful” and continue to be a place for sharing and growing the world’s knowledge.\n",
        "\n",
        "For each qid in the test set, you must predict whether the corresponding question_text is insincere (1) or not (0). Predictions should only be the integers 0 or 1. The file should contain a header and have the following format:\n",
        "\n",
        "```\n",
        "qid, prediction\n",
        "0000163e3ea7c7a74cd7,0\n",
        "00002bd4fb5d505b9161,0\n",
        "00007756b4a147d2b0b3,0\n",
        "```\n",
        "\n",
        "An insincere question is defined as a question intended to make a statement rather than look for helpful answers. Some characteristics that can signify that a question is insincere:\n",
        "\n",
        "Has a non-neutral tone \n",
        "*   Has an exaggerated tone to underscore a point about a group of people\n",
        "*   Is rhetorical and meant to imply a statement about a group of people\n",
        "\n",
        "Is disparaging or inflammatory \n",
        "*   Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype\n",
        "*   Makes disparaging attacks/insults against a specific person or group of people \n",
        "*   Based on an outlandish premise about a group of people \n",
        "*   Disparages against a characteristic that is not fixable and not measurable\n",
        "\n",
        "Isn't grounded in reality \n",
        "*   Based on false information, or contains absurd assumptions\n",
        "*   Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek genuine answers\n",
        "*   The training data includes the question that was asked, and whether it was identified as insincere (target = 1). The ground-truth labels contain some amount of noise: they are not guaranteed to be perfect.\n",
        "\n",
        "Note that the distribution of questions in the dataset should not be taken to be representative of the distribution of questions asked on Quora. This is, in part, because of the combination of sampling procedures and sanitization measures that have been applied to the final dataset.\n",
        "\n",
        "**Files description**\n",
        "\n",
        "train.csv - the training set\n",
        "\n",
        "test.csv - the test set\n",
        "\n",
        "sample_submission.csv - A sample submission in the correct format\n",
        "\n",
        "enbeddings/ - (see below)\n",
        "\n",
        "**Data Field** \n",
        "\n",
        "qid - unique question identifier\n",
        "\n",
        "question_text - Quora question text\n",
        "\n",
        "target - a question labeled \"insincere\" has a value of 1, otherwise 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWrQTomSrKYd",
        "colab_type": "text"
      },
      "source": [
        "## Data importation\n",
        "\n",
        "Below we import the data from Google Drive, this step requires a connection to a google drive containing the files. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pv2diMbML3-",
        "colab_type": "code",
        "outputId": "ec4a645b-073c-4857-81ad-1dcb7ca47932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        }
      },
      "source": [
        "# How to import the data stored in drive:\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from sklearn import model_selection\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import random as rd\n",
        "import time\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive_pers = GoogleDrive(gauth)\n",
        "\n",
        "# Extraction of train data\n",
        "link_train = 'https://drive.google.com/open?id=15fKt75l3ifY-v_VuGfQ2t9gLtLpobDqy' \n",
        "id_train = link_train.split('=')[-1]\n",
        "downloaded = drive_pers.CreateFile({'id':id_train}) \n",
        "downloaded.GetContentFile('train.csv')  \n",
        "df_train = pd.read_csv('train.csv')\n",
        "print(\"Train data:\")\n",
        "print(df_train)\n",
        "print()\n",
        "\n",
        "# Extraction of test data\n",
        "link_test = 'https://drive.google.com/open?id=1JFqNLXe8evmCF9KULkRPqjJJa-bf7iAc'\n",
        "id_test = link_test.split('=')[-1]\n",
        "downloaded = drive_pers.CreateFile({'id':id_test}) \n",
        "downloaded.GetContentFile('test.csv')  \n",
        "test_df = pd.read_csv('test.csv')\n",
        "print(\"Test data:\")\n",
        "print(test_df)\n",
        "\n",
        "train_df, validate_df = model_selection.train_test_split(df_train, test_size=0.1)\n",
        "del(df_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data:\n",
            "                          qid  ... target\n",
            "0        00002165364db923c7e6  ...      0\n",
            "1        000032939017120e6e44  ...      0\n",
            "2        0000412ca6e4628ce2cf  ...      0\n",
            "3        000042bf85aa498cd78e  ...      0\n",
            "4        0000455dfa3e01eae3af  ...      0\n",
            "...                       ...  ...    ...\n",
            "1306117  ffffcc4e2331aaf1e41e  ...      0\n",
            "1306118  ffffd431801e5a2f4861  ...      0\n",
            "1306119  ffffd48fb36b63db010c  ...      0\n",
            "1306120  ffffec519fa37cf60c78  ...      0\n",
            "1306121  ffffed09fedb5088744a  ...      0\n",
            "\n",
            "[1306122 rows x 3 columns]\n",
            "\n",
            "Test data:\n",
            "                         qid                                      question_text\n",
            "0       0000163e3ea7c7a74cd7  Why do so many women become so rude and arroga...\n",
            "1       00002bd4fb5d505b9161  When should I apply for RV college of engineer...\n",
            "2       00007756b4a147d2b0b3  What is it really like to be a nurse practitio...\n",
            "3       000086e4b7e1c7146103                             Who are entrepreneurs?\n",
            "4       0000c4c3fbe8785a3090   Is education really making good people nowadays?\n",
            "...                      ...                                                ...\n",
            "375801  ffff7fa746bd6d6197a9  How many countries listed in gold import in in...\n",
            "375802  ffffa1be31c43046ab6b  Is there an alternative to dresses on formal p...\n",
            "375803  ffffae173b6ca6bfa563  Where I can find best friendship quotes in Tel...\n",
            "375804  ffffb1f7f1a008620287        What are the causes of refraction of light?\n",
            "375805  fffff85473f4699474b0  Climate change is a worrying topic. How much t...\n",
            "\n",
            "[375806 rows x 2 columns]\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i81PlMWz6zMv",
        "colab_type": "code",
        "outputId": "fb133865-c7c2-442b-b78b-2206aa10a7b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "print(train_df.values[0])\n",
        "print(test_df.values[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['6c4d8021d53467487e69'\n",
            " 'What can be the explanation of \"the blue danube\" by Johann Strauss?' 0]\n",
            "['0000163e3ea7c7a74cd7'\n",
            " 'Why do so many women become so rude and arrogant when they get just a little bit of wealth and power?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZpOIbQBfx1x",
        "colab_type": "text"
      },
      "source": [
        "## Embeddings\n",
        "\n",
        "We have to turn this string sentence into something that could be an input for our Neural Network: a vector.\n",
        "For this, the Kaggle website suggests different embeddings: word2vec, Glove, wikinews. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUECTg9xYwTY",
        "colab_type": "text"
      },
      "source": [
        "### Glove\n",
        "Let's start with **Glove** (+ than 2M words, embeddings vectors of dimension 300, description [here](https://nlp.stanford.edu/projects/glove/)):\n",
        "\n",
        "*The following code, doesn't work on colab, it is just here as an appendix*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHA_CoZohOey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dict_glove():\n",
        "  '''\n",
        "  Creates dictionary by importing data from the glove.840B.300d.txt file. This \n",
        "  function is created especially for this file. \n",
        "  OUTPUT:\n",
        "  - words_dict : dictionary key = string of the word, value = array of the embeddings\n",
        "  corresponding to this word.\n",
        "  RUNNING TIME:\n",
        "  Very long: around 10 minutes to import the whole file.\n",
        "  '''\n",
        "  words_dict = {}\n",
        "#   # Extraction of embeddings dictionnary\n",
        "#   link_dict = \"https://drive.google.com/open?id=14HSnOa4OnH2J-unsFwBOGJQjzE1pk7nO\"\n",
        "#   id_dict = link_dict.split('=')[-1]\n",
        "#   downloaded = drive_pers.CreateFile({'id':id_dict}) \n",
        "#   downloaded.GetContentFile('glove.840B.300d.txt')  \n",
        "  f = open('glove.840B.300d.txt', 'r', encoding='utf8')\n",
        "  for line in tqdm(f):\n",
        "    # try:\n",
        "    values = line.split(\" \")\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    words_dict[word] = coefs\n",
        "    # except:\n",
        "      # print(f.readline())\n",
        "      # continue\n",
        "  f.close()\n",
        "  return words_dict\n",
        "\n",
        "# words_dict = create_dict_glove()\n",
        "# np.save( \"words_dict_glove.npy\", words_dict, allow_pickle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HYZj-SmYbzT",
        "colab_type": "text"
      },
      "source": [
        "We import the dictionary with numpy.load to go faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W00Z9nvM8GQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 'words_dict' not in globals():\n",
        "  # The dictionnary is already created in the drive:\n",
        "  link_dict = 'https://drive.google.com/open?id=1y2r7uB2HfO_it50EJfYJtAJqAs-4zqqO'\n",
        "  id_dict = link_dict.split('=')[-1]\n",
        "  downloaded = drive_pers.CreateFile({'id':id_dict}) \n",
        "  downloaded.GetContentFile('words_dict_glove.npy')  \n",
        "  words_dict = np.load('words_dict_glove.npy', allow_pickle = True).item()\n",
        "  words_dict[\" \"] = np.zeros(300, dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqSwa34GVTqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(len(words_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LbEW2aru8HN",
        "colab_type": "text"
      },
      "source": [
        "### Wiki-News\n",
        "We can also try to use the **Wiki_news** embedding (1 million words vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).) \n",
        "\n",
        "As a comparision, Oxford dictionnary contains 171,476 words.\n",
        "\n",
        "*The following code cannot be executed on colab, it is here just an appendix.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8SzhqPIu7lV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dict_wiki(file_name):\n",
        "  '''\n",
        "  Creates dictionary by importing data from the wiki-news-300d-1M.vec file. This \n",
        "  function is created especially for this file. \n",
        "  INPUT:\n",
        "  - file_name : string of the path where we can find the wiki-news file\n",
        "  OUTPUT:\n",
        "  - my_dict : dictionary key = string of the word, value = array of the embeddings\n",
        "  corresponding to this word.\n",
        "  RUNNING TIME:\n",
        "  Very long: around 10 minutes to import the whole file.\n",
        "  '''\n",
        "  my_file = open(file_name,'r', encoding=\"utf8\")\n",
        "  print(\"Extracting dictionnary from 'wiki-news-300d-1M.vec' file...\")\n",
        "  nb_lines = eval(my_file.readline().split(' ')[0])\n",
        "  my_dict = {}\n",
        "  for i in range(nb_lines):\n",
        "    if (i%50000==0): print('{:>7}'.format(i), '|', '{:<7}'.format(nb_lines))\n",
        "    line = my_file.readline()\n",
        "    list_elem = line.split(\" \")\n",
        "    key = list_elem[0]\n",
        "    value = []\n",
        "    list_elem = list_elem[1:]\n",
        "    for elem in list_elem:\n",
        "      elem = eval(elem)\n",
        "      value.append(elem)\n",
        "    my_dict[key] = np.array(value, dtype=np.float16)\n",
        "  print(\"Extraction finished.\")  \n",
        "  my_file.close()\n",
        "  return my_dict\n",
        "\n",
        "# words_dict = create_dict_wiki('/content/drive/My Drive/Cours/Kaggle/wiki-news-300d-1M.vec')\n",
        "# np.save( \"words_dict.npy\", words_dict, allow_pickle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1dalMeE3DS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#if 'words_dict' not in globals():\n",
        "#  # The dictionnary is already created in the drive:\n",
        "#  link_dict = 'https://drive.google.com/open?id=1k_uAS5IWudMPhJF6WzdLAEIlZsD0FVDG'\n",
        "#  id_dict = link_dict.split('=')[-1]\n",
        "#  downloaded = drive_pers.CreateFile({'id':id_dict}) \n",
        "#  downloaded.GetContentFile('words_dict_wiki.npy')  \n",
        "#  words_dict = np.load('words_dict_wiki.npy', allow_pickle = True).item()\n",
        "#  words_dict[\" \"] = np.zeros(300, dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J2cR17FnId3",
        "colab_type": "text"
      },
      "source": [
        "### Word2vec\n",
        "We can also use **word2vec**. (The following code is taken from Kaggle topic section)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuJFwBTOnmG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from gensim.models import KeyedVectors\n",
        "\n",
        "#news_path = '/content/drive/My Drive/Cours/Kaggle/GoogleNews-vectors-negative300.bin'\n",
        "#embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)\n",
        "#print(type(embeddings_index))\n",
        "#words_dict = {}\n",
        "#for word in embeddings_index.vocab:\n",
        "#  words_dict[word] = embeddings_index[word]\n",
        "#np.save(\"words_dict.npy\", words_dict, allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT8yOc9rIGSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#if 'words_dict' not in globals():\n",
        "#  # The dictionnary is already created in the drive:\n",
        "#  link_dict = 'https://drive.google.com/open?id=1GcEcGNODNQmuh3bdDFI8vXiCnLHR-Oib'\n",
        "#  id_dict = link_dict.split('=')[-1]\n",
        "#  downloaded = drive_pers.CreateFile({'id':id_dict}) \n",
        "#  downloaded.GetContentFile('words_dict_google.npy')  \n",
        "#  words_dict = np.load('words_dict_google.npy', allow_pickle = True).item()\n",
        "#  words_dict[\" \"] = np.zeros(300, dtype=np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8VyGIpJGw1_",
        "colab_type": "text"
      },
      "source": [
        "## Text normalization\n",
        "\n",
        "We have developped a function that transform the text a little so that it is \n",
        "easier to exploit. It merely isolates unusual characters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEFrcgl4HNAn",
        "colab_type": "code",
        "outputId": "31860344-03ea-4901-95e8-effe9e95e66c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "dict_replace = {\"€\": \"euros\", \n",
        "\"$\": \"dollar\", \n",
        "\"£\": \"pound\", \n",
        "\"aren't\": \"are not\", \n",
        "\"can't\" : \"cannot\",\n",
        "\"couldn't\" : \"could not\",\n",
        "\"didn't\" : \"did not\",\n",
        "\"doesn't\" : \"does not\", \n",
        "\"don't\" : \"do not\", \n",
        "\"hadn't\" : \"had not\", \n",
        "\"hasn't\" : \"has not\", \n",
        "\"haven't\" : \"have not\", \n",
        "\"he'd\" : \"he would\", \n",
        "\"he'll\" : \"he will\", \n",
        "\"he's\" : \"he is\", \n",
        "\"i'd\" : \"I would\", \n",
        "\"i'll\" : \"I will\", \n",
        "\"i'm\" : \"I am\", \n",
        "\"isn't\" : \"is not\", \n",
        "\"it's\" : \"it is\", \n",
        "\"it'll\": \"it will\", \n",
        "\"i've\" : \"I have\", \n",
        "\"let's\" : \"let us\", \n",
        "\"mightn't\" : \"might not\", \n",
        "\"mustn't\" : \"must not\", \n",
        "\"shan't\" : \"shall not\", \n",
        "\"she'd\" : \"she would\", \n",
        "\"she'll\" : \"she will\", \n",
        "\"she's\" : \"she is\", \n",
        "\"shouldn't\" : \"should not\",\n",
        "\"that's\" : \"that is\", \n",
        "\"there's\" : \"there is\", \n",
        "\"they'd\" : \"they would\", \n",
        "\"they'll\" : \"they will\", \n",
        "\"they're\" : \"they are\", \n",
        "\"they've\" : \"they have\", \n",
        "\"we'd\" : \"we would\", \n",
        "\"we're\" : \"we are\", \n",
        "\"weren't\" : \"were not\", \n",
        "\"we've\" : \"we have\", \n",
        "\"what'll\" : \"what will\", \n",
        "\"what're\" : \"what are\", \n",
        "\"what's\" : \"what is\", \n",
        "\"what've\" : \"what have\", \n",
        "\"where's\" : \"where is\", \n",
        "\"who'd\" : \"who would\", \n",
        "\"who'll\" : \"who will\", \n",
        "\"who're\" : \"who are\", \n",
        "\"who's\" : \"who is\", \n",
        "\"who've\" : \"who have\", \n",
        "\"won't\" : \"will not\", \n",
        "\"wouldn't\" : \"would not\", \n",
        "\"you'd\" : \"you would\", \n",
        "\"you'll\" : \"you will\", \n",
        "\"you're\" : \"you are\", \n",
        "\"you've\" : \"you have\", \n",
        "\"'re\" : \" are\", \n",
        "\"wasn't\" : \"was not\", \n",
        "\"we'll\" : \"will\", \n",
        "\"didn't\" : \"did not\", \n",
        "\"tryin'\" : \"trying\", \n",
        "\"gey\" : \"gay\", \n",
        "\"fck\" : \"fuck\", \n",
        "\"gayyy\" :\"gay\", \n",
        "\"mem\" : \"mother\", \n",
        "\"dont\" :\"do not\", \n",
        "\"wont\" : \"would not\", \n",
        "\"cant\" : \"could not\",\n",
        "'colour' : 'color',\n",
        "'centre' : 'center',\n",
        "'favourite' : 'favorite', \n",
        "'travelling' : 'traveling', \n",
        "'counselling' : 'counseling', \n",
        "'theatre' : 'theater', \n",
        "'cancelled' : 'canceled', \n",
        "'labour' : 'labor', \n",
        "'organisation' : 'organization',\n",
        "'wwii' : 'world war 2',\n",
        "'citicise' : 'criticize',\n",
        "'youtu ' : 'youtube',\n",
        "'yt': 'youtube',\n",
        "'fb': 'facebook', \n",
        "'Qoura' : 'Quora',\n",
        "'sallary' : 'salary',\n",
        "'Whta' : 'What',\n",
        "'narcisist' : 'narcissist',\n",
        "'howdo' : 'how do',\n",
        "'whatare' : 'what are',\n",
        "'howcan' : 'how can',\n",
        "'howmuch' : 'how much',\n",
        "'howmany' : 'how many',\n",
        "'whydo' : 'why do',\n",
        "'doI' : 'do I',\n",
        "'theBest' : 'the best',\n",
        "'howdoes' : 'how does',\n",
        "'mastrubation' : 'masturbation',\n",
        "'mastrubate' : 'masturbate',\n",
        "\"mastrubating\" : 'masturbating',\n",
        "'pennis' : 'penis',\n",
        "'Etherium' : 'Ethereum',\n",
        "'narcissit' : 'narcissist',\n",
        "'bigdata' : 'big data',\n",
        "'2k17' : '2017',\n",
        "'2k18' : '2018',\n",
        "'qouta': 'quota',\n",
        "'exboyfriend' : 'ex boyfriend',\n",
        "'airhostess' : 'air hostess',\n",
        "\"whst\" : 'what',\n",
        "'watsapp' : 'whatsapp',\n",
        "'demonitisation' : 'demonetization',\n",
        "'demonitization' : 'demonetization',\n",
        "'demonetisation' : 'demonetization',\n",
        "\"f*ck\" : \"fuck\",\n",
        "\"f**k\" : \"fuck\",\n",
        "\"fuuck\" : \"fuck\",\n",
        "\"d*ck\" : \"dick\",\n",
        "\"c*ck\" : \"cock\",\n",
        "\"b*tch\" : \"bitch\",\n",
        "\"b**ch\" : \"bitch\",\n",
        "\"sh*t\" : \"shit\",\n",
        "\"d*ckhead\" : \"dickhead\"\n",
        "}\n",
        "\n",
        "contraction_mapping = {\"ain't\": \"is not\", \n",
        "                       \"aren't\": \"are not\",\n",
        "                       \"can't\": \"cannot\", \n",
        "                       \"'cause\": \"because\", \n",
        "                       \"could've\": \"could have\", \n",
        "                       \"couldn't\": \"could not\", \n",
        "                       \"didn't\": \"did not\",  \n",
        "                       \"doesn't\": \"does not\", \n",
        "                       \"don't\": \"do not\", \n",
        "                       \"hadn't\": \"had not\", \n",
        "                       \"hasn't\": \"has not\", \n",
        "                       \"haven't\": \"have not\",\n",
        "                       \"he'd\": \"he would\",\n",
        "                       \"he'll\": \"he will\", \n",
        "                       \"he's\": \"he is\", \n",
        "                       \"how'd\": \"how did\",\n",
        "                       \"how'd'y\": \"how do you\", \n",
        "                       \"how'll\": \"how will\", \n",
        "                       \"how's\": \"how is\",  \n",
        "                       \"I'd\": \"I would\", \n",
        "                       \"I'd've\": \"I would have\",\n",
        "                       \"I'll\": \"I will\", \n",
        "                       \"I'll've\": \"I will have\",\n",
        "                       \"I'm\": \"I am\", \n",
        "                       \"I've\": \"I have\", \n",
        "                       \"i'd\": \"i would\",\n",
        "                       \"i'd've\": \"i would have\",\n",
        "                       \"i'll\": \"i will\", \n",
        "                       \"i'll've\": \"i will have\",\n",
        "                       \"i'm\": \"i am\", \n",
        "                       \"i've\": \"i have\", \n",
        "                       \"isn't\": \"is not\", \n",
        "                       \"it'd\": \"it would\",\n",
        "                       \"it'd've\": \"it would have\", \n",
        "                       \"it'll\": \"it will\",\n",
        "                       \"it'll've\": \"it will have\",\n",
        "                       \"it's\": \"it is\",\n",
        "                       \"let's\": \"let us\",\n",
        "                       \"ma'am\": \"madam\",\n",
        "                       \"mayn't\": \"may not\",\n",
        "                       \"might've\": \"might have\",\n",
        "                       \"mightn't\": \"might not\",\n",
        "                       \"mightn't've\": \"might not have\",\n",
        "                       \"must've\": \"must have\",\n",
        "                       \"mustn't\": \"must not\",\n",
        "                       \"mustn't've\": \"must not have\",\n",
        "                       \"needn't\": \"need not\",\n",
        "                       \"needn't've\": \"need not have\",\n",
        "                       \"o'clock\": \"of the clock\",\n",
        "                       \"oughtn't\": \"ought not\",\n",
        "                       \"oughtn't've\": \"ought not have\",\n",
        "                       \"shan't\": \"shall not\",\n",
        "                       \"sha'n't\": \"shall not\",\n",
        "                       \"shan't've\": \"shall not have\",\n",
        "                       \"she'd\": \"she would\",\n",
        "                       \"she'd've\": \"she would have\", \n",
        "                       \"she'll\": \"she will\",\n",
        "                       \"she'll've\": \"she will have\",\n",
        "                       \"she's\": \"she is\",\n",
        "                       \"should've\": \"should have\",\n",
        "                       \"shouldn't\": \"should not\",\n",
        "                       \"shouldn't've\": \"should not have\",\n",
        "                       \"so've\": \"so have\",\n",
        "                       \"so's\": \"so as\",\n",
        "                       \"this's\": \"this is\",\n",
        "                       \"that'd\": \"that would\",\n",
        "                       \"that'd've\": \"that would have\", \n",
        "                       \"that's\": \"that is\",\n",
        "                       \"there'd\": \"there would\",\n",
        "                       \"there'd've\": \"there would have\",\n",
        "                       \"there's\": \"there is\", \n",
        "                       \"here's\": \"here is\",\n",
        "                       \"they'd\": \"they would\",\n",
        "                       \"they'd've\": \"they would have\", \n",
        "                       \"they'll\": \"they will\", \n",
        "                       \"they'll've\": \"they will have\", \n",
        "                       \"they're\": \"they are\", \n",
        "                       \"they've\": \"they have\", \n",
        "                       \"to've\": \"to have\", \n",
        "                       \"wasn't\": \"was not\", \n",
        "                       \"we'd\": \"we would\",\n",
        "                       \"we'd've\": \"we would have\",\n",
        "                       \"we'll\": \"we will\",\n",
        "                       \"we'll've\": \"we will have\", \n",
        "                       \"we're\": \"we are\", \n",
        "                       \"we've\": \"we have\", \n",
        "                       \"weren't\": \"were not\", \n",
        "                       \"what'll\": \"what will\", \n",
        "                       \"what'll've\": \"what will have\", \n",
        "                       \"what're\": \"what are\",  \n",
        "                       \"what's\": \"what is\", \n",
        "                       \"what've\": \"what have\", \n",
        "                       \"when's\": \"when is\", \n",
        "                       \"when've\": \"when have\", \n",
        "                       \"where'd\": \"where did\", \n",
        "                       \"where's\": \"where is\", \n",
        "                       \"where've\": \"where have\", \n",
        "                       \"who'll\": \"who will\", \n",
        "                       \"who'll've\": \"who will have\", \n",
        "                       \"who's\": \"who is\", \n",
        "                       \"who've\": \"who have\", \n",
        "                       \"why's\": \"why is\", \n",
        "                       \"why've\": \"why have\", \n",
        "                       \"will've\": \"will have\",\n",
        "                       \"won't\": \"will not\", \n",
        "                       \"won't've\": \"will not have\",\n",
        "                       \"would've\": \"would have\",\n",
        "                       \"wouldn't\": \"would not\",\n",
        "                       \"wouldn't've\": \"would not have\",\n",
        "                       \"y'all\": \"you all\", \n",
        "                       \"y'all'd\": \"you all would\",\n",
        "                       \"y'all'd've\": \"you all would have\",\n",
        "                       \"y'all're\": \"you all are\",\n",
        "                       \"y'all've\": \"you all have\",\n",
        "                       \"you'd\": \"you would\", \n",
        "                       \"you'd've\": \"you would have\",\n",
        "                       \"you'll\": \"you will\", \n",
        "                       \"you'll've\": \"you will have\", \n",
        "                       \"you're\": \"you are\", \n",
        "                       \"you've\": \"you have\" }\n",
        "\n",
        "unusual_characters_list = [',', ';', ':', '!', '?', '.', '/', '(', ')', \n",
        "                           '[', ']', '{', '}', '~', '<', '>', '%', '\"', '’', \n",
        "                           '”','‘','…', \"-\", \"“\", \"^\", '\\\\', \"=\",'$','£',\n",
        "                           \"€\"] #\"'\", \"*\"\n",
        "                           \n",
        "figures_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "regex = re.compile(r'[\\n\\r\\t]')\n",
        "\n",
        "def replace_in_text(text, dict_):\n",
        "  '''\n",
        "  This function replace the words in text that are mentionned in dict_\n",
        "  by there corresponding value.\n",
        "  INPUT:\n",
        "  - text: string\n",
        "  - dict_: dictionary string to string\n",
        "  OUTPUT:\n",
        "  - text_bis: string\n",
        "  RUNNING TIME:\n",
        "  Immediate\n",
        "  '''\n",
        "  text_bis = \"\"\n",
        "  words = text.split(\" \")\n",
        "  for word in words:\n",
        "    if word in dict_.keys():\n",
        "      text_bis += dict_[word] + \" \"\n",
        "    else:\n",
        "      text_bis += word + \" \"\n",
        "  text_bis.replace(\"*\", \" * \")\n",
        "  return text_bis\n",
        "\n",
        "def normalize_text(text, unusual_characters, dict_, no_figure = True):\n",
        "  '''\n",
        "  Transforms the text string by removing every unusual character and by \n",
        "  lowering the characters that are located inside a word.\n",
        "  INPUT:\n",
        "  - text : string corresponding to the word that we want to normalize.\n",
        "  - unusual_characters : is a list of characters that we want to remove \n",
        "  from the text.\n",
        "  - dict_ : dictionary containing the string that needs to be replaced by \n",
        "  another string.\n",
        "  - no_figure : boolean saying if we want numbers to appear in the resulting\n",
        "  text or not.\n",
        "  OUTPUT:\n",
        "  - normalized_text : string of the normalized word.\n",
        "  RUNNING TIME:\n",
        "  Immediate\n",
        "  '''\n",
        "  text = regex.sub(\" \", text)\n",
        "  normalized_text = \"\"\n",
        "  previous_char = \"\"\n",
        "  text = text[0].lower() + text[1:]\n",
        "  first = True\n",
        "  for char in text:\n",
        "    # Number normalization\n",
        "    if char in figures_list:\n",
        "      if no_figure:\n",
        "        if first:\n",
        "          if previous_char != ' ':\n",
        "            normalized_text += \" number\"\n",
        "          else: \n",
        "            normalized_text += \"number\"\n",
        "          first = False\n",
        "    else: \n",
        "      first = True\n",
        "      # Removing maj in the beginning of sentences\n",
        "      if char.isupper():\n",
        "        index = text.find(char)\n",
        "        if text[index - 2] in ['.', '!', '?', '…']:\n",
        "          char = char.lower()\n",
        "      \n",
        "      # Isolating unsual characters\n",
        "      if previous_char not in unusual_characters:\n",
        "        if char not in unusual_characters:\n",
        "          if previous_char != ' ':\n",
        "            normalized_text += char.lower()\n",
        "          else:\n",
        "            if char != ' ': \n",
        "              normalized_text += char\n",
        "        else:\n",
        "          if previous_char != ' ':\n",
        "            normalized_text += ' ' + char\n",
        "          else:\n",
        "            normalized_text += char\n",
        "      else:\n",
        "        if char != ' ':\n",
        "          normalized_text += ' ' + char\n",
        "        else:\n",
        "          normalized_text += char\n",
        "    previous_char = char\n",
        "  normalized_text = replace_in_text(normalized_text, dict_)\n",
        "  return normalized_text\n",
        "\n",
        "# # Test of the normalize_text function:\n",
        "print(\"Here is what the normalization looks like:\")\n",
        "text = \"The sentence Charles said is cumplex, with mistakes and unUsual \\\n",
        "characters like 24/7. Sometimes, it can even be 2 sentences mentioning money or €. But normalizing it, it's doable...\"\n",
        "print(\"Origin text: '\", text, \"'\")\n",
        "normalized_text = normalize_text(text, unusual_characters_list, dict_replace)\n",
        "print(\"Normalized text: '\", normalized_text, \"'\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here is what the normalization looks like:\n",
            "Origin text: ' The sentence Charles said is cumplex, with mistakes and unUsual characters like 24/7. Sometimes, it can even be 2 sentences mentioning money or €. But normalizing it, it's doable... '\n",
            "Normalized text: ' the sentence Charles said is cumplex , with mistakes and unusual characters like number / number . sometimes , it can even be number sentences mentioning money or euros . but normalizing it , it is doable . . .  '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J809BwIDHj2O",
        "colab_type": "text"
      },
      "source": [
        "## A Little Data Analysis\n",
        "\n",
        "Let's see a little bit more what are exactly inside the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsOrocnOHsmb",
        "colab_type": "code",
        "outputId": "aa39dff8-d57d-4025-d5a4-d0ff802d8df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "source": [
        "words = words_dict.keys()\n",
        "values = train_df.values\n",
        "nb_in_dict = 0\n",
        "nb_not_in_dict = 0\n",
        "size_max = 0\n",
        "mean_size = 0\n",
        "nb_examples = 100000\n",
        "nb_of_ones = 0\n",
        "print(\"Checking the proportion of words from the train set in the dictionary...\")\n",
        "dict_frequencies = {}\n",
        "for value in values[:nb_examples]:\n",
        "  if value[2] == 1: nb_of_ones += 1\n",
        "  question = normalize_text(value[1], unusual_characters_list, dict_replace)\n",
        "  list_of_words = question.split(\" \")\n",
        "  size = len(list_of_words) \n",
        "  mean_size += size\n",
        "  if size > size_max: size_max = size\n",
        "  for word in list_of_words:\n",
        "    if len(word) != 0:\n",
        "      if word in words:\n",
        "        nb_in_dict += 1\n",
        "      else:\n",
        "        nb_not_in_dict +=1\n",
        "      if word not in dict_frequencies.keys():\n",
        "        dict_frequencies[word] = 1\n",
        "      else:\n",
        "        dict_frequencies[word] += 1\n",
        "     \n",
        "print(\"Number of words from the train set in the dictionary:\", nb_in_dict)\n",
        "print(\"Number of words from the train set not in the dictionary:\", nb_not_in_dict)\n",
        "print(\"Proportion of words inside the dictionary:\", \n",
        "      nb_in_dict/(nb_in_dict + nb_not_in_dict))\n",
        "print(\"Maximum size of the questions:\", size_max)\n",
        "print(\"Average size of the question:\", mean_size/nb_examples)\n",
        "print(\"Number of disposable questions:\", nb_of_ones, \"Percentage of ones:\", \n",
        "      nb_of_ones/nb_examples)\n",
        "\n",
        "list_words = []\n",
        "list_frequencies = []\n",
        "i = 1\n",
        "for elem in sorted(dict_frequencies.items(), key = \n",
        "             lambda kv:(kv[1], kv[0]), reverse = True):\n",
        "  list_words.append(elem[0])\n",
        "  list_frequencies.append(elem[1])\n",
        "  if i > 200:\n",
        "    break\n",
        "  i += 1\n",
        "print(list_words)\n",
        "index = np.arange(len(list_words[:100]))\n",
        "plt.bar(list_words[:100], list_frequencies[:100])\n",
        "plt.xlabel('Mots', fontsize=5)\n",
        "plt.ylabel(\"Fréquence d'apparition\", fontsize=5)\n",
        "plt.xticks(index, list_words[:100], fontsize=4, rotation=70)\n",
        "plt.title('Les 100 mots les plus fréquents')\n",
        "plt.savefig(\"proportion\", dpi = 400)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the proportion of words from the train set in the dictionary...\n",
            "Number of words from the train set in the dictionary: 1450680\n",
            "Number of words from the train set not in the dictionary: 15360\n",
            "Proportion of words inside the dictionary: 0.9895227961037898\n",
            "Maximum size of the questions: 150\n",
            "Average size of the question: 15.66138\n",
            "Number of disposable questions: 6212 Percentage of ones: 0.06212\n",
            "['?', 'the', 'what', 'is', 'a', 'to', 'in', 'of', 'how', 'I', 'do', 'and', 'are', ',', 'you', 'for', 'number', 'can', 'why', 'it', '.', 'my', 'that', 'have', 'if', 'on', 'with', 'does', 'or', 'be', 'not', '\"', 'from', 'an', 'your', '-', 'should', 'which', 'would', 'get', 'best', 'when', 'as', '(', ')', 'there', 'people', 'some', 'will', 'who', 'like', 'at', 'about', 'did', 'they', 'by', '/', 'was', 'any', 'we', 'i', 'so', 'good', 'me', 'their', 'one', 'India', 'has', 'after', 'most', 'where', 'but', 'this', 'make', 'think', 'all', 'am', 'more', 'between', 'much', 'many', 'than', 'time', 'he', 'other', 'use', 'life', 'someone', '’', 'way', 'out', 'take', 'know', 'being', 'were', 'up', 'work', 'ever', 'want', 'them', 'find', 'his', 'without', 'could', 'become', 'just', 'into', 'better', 'year', 'Quora', 'feel', 'person', 'only', 'job', 'world', 'go', 'possible', 'mean', 'no', 'been', 'her', 'years', 'need', 'women', 'difference', 'used', 'start', 'Us', 'had', 'Trump', 'first', 'Indian', 'different', 'money', 'still', 'while', 'really', 'long', 'learn', 'old', 'our', 'country', 'even', 'same', 'over', 'she', 'business', 'now', 'school', 'before', 'new', 'give', 's', 'love', 'things', 'help', 'online', 'say', 'during', 'its', 'book', 'see', 'using', 'college', 'because', 'cannot', 'men', 'buy', 'bad', 'stop', 'day', 'sex', 'back', 'anyone', 'change', 'girl', 'made', 'own', 'high', 'live', 'thing', 'then', 'him', 'engineering', 'company', 'student', 'two', 'happen', 'right', 'getting', 'China', 'going', 'such', 'look', 'study', 'having', ':', 'something', 'true', 'numberth', 'come']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac89nOyY0ewT",
        "colab_type": "text"
      },
      "source": [
        "99% of the words contained in the train data set are in the dictionary which makes unecessary the use of a self-trained embedding layer. However, we have to know how to handle the \"unknown\" words, the OOV (Out Of Vocabulary). \n",
        "\n",
        "The maximum size of the questions is usually around 100 words, but the average is around 15.\n",
        "\n",
        "Finally we only have 6% of ones (bad questions). \n",
        "\n",
        "Based on this information, there are several possibilities:\n",
        "In order to train properly, we need to increase this percentage or we need to \n",
        "\n",
        "*   downsampling: extracting all the ones and some zeros to reach a 50/50 proportion.\n",
        "*   oversampling: make copies of some ones to reach a 50/50 proportion.\n",
        "*   cost-sensitive learning: set the loss so that it gives a greater importance to ones than to zeros.\n",
        "*   don't change the proportion.\n",
        "\n",
        "We chose to study two methods: **downsampling** and **not changing the proportion**. You can see both methods by commenting out the corresponding code below.\n",
        "\n",
        "Remark: in both cases we extract only a part on the big training dataset which should have an impact on the final score.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPkbbe1bcJqI",
        "colab_type": "code",
        "outputId": "1d222455-226e-4489-a744-be082853649d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "# Modification of the train set to increase the proportion of ones\n",
        "\n",
        "#train_set = []\n",
        "#nb = 0\n",
        "#for value in train_df.values:\n",
        "#  if value[2] == 1:\n",
        "#    train_set.append(value)\n",
        "#    nb += 1\n",
        "#  else:\n",
        "#    if nb > 0:\n",
        "#      train_set.append(value)\n",
        "#      nb -= 1\n",
        "#train_set = pd.DataFrame(train_set)\n",
        "#train_df_bis, validate_df_bis = model_selection.train_test_split(train_set, test_size=0.1)\n",
        "#print(\"New train set:\", train_df_bis)\n",
        "#print(\"New validation set:\", validate_df_bis)\n",
        "\n",
        "# If we don't want to reequilibrate the proportion:\n",
        "unused_df, train_df_bis = model_selection.train_test_split(train_df, test_size=0.25)\n",
        "del(unused_df)\n",
        "train_df_bis, validate_df_bis = model_selection.train_test_split(train_df_bis, test_size=0.2)\n",
        "print(\"New train set:\", train_df_bis.shape)\n",
        "print(\"New validation set:\", validate_df_bis.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New train set:                          qid  ... target\n",
            "843386  a54990c28a3db2629032  ...      0\n",
            "48745   098ba78bd5888c5d50d4  ...      0\n",
            "709649  8aee206d441f3b23fb6f  ...      0\n",
            "731442  8f3f3eaa5c1946e0a69c  ...      0\n",
            "530540  67deae75d331af8d70ec  ...      0\n",
            "...                      ...  ...    ...\n",
            "422756  52ddd0540d937a29f2c0  ...      0\n",
            "319786  3eac0cb76db5652b900d  ...      1\n",
            "673285  83dc338cfb3dc1092e9c  ...      0\n",
            "592745  74195a2e04c30651f5b9  ...      0\n",
            "353124  45361a9475952a5ede50  ...      0\n",
            "\n",
            "[235102 rows x 3 columns]\n",
            "New validation set:                           qid  ... target\n",
            "656282   808c9dcd10446bcb9b6d  ...      0\n",
            "779414   98ae700637a9d6b74820  ...      0\n",
            "553376   6c6d5ac4b0c3d1c738a6  ...      0\n",
            "414991   5151b208697733125dc8  ...      0\n",
            "32719    06678cfbd1af160b45e3  ...      0\n",
            "...                       ...  ...    ...\n",
            "850561   a6a8c3dafa8eaf533851  ...      0\n",
            "1177724  e6c86e61d7b8897221e2  ...      0\n",
            "436921   559e29acce0383fcec7b  ...      1\n",
            "1032852  ca64b751a178414b447a  ...      0\n",
            "287700   385727d951e9df8dd60e  ...      0\n",
            "\n",
            "[58776 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UURAm6LcFrf",
        "colab_type": "text"
      },
      "source": [
        "## Out Of Vocabulary Handling\n",
        "\n",
        "A good way to do that could be to attribute to this word an embedding depending on the embeddings of the neighbors. Let's imagine that we find this unknown word several times, it could be a great idea to recompute an embedding, the final embedding will then be the average of all the embeddings computed depending on the context, then we had this word to our dictionary.\n",
        "Otherwise, we can just compute an average embedding for this sentence without using a new dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkPKu5Mkz6Y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "common_words_list = [\"the\", \"a\", \"and\", \"for\", \"what\", \"that\", \"as\", \"but\",\n",
        "                     \"if\", \"so\", \"of\", \"how\", \"is\", \"are\",',', ';', ':', '!', \n",
        "                     '?', '.', '/', \"'\", '(', ')','[', ']', '{', '}', '~','\"',\n",
        "                     '’','”','‘','…', \"-\", \"“\", \"^\", '\\\\',]\n",
        "\n",
        "# Computation of the contextual embedding:\n",
        "\n",
        "def create_contextual_embedding(word_index, sentence, dictionary, common_words, \n",
        "                                nb_neighbors = 3):\n",
        "  '''\n",
        "  Computes the embedding vector for a word depending on the context.\n",
        "  The context is defined by the number of neighbors in the sentence.\n",
        "  If one of the words in the work list is not a part of dictionary.keys, \n",
        "  we ignore this word.\n",
        "\n",
        "  INPUTS:\n",
        "  - word_index: is the index of the word in the sentence.\n",
        "  - sentence: is a list of string, each string is a word.\n",
        "  - dictionary: is the dictionary associating embedding to each word.\n",
        "  - common_words: list of usual words that we will not take into account because\n",
        "  they might pollute our final embedding\n",
        "  - nb_neighbors: is an int that defines the number of neighbors that we will \n",
        "  take into account to compute the output.\n",
        "  OUTPUTS:\n",
        "  - embedding_vector: is an array of size 300 corresponding to the estimated \n",
        "  embedding vector.\n",
        "  RUNNING TIME: \n",
        "  Very short\n",
        "  '''\n",
        "\n",
        "  # Extraction of the work list:\n",
        "  size = len(sentence)-1\n",
        "  work_list = []\n",
        "  if 2*nb_neighbors > size:\n",
        "    if word_index < size:\n",
        "      work_list = sentence[:word_index] + sentence[word_index+1:]\n",
        "    else:\n",
        "      work_list = sentence[:word_index]\n",
        "  if word_index - nb_neighbors < 0:\n",
        "    work_list = sentence[:word_index] + sentence[word_index+1:2*nb_neighbors + 1]\n",
        "  else:\n",
        "    if size >= word_index + nb_neighbors:\n",
        "      work_list = sentence[word_index - nb_neighbors:word_index] + \\\n",
        "      sentence[word_index + 1: word_index + nb_neighbors +1 ]\n",
        "    else:\n",
        "      if word_index < size:\n",
        "        work_list = sentence[size-2*nb_neighbors:word_index] + \\\n",
        "        sentence[word_index +1:]\n",
        "      if word_index == size:\n",
        "        work_list = sentence[-2*nb_neighbors-1:-1]\n",
        "\n",
        "  # Extraction of the embeddings:\n",
        "  embedding_list = []\n",
        "  for word in work_list:\n",
        "    if word in dictionary.keys() and word not in common_words:\n",
        "      embedding_list.append(dictionary[word])\n",
        "  embedding_list = np.array(embedding_list)\n",
        "\n",
        "  # Computation of the unknown embedding:\n",
        "  if len(embedding_list) != 0: \n",
        "    return np.mean(embedding_list, axis = 0, dtype = np.float32)\n",
        "  else: return np.zeros(300, dtype=np.float32)\n",
        "  \n",
        "# # Test de create contextual embedding:\n",
        "\n",
        "# maliste = [\"What\", \"are\", \"some\", \"of\", \"the\", \"ways\", \"of\", \"earning\", \"money\",\n",
        "#            \"online\", \"for\", \"a\", \"beginer\"]\n",
        "# embedding = create_contextual_embedding(7, maliste, words_dict, \n",
        "#                                         common_words_list, 2)\n",
        "# print(embedding)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zTcBRU_F_6E",
        "colab_type": "text"
      },
      "source": [
        "Creation of a new dictionary with the 1% unkown words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm788-0OJLGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"Computing the contextual embeddings for the OOV words...\")\n",
        "# new_dict = {}\n",
        "# for value in values:\n",
        "#   question = normalize_text(value[1], unusual_characters_list, dict_replace)\n",
        "#   list_of_words = question.split(\" \")\n",
        "#   size = len(list_of_words)\n",
        "#   for i in range(size):\n",
        "#     word = list_of_words[i]\n",
        "#     if word not in words:\n",
        "#       if word not in new_dict.keys():\n",
        "#         new_dict[word] = [create_contextual_embedding(i, list_of_words, \n",
        "#                                                       words_dict,\n",
        "#                                                       common_words_list,2)]\n",
        "#       else: new_dict[word] = new_dict[word] + \\\n",
        "#         [create_contextual_embedding(i, list_of_words, words_dict, \n",
        "#                                      common_words_list,2)]\n",
        "# new_words = new_dict.keys()\n",
        "\n",
        "# for new_word in new_words:\n",
        "#   new_dict[new_word] = np.mean(np.array(new_dict[new_word], dtype=np.float32), \n",
        "#                                axis = 0, dtype = np.float16)\n",
        "# print(\"Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvp0HCPqRdP-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Displaying the embeddings for the first sentence containing an unknown word \n",
        "# encountered = False\n",
        "# stop = False\n",
        "# for value in values:\n",
        "#   if stop: break\n",
        "#   question = normalize_text(value[1], unusual_characters_list, dict_replace)\n",
        "#   list_of_words = question.split(\" \")\n",
        "#   for word in list_of_words:\n",
        "#     if word not in words:\n",
        "#       encountered = True\n",
        "#       print(list_of_words)\n",
        "#   if encountered:\n",
        "#     for word in list_of_words:\n",
        "#       print(word)\n",
        "#       if word in words:\n",
        "#         print(words_dict[word])\n",
        "#       else:\n",
        "#         print(\"Unknown\", new_dict[word])\n",
        "#     stop = True\n",
        "# for new_word in new_words:\n",
        "#   print(new_word)\n",
        "#   print(new_dict[new_word])\n",
        "#   break         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xwoPr7oi-G6",
        "colab_type": "text"
      },
      "source": [
        "## The RNN Networks\n",
        "\n",
        "Now that we have our dictionary and that we have preprocessed our data, we can go into the heart of the problem: the network.\n",
        "\n",
        "The difficulty of the training comes from the fact that we have to analyse sentences not words. \n",
        "Solutions: \n",
        "\n",
        "\n",
        "1.   Recurrent Neural Network \n",
        "2.   bag of words\n",
        "\n",
        "Here we will study Recurrent Neural Networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ekO5UHECBkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, hidden_dim, layer_dim=1, output_dim=1, dropout_rate=0.4):\n",
        "    super(LSTM, self).__init__()\n",
        "    # For bidirectional : multiply by 2 hidden_dim\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim_1 = hidden_dim\n",
        "    self.hidden_dim_2 = int(hidden_dim/2)\n",
        "    self.output_dim = output_dim\n",
        "    self.layer_dim = layer_dim\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, \n",
        "                        bidirectional = False)#True\n",
        "\n",
        "    self.activation_1 = th.relu\n",
        "    self.linear_1 = nn.Linear(self.hidden_dim_1, self.hidden_dim_2, bias = True)\n",
        "    self.activation_2 = th.relu\n",
        "    self.linear_2 = nn.Linear(self.hidden_dim_2, output_dim, bias = True)\n",
        "\n",
        "  def init_hidden(self, input):\n",
        "    return (th.zeros(self.layer_dim, input.size(0), self.hidden_dim_1),\n",
        "            th.zeros(self.layer_dim, input.size(0), self.hidden_dim_1))\n",
        "\n",
        "  def forward(self, input):\n",
        "    h0 = self.init_hidden(input)\n",
        "    lstm_out, hn = self.lstm(input, h0) # we modify self.hidden \n",
        "    # We take only the last value\n",
        "    lstm_out = self.dropout(lstm_out[:,-1,:])\n",
        "    lstm_out = self.activation_1(lstm_out)\n",
        "    inter = self.linear_1(lstm_out)\n",
        "    inter = self.dropout(inter)\n",
        "    inter = self.activation_2(inter)\n",
        "    out = self.linear_2(inter)#lstm_out[:, -1, :],lstm_out[:, 0, :]\n",
        "    out = th.sigmoid(out)\n",
        "    return out[:,0]\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_rate = 0.4):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim_1 = hidden_dim\n",
        "    self.hidden_dim_2 = int(hidden_dim/2)\n",
        "    self.output_dim = output_dim\n",
        "    self.layer_dim = layer_dim\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, \n",
        "                      nonlinearity='relu', bidirectional = False) #True\n",
        "    self.activation_1 = th.relu\n",
        "    self.linear_1 = nn.Linear(self.hidden_dim_1, self.hidden_dim_2, bias = True)\n",
        "    self.activation_2 = th.relu\n",
        "    self.linear_2 = nn.Linear(self.hidden_dim_2, output_dim, bias = True)\n",
        "    \n",
        "  def forward(self, input):\n",
        "    h0 = Variable(th.zeros(self.layer_dim, input.size(0), self.hidden_dim_1))\n",
        "    # One time step\n",
        "    rnn_out, hn = self.rnn(input, h0)\n",
        "    # We take only the last value\n",
        "    rnn_out = self.dropout(rnn_out[:,-1,:])\n",
        "    rnn_out = self.activation_1(rnn_out)\n",
        "    inter = self.linear_1(rnn_out)\n",
        "    inter = self.dropout(inter)\n",
        "    inter = self.activation_2(inter)\n",
        "    out = self.linear_2(inter)\n",
        "    out = th.sigmoid(out)\n",
        "    return out[:,0]\n",
        "\n",
        "class RNN_Conv(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, sentence_size,\n",
        "               number_channels, conv_kernel, max_kernel, bidirectional=True,\n",
        "               dropout_rate = 0.4):\n",
        "    super(RNN_Conv, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim_1 = hidden_dim\n",
        "    self.hidden_dim_2 = int(hidden_dim/2)\n",
        "    self.output_dim = output_dim\n",
        "    self.layer_dim = layer_dim\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, \n",
        "                      nonlinearity='relu', bidirectional = bidirectional) #True\n",
        "    self.conv = nn.Conv1d(in_channels = sentence_size, out_channels = number_channels,\n",
        "                          kernel_size = conv_kernel, stride = 1, bias = True)\n",
        "    i = (2 if bidirectional else 1)\n",
        "    self.conv_output_size = i*hidden_dim - conv_kernel + 1 \n",
        "    \n",
        "    self.maxp = nn.MaxPool1d(kernel_size = max_kernel)\n",
        "    self.maxp_output_size = number_channels * int(self.conv_output_size/max_kernel)\n",
        "\n",
        "    self.flatten = nn.Flatten(start_dim = 1)\n",
        "    self.activation_1 = th.relu\n",
        "    self.linear_1 = nn.Linear(self.maxp_output_size, self.hidden_dim_2, bias = True)\n",
        "    self.activation_2 = th.relu\n",
        "    self.batch_norm = nn.BatchNorm1d(num_features = self.hidden_dim_2)\n",
        "    self.linear_2 = nn.Linear(self.hidden_dim_2, output_dim, bias = True)\n",
        "    \n",
        "  def forward(self, input):\n",
        "    h0 = Variable(th.zeros(self.layer_dim+1, input.size(0), self.hidden_dim_1))\n",
        "    \n",
        "    # RNN\n",
        "    input = self.dropout(input)\n",
        "    rnn_out, hn = self.rnn(input, h0)\n",
        "    rnn_out = self.dropout(rnn_out)\n",
        "    rnn_out = self.activation_1(rnn_out)\n",
        "    \n",
        "    # Conv\n",
        "    conv_out = self.conv(rnn_out)\n",
        "    conv_out = self.dropout(conv_out)\n",
        "    conv_out = self.activation_1(conv_out)\n",
        "\n",
        "    # Max\n",
        "    conv_out = self.maxp(conv_out)\n",
        "\n",
        "    # Flatten\n",
        "    inter = self.flatten(conv_out)\n",
        "    inter = self.linear_1(inter)\n",
        "    inter = self.dropout(inter)\n",
        "    # inter = self.batch_norm(inter)\n",
        "    inter = self.activation_2(inter)\n",
        "    out = self.linear_2(inter)\n",
        "    out = th.sigmoid(out)\n",
        "    return out[:,0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1QNWspyR5Mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(data, dictionary, nb_words_per_sentence, neighboring_area):\n",
        "  '''\n",
        "  Computes or extract the embedding for each sentence in the data set, after \n",
        "  this, the data can be put into the network directly.\n",
        "\n",
        "  INPUTS:\n",
        "  - data: is the data frame containing for each example : qid, string, target.\n",
        "  - dictionary: is the dictionary associating embedding to each word.\n",
        "  - nb_words_per_sentence: number of embeddings per sentences, if the sentence\n",
        "  is longer, it takes the first nb_words_per_sentences embeddings\n",
        "  - neighboring_area: int, size of the area to create contextual embeddings\n",
        "  OUTPUTS:\n",
        "  - trans_data: is an array of size data.shape[0], nb_words_per_sentences, 300 \n",
        "  corresponding to the sentences in the embedding world.\n",
        "  - trans_targets: same thing but containing the labels\n",
        "  COMPUTATION TIME:\n",
        "  proportional to data.shape[0], quite long.\n",
        "  '''\n",
        "  trans_data = []\n",
        "  trans_targets = []\n",
        "  a = 2\n",
        "  for value in data:\n",
        "    trans_value = []\n",
        "    trans_target = []\n",
        "    question = normalize_text(value[1], unusual_characters_list, dict_replace)\n",
        "    list_of_words = question.split(\" \")\n",
        "    size = len(list_of_words)\n",
        "    #limit the size of the sentence\n",
        "    if size > nb_words_per_sentence:\n",
        "      size = nb_words_per_sentence\n",
        "    for i in range(size):\n",
        "      word = list_of_words[i]\n",
        "      if word not in words:\n",
        "        trans_value += [create_contextual_embedding(i, list_of_words, \n",
        "                                                    dictionary,\n",
        "                                                    common_words_list,\n",
        "                                                    neighboring_area)]\n",
        "      else: \n",
        "        if word.lower() in words:\n",
        "          trans_value += [dictionary[word]]\n",
        "        else: trans_value += [dictionary[word]]\n",
        "    if size < nb_words_per_sentence:\n",
        "      for i in range(nb_words_per_sentence - size):\n",
        "        trans_value.append(np.zeros(300))      \n",
        "    trans_data += [trans_value]\n",
        "    trans_targets += [value[a]]\n",
        "  trans_data = np.array(trans_data, dtype = np.float32)\n",
        "  trans_targets = np.array(trans_targets)\n",
        "  return trans_data, trans_targets   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdKazLIDSAAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(model, validate_df, words_dict, test_size, sentence_size, \n",
        "               neighboring_area, batch_size = 200):\n",
        "  '''\n",
        "  Computes the accuracy on a test set extracted from the validate_df with the \n",
        "  size given by test_size.\n",
        "\n",
        "  INPUTS:\n",
        "  - model: Net in pytorch.\n",
        "  - validate_df: The whole validation dataframe (pandas).\n",
        "  - words_dict: is the dictionary associating embedding to each word.\n",
        "  - test_size: number of sentences that will be tested.\n",
        "  - sentence_size: int, max size of a sentence\n",
        "  - neighboring_area: int, size of the area used to create contextual embeddings \n",
        "  OUTPUTS:\n",
        "  - avg acc: average accuracy of the model on the sample.\n",
        "  - 0 acc: accuracy of the model on the 0 class.\n",
        "  - 1 acc: accuracy of the model on the 1 class.\n",
        "  - recall\n",
        "  - precision\n",
        "  - f1 score\n",
        "  COMPUTATION TIME:\n",
        "  proportional to test_size, quite long and cost in memory\n",
        "  '''\n",
        "  model.eval()\n",
        "  sample = rd.sample(range(0, validate_df.shape[0]), test_size)\n",
        "  val_df = [validate_df[i] for i in sample]\n",
        "\n",
        "  size_of_batches = 5000\n",
        "  const = int(2000/batch_size)\n",
        "  print(\"Testing on {}...\".format(test_size), end=\"\", flush = True)\n",
        "\n",
        "  ticks0 = time.time()\n",
        "\n",
        "  nb_good, nb_good_zero, nb_good_one = 0, 0, 0\n",
        "  nb_total, nb_zero, nb_one = 0, 0, 0\n",
        "  nb_one_pred, nb_zero_pred = 0, 0\n",
        "\n",
        "  for i in range(test_size//size_of_batches):\n",
        "    features_test, targets_test = process_data(val_df[i*size_of_batches:(i+1)*size_of_batches],\n",
        "                                              words_dict, sentence_size, \n",
        "                                              neighboring_area)\n",
        "    features_test = th.from_numpy(features_test)\n",
        "    targets_test = th.from_numpy(targets_test)\n",
        "\n",
        "    test = th.utils.data.TensorDataset(features_test, targets_test)\n",
        "    test_loader = th.utils.data.DataLoader(test, batch_size = batch_size, \n",
        "                                              shuffle = False)\n",
        "\n",
        "    for i, (sentences, labels) in enumerate(test_loader):\n",
        "      outputs = model(sentences)\n",
        "      for i in range(outputs.shape[0]):\n",
        "        output = outputs[i].item()\n",
        "        label = labels[i].item()\n",
        "        if label == 1:\n",
        "          nb_one += 1\n",
        "        else:\n",
        "          nb_zero += 1\n",
        "\n",
        "        if output > 0.5:\n",
        "          output = 1\n",
        "          nb_one_pred += 1\n",
        "        else:\n",
        "          output = 0\n",
        "          nb_zero_pred += 1\n",
        "\n",
        "        if output == label:\n",
        "          nb_good += 1\n",
        "          if output == 1:\n",
        "            nb_good_one += 1\n",
        "          else:\n",
        "            nb_good_zero += 1\n",
        "        nb_total += 1\n",
        "  acc_avg = nb_good/nb_total\n",
        "  acc_0 = nb_good_zero/nb_zero\n",
        "  acc_1 = nb_good_one/nb_one\n",
        "  recall = nb_good_one/(nb_good_one + nb_zero_pred - nb_good_zero)\n",
        "  precision = nb_good_one/nb_one_pred\n",
        "  ticks1 = time.time()\n",
        "  print(\"Done. {:.2f}s\".format(ticks1 - ticks0))\n",
        "  print(\"acc avg: {:.3f}, acc 0: {:.3f}, acc 1: {:.3f}, recall: {:.3f},\\\n",
        "  precision: {:.3f}\".format(acc_avg, acc_0, acc_1, recall, precision))\n",
        "  f1_score = 2/(1/recall + 1/precision)\n",
        "  print(\"F1 Score:\", f1_score)\n",
        "  return nb_good/nb_total, nb_good_zero/nb_zero, nb_good_one/nb_one, recall, precision, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1onVVoWxSPST",
        "colab_type": "text"
      },
      "source": [
        "### Training of RNN model\n",
        "\n",
        "La fonction *train_model* est particulièrement complexe car elle doit prendre en compte le fait que toutes le train set avec pour chacune des quasi 200 000 phrases, une vingtaine de vecteurs de taille 300. Nous répartissons ainsi la charge de calcul:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WReB28a-SNxq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, train, val, words_dict, loss, optimizer, sentence_size,\n",
        "                neighboring_area, batch_size, num_epochs):\n",
        "  '''\n",
        "  Train the model on the train set.\n",
        "\n",
        "  INPUTS:\n",
        "  - model: Net in pytorch.\n",
        "  - train: whole training dataframe (pandas)\n",
        "  - val: The whole validation dataframe (pandas).\n",
        "  - words_dict: is the dictionary associating embedding to each word.\n",
        "  - loss: function respecting pytorch standards\n",
        "  - optimizer: function respecting pytorch standards\n",
        "  - sentence_size: int, size of the sequence\n",
        "  - neighboring_area: int, size of the area to create contextual embeddings\n",
        "  - batch_size: int, size of mini-batches\n",
        "  - num_epochs: int, number of epochs\n",
        "  OUTPUTS:\n",
        "  - NONE\n",
        "  COMPUTATION TIME:\n",
        "  Extremely long and costly in RAM\n",
        "  '''\n",
        "  model.train()\n",
        "  model.zero_grad()\n",
        "  nb_train = train.shape[0]\n",
        "  nb_val = val.shape[0]\n",
        "  list_perfs = []\n",
        "  size_of_batches = 10000\n",
        "  coeff = int(nb_train/nb_val) + 1\n",
        "  size_of_val_batches = int(size_of_batches/coeff)\n",
        "  const = int(2000/batch_size)\n",
        "  ticks0 = time.time()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    ticks0 = time.time()\n",
        "    print(\"Epoch :\", epoch + 1)\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    for k in range(nb_train//size_of_batches): \n",
        "      ticks1 = time.time()\n",
        "      print(\"Preparing {} data...\".format(size_of_batches), end=\"\", flush=True)\n",
        "      # if epoch == 0:\n",
        "      #   features_train, targets_train = process_data(train.values[k*size_of_batches:(k+1)*size_of_batches], \n",
        "      #                                                words_dict, sentence_size,\n",
        "      #                                                neighboring_area)\n",
        "      #   np.save(\"features_{}.npy\".format(k), features_train, allow_pickle=True)\n",
        "      #   np.save(\"targets_{}.npy\".format(k), targets_train, allow_pickle=True)\n",
        "      # else:\n",
        "      #   features_train = np.load(\"features_{}.npy\".format(k), allow_pickle=True)\n",
        "      #   targets_train = np.load(\"targets_{}.npy\".format(k), allow_pickle=True)\n",
        "      features_train, targets_train = process_data(train.values[k*size_of_batches:(k+1)*size_of_batches], \n",
        "                                                   words_dict, sentence_size,\n",
        "                                                   neighboring_area)\n",
        "      features_train = th.from_numpy(features_train)\n",
        "      targets_train = th.from_numpy(targets_train)\n",
        "\n",
        "      # if epoch == 0:\n",
        "      #   features_test, targets_test = process_data(val.values[k*size_of_val_batches:(k+1)*size_of_val_batches], \n",
        "      #                                              words_dict, sentence_size,\n",
        "      #                                              neighboring_area)\n",
        "      #   np.save(\"features_val_{}.npy\".format(k), features_train, allow_pickle=True)\n",
        "      #   np.save(\"targets_val_{}.npy\".format(k), targets_train, allow_pickle=True)\n",
        "      # else:\n",
        "      #   features_test = np.load(\"features_val_{}.npy\".format(k), allow_pickle=True)\n",
        "      #   targets_test = np.load(\"targets_val_{}.npy\".format(k), allow_pickle=True)\n",
        "      features_test, targets_test = process_data(val.values[k*size_of_val_batches:(k+1)*size_of_val_batches], \n",
        "                                                 words_dict, sentence_size,\n",
        "                                                 neighboring_area)\n",
        "      features_test = th.from_numpy(features_test)\n",
        "      targets_test = th.from_numpy(targets_test)\n",
        "\n",
        "      train_set = th.utils.data.TensorDataset(features_train, targets_train)\n",
        "      test_set = th.utils.data.TensorDataset(features_test, targets_test)\n",
        "\n",
        "      train_loader = th.utils.data.DataLoader(train_set, batch_size = batch_size, \n",
        "                                                shuffle = True)\n",
        "      test_loader = th.utils.data.DataLoader(test_set, batch_size = size_of_val_batches, \n",
        "                                                shuffle = True)\n",
        "      ticks2 = time.time()\n",
        "      print(\"Done {:.2f}s.\".format(ticks2-ticks1))\n",
        "      print(\"Training...\")\n",
        "      for i, (sentences, labels) in enumerate(train_loader):\n",
        "        labels = labels.float()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sentences)\n",
        "        train_loss = loss(outputs, labels)\n",
        "        losses += [train_loss.item()]\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i%const == 0):\n",
        "          test_sentences, test_labels = next(iter(test_loader))\n",
        "          model.eval()\n",
        "          test_labels = test_labels.float()\n",
        "          outputs = model(test_sentences)\n",
        "          test_loss = loss(outputs, test_labels)\n",
        "          ticks3 = time.time()\n",
        "          val_losses += [test_loss.item()]\n",
        "          model.train()\n",
        "          print(\"Training loss: {:.4f} | Validation loss: {:.4f} -- {} / {} in {:.2f}s\".format(train_loss.item(),\n",
        "                                                                                      test_loss.item(),\n",
        "                                                                                      k*size_of_batches + (i+1)*batch_size,\n",
        "                                                                                      nb_train,\n",
        "                                                                                      ticks3-ticks1))\n",
        "    ticks2 = time.time()\n",
        "    nb = nb_train - (nb_train//size_of_batches)*size_of_batches\n",
        "    print(\"Preparing {} data...\".format(nb), end = \"\", flush = True)\n",
        "    # if epoch == 0:\n",
        "    #   features_train, targets_train = process_data(train.values[k*size_of_batches:(k+1)*size_of_batches], \n",
        "    #                                                words_dict, sentence_size,\n",
        "    #                                                neighboring_area)\n",
        "    #   np.save(\"features_{}.npy\".format(k), features_train, allow_pickle=True)\n",
        "    #   np.save(\"targets_{}.npy\".format(k), targets_train, allow_pickle=True)\n",
        "    # else:\n",
        "    #   features_train = np.load(\"features_{}.npy\".format(k), allow_pickle=True)\n",
        "    #   targets_train = np.load(\"targets_{}.npy\".format(k), allow_pickle=True)\n",
        "    features_train, targets_train = process_data(train.values[k*size_of_batches:(k+1)*size_of_batches], \n",
        "                                                 words_dict, sentence_size,\n",
        "                                                  neighboring_area)\n",
        "    features_train = th.from_numpy(features_train)\n",
        "    targets_train = th.from_numpy(targets_train)\n",
        "\n",
        "    # if epoch == 0:\n",
        "    #   features_test, targets_test = process_data(val.values[k*size_of_val_batches:(k+1)*size_of_val_batches], \n",
        "    #                                              words_dict, sentence_size,\n",
        "    #                                              neighboring_area)\n",
        "    #   np.save(\"features_val_{}.npy\".format(k), features_train, allow_pickle=True)\n",
        "    #   np.save(\"targets_val_{}.npy\".format(k), targets_train, allow_pickle=True)\n",
        "    # else:\n",
        "    #   features_test = np.load(\"features_val_{}.npy\".format(k), allow_pickle=True)\n",
        "    #   targets_test = np.load(\"targets_val_{}.npy\".format(k), allow_pickle=True)\n",
        "    features_test, targets_test = process_data(val.values[k*size_of_val_batches:(k+1)*size_of_val_batches], \n",
        "                                               words_dict, sentence_size,\n",
        "                                               neighboring_area)\n",
        "    features_test = th.from_numpy(features_test)\n",
        "    targets_test = th.from_numpy(targets_test)\n",
        "\n",
        "    train_set = th.utils.data.TensorDataset(features_train, targets_train)\n",
        "    test_set = th.utils.data.TensorDataset(features_test, targets_test)\n",
        "\n",
        "    train_loader = th.utils.data.DataLoader(train_set, batch_size = batch_size, \n",
        "                                               shuffle = True)\n",
        "    test_loader = th.utils.data.DataLoader(test_set, batch_size = size_of_val_batches, \n",
        "                                              shuffle = True)\n",
        "    ticks3 = time.time()\n",
        "    print(\"Done {:.2f}s.\".format(ticks3-ticks2))\n",
        "    print(\"Training...\", end=\"\", flush=True)\n",
        "    for i, (sentences, labels) in enumerate(train_loader):\n",
        "        labels = labels.float()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sentences)\n",
        "        train_loss = loss(outputs, labels)\n",
        "        losses += [train_loss.item()]\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "    ticks3 = time.time()\n",
        "    print(\"Done.\")\n",
        "    print(\"Epoch n°{}: TRAIN LOSS {:.4f} | VAL LOSS {:.4f} TOTAL TIME: {:.2f}s.\".format(epoch+1, \n",
        "                                                                        sum(losses)/len(losses),\n",
        "                                                                        sum(val_losses)/len(val_losses),\n",
        "                                                                        ticks3-ticks0))\n",
        "    model.eval()\n",
        "    perfs = test_model(model, validate_df.values, words_dict, 10000, sentence_size, \n",
        "               neighboring_area)\n",
        "    th.save(model.state_dict(), \"/content/epoch_{}.pt\".format(epoch+1))\n",
        "    list_perfs.append(perfs[-1])\n",
        "    model.train()\n",
        "    print(\"______________________________________________________________________\")\n",
        "    del(features_train)\n",
        "    del(targets_train)\n",
        "    del(features_test)\n",
        "    del(targets_test)\n",
        "    del(train_set)\n",
        "    del(test_set)\n",
        "    del(train_loader)\n",
        "    del(test_loader)\n",
        "    del(losses)\n",
        "  # we load the model with the best performances\n",
        "  index_best = np.argmax(np.array(list_perfs))\n",
        "  print(\"Loading model with best performances: epoch {}\".format(index_best+1))\n",
        "  model = th.load(\"/content/epoch_{}.pt\".format(index_best+1))\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptkyiUHPMDzz",
        "colab_type": "code",
        "outputId": "05e470ac-3e0b-4576-c1a8-56bf64836607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Connect your personnal drive to save the model generated:\n",
        "# drive.mount('/content/drive')\n",
        "# Change this line to adapt it to your drive\n",
        "# path = \"/content/drive/My Drive/Cours/Kaggle/\"\n",
        "\n",
        "#Hyperparameters:\n",
        "sentence_size = 30\n",
        "neighboring_area = 3\n",
        "\n",
        "# Create RNN\n",
        "input_dim = 300   \n",
        "hidden_dim = 400 \n",
        "layer_dim = 1    \n",
        "output_dim = 1 \n",
        "number_channels = 16\n",
        "conv_kernel = 3\n",
        "max_kernel = 4\n",
        "dropout_rate = 0.4 \n",
        "\n",
        "# model = RNN(input_dim, hidden_dim, layer_dim, output_dim)\n",
        "# model = LSTM(input_dim, hidden_dim, layer_dim, output_dim)\n",
        "model = RNN_Conv(input_dim, hidden_dim, layer_dim, output_dim, sentence_size, \n",
        "                 number_channels, conv_kernel, max_kernel, dropout_rate=dropout_rate)\n",
        "# print(model)\n",
        "\n",
        "# Training Parameters\n",
        "batch_size = 200\n",
        "num_epochs = 10\n",
        "learning_rate = 0.00005\n",
        "loss = nn.BCELoss()\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "optimizer = th.optim.Adam(model.parameters(), lr=learning_rate, \n",
        "                          betas=(0.9, 0.999), eps=1e-08, \n",
        "                          weight_decay=0.0001, amsgrad=False)\n",
        "\n",
        "# Training on train bis:\n",
        "train_model(model, train_df_bis, validate_df_bis, words_dict, loss, optimizer, \n",
        "            sentence_size, neighboring_area, batch_size, num_epochs)\n",
        "\n",
        "# Saving\n",
        "# th.save(model.state_dict(), path+'model_RNNConv_U.pt') #Epochs 20"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch : 1\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.6762 | Validation loss: 0.6589 -- 200 / 235102 in 4.54s\n",
            "Training loss: 0.4339 | Validation loss: 0.5029 -- 2200 / 235102 in 9.40s\n",
            "Training loss: 0.2860 | Validation loss: 0.3782 -- 4200 / 235102 in 14.15s\n",
            "Training loss: 0.3066 | Validation loss: 0.2968 -- 6200 / 235102 in 18.81s\n",
            "Training loss: 0.2172 | Validation loss: 0.2736 -- 8200 / 235102 in 23.58s\n",
            "Preparing 10000 data...Done 2.97s.\n",
            "Training...\n",
            "Training loss: 0.2349 | Validation loss: 0.2912 -- 10200 / 235102 in 4.67s\n",
            "Training loss: 0.2188 | Validation loss: 0.3056 -- 12200 / 235102 in 9.31s\n",
            "Training loss: 0.2522 | Validation loss: 0.2987 -- 14200 / 235102 in 13.86s\n",
            "Training loss: 0.2586 | Validation loss: 0.2883 -- 16200 / 235102 in 18.49s\n",
            "Training loss: 0.2599 | Validation loss: 0.2895 -- 18200 / 235102 in 23.14s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.2878 | Validation loss: 0.2885 -- 20200 / 235102 in 4.63s\n",
            "Training loss: 0.2161 | Validation loss: 0.2911 -- 22200 / 235102 in 9.42s\n",
            "Training loss: 0.2487 | Validation loss: 0.2829 -- 24200 / 235102 in 14.10s\n",
            "Training loss: 0.2835 | Validation loss: 0.2683 -- 26200 / 235102 in 18.79s\n",
            "Training loss: 0.3085 | Validation loss: 0.2654 -- 28200 / 235102 in 23.35s\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.2289 | Validation loss: 0.2762 -- 30200 / 235102 in 4.56s\n",
            "Training loss: 0.2192 | Validation loss: 0.2723 -- 32200 / 235102 in 9.10s\n",
            "Training loss: 0.1576 | Validation loss: 0.2736 -- 34200 / 235102 in 13.75s\n",
            "Training loss: 0.2166 | Validation loss: 0.2638 -- 36200 / 235102 in 18.41s\n",
            "Training loss: 0.2533 | Validation loss: 0.2770 -- 38200 / 235102 in 23.17s\n",
            "Preparing 10000 data...Done 2.95s.\n",
            "Training...\n",
            "Training loss: 0.2085 | Validation loss: 0.2854 -- 40200 / 235102 in 4.65s\n",
            "Training loss: 0.2532 | Validation loss: 0.3322 -- 42200 / 235102 in 9.36s\n",
            "Training loss: 0.1312 | Validation loss: 0.2437 -- 44200 / 235102 in 14.05s\n",
            "Training loss: 0.1434 | Validation loss: 0.2729 -- 46200 / 235102 in 18.66s\n",
            "Training loss: 0.1624 | Validation loss: 0.2901 -- 48200 / 235102 in 23.26s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1724 | Validation loss: 0.2755 -- 50200 / 235102 in 4.61s\n",
            "Training loss: 0.1742 | Validation loss: 0.3063 -- 52200 / 235102 in 9.40s\n",
            "Training loss: 0.3407 | Validation loss: 0.2689 -- 54200 / 235102 in 14.10s\n",
            "Training loss: 0.1653 | Validation loss: 0.1814 -- 56200 / 235102 in 18.89s\n",
            "Training loss: 0.1888 | Validation loss: 0.2870 -- 58200 / 235102 in 23.64s\n",
            "Preparing 10000 data...Done 2.95s.\n",
            "Training...\n",
            "Training loss: 0.1430 | Validation loss: 0.3315 -- 60200 / 235102 in 4.60s\n",
            "Training loss: 0.1459 | Validation loss: 0.2792 -- 62200 / 235102 in 9.27s\n",
            "Training loss: 0.1831 | Validation loss: 0.2441 -- 64200 / 235102 in 13.82s\n",
            "Training loss: 0.0967 | Validation loss: 0.2407 -- 66200 / 235102 in 18.53s\n",
            "Training loss: 0.1232 | Validation loss: 0.2760 -- 68200 / 235102 in 23.22s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.1675 | Validation loss: 0.2634 -- 70200 / 235102 in 4.63s\n",
            "Training loss: 0.1281 | Validation loss: 0.2658 -- 72200 / 235102 in 9.30s\n",
            "Training loss: 0.1720 | Validation loss: 0.2344 -- 74200 / 235102 in 14.10s\n",
            "Training loss: 0.1324 | Validation loss: 0.2678 -- 76200 / 235102 in 18.79s\n",
            "Training loss: 0.1295 | Validation loss: 0.2920 -- 78200 / 235102 in 23.30s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1156 | Validation loss: 0.2028 -- 80200 / 235102 in 4.59s\n",
            "Training loss: 0.1525 | Validation loss: 0.2511 -- 82200 / 235102 in 9.32s\n",
            "Training loss: 0.1416 | Validation loss: 0.2241 -- 84200 / 235102 in 14.09s\n",
            "Training loss: 0.1694 | Validation loss: 0.2359 -- 86200 / 235102 in 18.80s\n",
            "Training loss: 0.2048 | Validation loss: 0.2218 -- 88200 / 235102 in 23.50s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1953 | Validation loss: 0.2204 -- 90200 / 235102 in 4.62s\n",
            "Training loss: 0.1088 | Validation loss: 0.2278 -- 92200 / 235102 in 9.30s\n",
            "Training loss: 0.1861 | Validation loss: 0.3571 -- 94200 / 235102 in 13.92s\n",
            "Training loss: 0.1604 | Validation loss: 0.2755 -- 96200 / 235102 in 18.65s\n",
            "Training loss: 0.1284 | Validation loss: 0.2109 -- 98200 / 235102 in 23.41s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.1234 | Validation loss: 0.2315 -- 100200 / 235102 in 4.64s\n",
            "Training loss: 0.1430 | Validation loss: 0.2857 -- 102200 / 235102 in 9.24s\n",
            "Training loss: 0.1226 | Validation loss: 0.2186 -- 104200 / 235102 in 13.81s\n",
            "Training loss: 0.1856 | Validation loss: 0.2567 -- 106200 / 235102 in 18.40s\n",
            "Training loss: 0.1336 | Validation loss: 0.2783 -- 108200 / 235102 in 23.11s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.2264 | Validation loss: 0.2292 -- 110200 / 235102 in 4.71s\n",
            "Training loss: 0.1991 | Validation loss: 0.2723 -- 112200 / 235102 in 9.43s\n",
            "Training loss: 0.1270 | Validation loss: 0.2620 -- 114200 / 235102 in 14.11s\n",
            "Training loss: 0.1272 | Validation loss: 0.2273 -- 116200 / 235102 in 18.79s\n",
            "Training loss: 0.1122 | Validation loss: 0.2022 -- 118200 / 235102 in 23.52s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1803 | Validation loss: 0.2803 -- 120200 / 235102 in 4.59s\n",
            "Training loss: 0.1949 | Validation loss: 0.2623 -- 122200 / 235102 in 9.22s\n",
            "Training loss: 0.1083 | Validation loss: 0.2758 -- 124200 / 235102 in 13.73s\n",
            "Training loss: 0.1632 | Validation loss: 0.2386 -- 126200 / 235102 in 18.25s\n",
            "Training loss: 0.1726 | Validation loss: 0.2538 -- 128200 / 235102 in 22.90s\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.1206 | Validation loss: 0.2438 -- 130200 / 235102 in 4.61s\n",
            "Training loss: 0.0982 | Validation loss: 0.2469 -- 132200 / 235102 in 9.22s\n",
            "Training loss: 0.1488 | Validation loss: 0.3274 -- 134200 / 235102 in 13.87s\n",
            "Training loss: 0.1342 | Validation loss: 0.2658 -- 136200 / 235102 in 18.48s\n",
            "Training loss: 0.1649 | Validation loss: 0.2178 -- 138200 / 235102 in 23.07s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1324 | Validation loss: 0.2516 -- 140200 / 235102 in 4.51s\n",
            "Training loss: 0.1309 | Validation loss: 0.2475 -- 142200 / 235102 in 9.04s\n",
            "Training loss: 0.1025 | Validation loss: 0.2182 -- 144200 / 235102 in 13.68s\n",
            "Training loss: 0.0898 | Validation loss: 0.2348 -- 146200 / 235102 in 18.16s\n",
            "Training loss: 0.1461 | Validation loss: 0.2653 -- 148200 / 235102 in 22.67s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1726 | Validation loss: 0.2264 -- 150200 / 235102 in 4.49s\n",
            "Training loss: 0.1374 | Validation loss: 0.2452 -- 152200 / 235102 in 9.03s\n",
            "Training loss: 0.1500 | Validation loss: 0.2728 -- 154200 / 235102 in 14.01s\n",
            "Training loss: 0.1132 | Validation loss: 0.2114 -- 156200 / 235102 in 18.55s\n",
            "Training loss: 0.1253 | Validation loss: 0.2408 -- 158200 / 235102 in 23.04s\n",
            "Preparing 10000 data...Done 2.94s.\n",
            "Training...\n",
            "Training loss: 0.1225 | Validation loss: 0.2376 -- 160200 / 235102 in 4.65s\n",
            "Training loss: 0.1404 | Validation loss: 0.2495 -- 162200 / 235102 in 9.12s\n",
            "Training loss: 0.1336 | Validation loss: 0.2386 -- 164200 / 235102 in 13.62s\n",
            "Training loss: 0.1524 | Validation loss: 0.2550 -- 166200 / 235102 in 18.20s\n",
            "Training loss: 0.1350 | Validation loss: 0.2167 -- 168200 / 235102 in 22.86s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1379 | Validation loss: 0.2171 -- 170200 / 235102 in 4.49s\n",
            "Training loss: 0.1056 | Validation loss: 0.2359 -- 172200 / 235102 in 8.90s\n",
            "Training loss: 0.1549 | Validation loss: 0.2570 -- 174200 / 235102 in 13.40s\n",
            "Training loss: 0.1186 | Validation loss: 0.1713 -- 176200 / 235102 in 17.96s\n",
            "Training loss: 0.1870 | Validation loss: 0.2052 -- 178200 / 235102 in 22.50s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.1300 | Validation loss: 0.2071 -- 180200 / 235102 in 4.57s\n",
            "Training loss: 0.1501 | Validation loss: 0.3139 -- 182200 / 235102 in 9.03s\n",
            "Training loss: 0.1528 | Validation loss: 0.2444 -- 184200 / 235102 in 13.56s\n",
            "Training loss: 0.1209 | Validation loss: 0.2258 -- 186200 / 235102 in 18.09s\n",
            "Training loss: 0.1058 | Validation loss: 0.2328 -- 188200 / 235102 in 22.51s\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.1721 | Validation loss: 0.2823 -- 190200 / 235102 in 4.56s\n",
            "Training loss: 0.1414 | Validation loss: 0.2383 -- 192200 / 235102 in 9.13s\n",
            "Training loss: 0.1423 | Validation loss: 0.2413 -- 194200 / 235102 in 13.76s\n",
            "Training loss: 0.1059 | Validation loss: 0.2328 -- 196200 / 235102 in 18.43s\n",
            "Training loss: 0.1124 | Validation loss: 0.2407 -- 198200 / 235102 in 23.02s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.1108 | Validation loss: 0.2093 -- 200200 / 235102 in 4.53s\n",
            "Training loss: 0.1341 | Validation loss: 0.2031 -- 202200 / 235102 in 9.13s\n",
            "Training loss: 0.1440 | Validation loss: 0.2177 -- 204200 / 235102 in 13.66s\n",
            "Training loss: 0.1924 | Validation loss: 0.2556 -- 206200 / 235102 in 18.22s\n",
            "Training loss: 0.1407 | Validation loss: 0.2050 -- 208200 / 235102 in 22.92s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.1343 | Validation loss: 0.2468 -- 210200 / 235102 in 4.53s\n",
            "Training loss: 0.2052 | Validation loss: 0.2642 -- 212200 / 235102 in 9.11s\n",
            "Training loss: 0.1958 | Validation loss: 0.2089 -- 214200 / 235102 in 13.66s\n",
            "Training loss: 0.1182 | Validation loss: 0.1709 -- 216200 / 235102 in 18.26s\n",
            "Training loss: 0.1402 | Validation loss: 0.3071 -- 218200 / 235102 in 22.90s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1249 | Validation loss: 0.2117 -- 220200 / 235102 in 4.58s\n",
            "Training loss: 0.0551 | Validation loss: 0.1988 -- 222200 / 235102 in 9.10s\n",
            "Training loss: 0.1728 | Validation loss: 0.2971 -- 224200 / 235102 in 13.67s\n",
            "Training loss: 0.0914 | Validation loss: 0.2144 -- 226200 / 235102 in 18.30s\n",
            "Training loss: 0.0987 | Validation loss: 0.2386 -- 228200 / 235102 in 22.93s\n",
            "Preparing 5102 data...Done 2.90s.\n",
            "Training...Done.\n",
            "Epoch n°1: TRAIN LOSS 0.1701 | VAL LOSS 0.2599 TOTAL TIME: 619.14s.\n",
            "Testing on 10000...Done. 8.15s\n",
            "acc avg: 0.948, acc 0: 0.987, acc 1: 0.346, recall: 0.346,  precision: 0.639\n",
            "F1 Score: 0.44893617021276594\n",
            "______________________________________________________________________\n",
            "Epoch : 2\n",
            "Preparing 10000 data...Done 2.98s.\n",
            "Training...\n",
            "Training loss: 0.0509 | Validation loss: 0.2121 -- 200 / 235102 in 4.66s\n",
            "Training loss: 0.1223 | Validation loss: 0.2550 -- 2200 / 235102 in 9.28s\n",
            "Training loss: 0.1431 | Validation loss: 0.2098 -- 4200 / 235102 in 13.89s\n",
            "Training loss: 0.1505 | Validation loss: 0.2303 -- 6200 / 235102 in 18.56s\n",
            "Training loss: 0.1644 | Validation loss: 0.2362 -- 8200 / 235102 in 23.18s\n",
            "Preparing 10000 data...Done 2.94s.\n",
            "Training...\n",
            "Training loss: 0.1148 | Validation loss: 0.2320 -- 10200 / 235102 in 4.66s\n",
            "Training loss: 0.1259 | Validation loss: 0.2123 -- 12200 / 235102 in 9.21s\n",
            "Training loss: 0.1533 | Validation loss: 0.2412 -- 14200 / 235102 in 13.76s\n",
            "Training loss: 0.1927 | Validation loss: 0.2497 -- 16200 / 235102 in 18.35s\n",
            "Training loss: 0.2477 | Validation loss: 0.2739 -- 18200 / 235102 in 23.01s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.0922 | Validation loss: 0.1879 -- 20200 / 235102 in 4.57s\n",
            "Training loss: 0.1854 | Validation loss: 0.2123 -- 22200 / 235102 in 9.12s\n",
            "Training loss: 0.1564 | Validation loss: 0.2264 -- 24200 / 235102 in 13.84s\n",
            "Training loss: 0.1419 | Validation loss: 0.1788 -- 26200 / 235102 in 18.49s\n",
            "Training loss: 0.1317 | Validation loss: 0.2412 -- 28200 / 235102 in 23.16s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1460 | Validation loss: 0.2049 -- 30200 / 235102 in 4.52s\n",
            "Training loss: 0.1979 | Validation loss: 0.1926 -- 32200 / 235102 in 9.08s\n",
            "Training loss: 0.1897 | Validation loss: 0.2128 -- 34200 / 235102 in 13.68s\n",
            "Training loss: 0.1162 | Validation loss: 0.1947 -- 36200 / 235102 in 18.37s\n",
            "Training loss: 0.1108 | Validation loss: 0.2377 -- 38200 / 235102 in 23.08s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1298 | Validation loss: 0.2495 -- 40200 / 235102 in 4.55s\n",
            "Training loss: 0.1175 | Validation loss: 0.2013 -- 42200 / 235102 in 9.21s\n",
            "Training loss: 0.1226 | Validation loss: 0.2262 -- 44200 / 235102 in 13.83s\n",
            "Training loss: 0.1121 | Validation loss: 0.2405 -- 46200 / 235102 in 18.48s\n",
            "Training loss: 0.1067 | Validation loss: 0.2674 -- 48200 / 235102 in 23.01s\n",
            "Preparing 10000 data...Done 2.85s.\n",
            "Training...\n",
            "Training loss: 0.1632 | Validation loss: 0.2123 -- 50200 / 235102 in 4.53s\n",
            "Training loss: 0.1346 | Validation loss: 0.2077 -- 52200 / 235102 in 9.24s\n",
            "Training loss: 0.1627 | Validation loss: 0.2325 -- 54200 / 235102 in 13.97s\n",
            "Training loss: 0.1179 | Validation loss: 0.1896 -- 56200 / 235102 in 18.66s\n",
            "Training loss: 0.1660 | Validation loss: 0.2177 -- 58200 / 235102 in 23.24s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1597 | Validation loss: 0.2186 -- 60200 / 235102 in 4.57s\n",
            "Training loss: 0.1166 | Validation loss: 0.1978 -- 62200 / 235102 in 9.28s\n",
            "Training loss: 0.1424 | Validation loss: 0.2304 -- 64200 / 235102 in 13.91s\n",
            "Training loss: 0.1543 | Validation loss: 0.2678 -- 66200 / 235102 in 18.56s\n",
            "Training loss: 0.0750 | Validation loss: 0.1914 -- 68200 / 235102 in 23.22s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.1714 | Validation loss: 0.2163 -- 70200 / 235102 in 4.62s\n",
            "Training loss: 0.1669 | Validation loss: 0.2215 -- 72200 / 235102 in 9.21s\n",
            "Training loss: 0.1700 | Validation loss: 0.2048 -- 74200 / 235102 in 13.87s\n",
            "Training loss: 0.1332 | Validation loss: 0.2189 -- 76200 / 235102 in 18.52s\n",
            "Training loss: 0.1192 | Validation loss: 0.1869 -- 78200 / 235102 in 23.16s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1705 | Validation loss: 0.1919 -- 80200 / 235102 in 4.52s\n",
            "Training loss: 0.1708 | Validation loss: 0.2005 -- 82200 / 235102 in 9.02s\n",
            "Training loss: 0.0602 | Validation loss: 0.1895 -- 84200 / 235102 in 13.74s\n",
            "Training loss: 0.1585 | Validation loss: 0.2194 -- 86200 / 235102 in 18.38s\n",
            "Training loss: 0.0965 | Validation loss: 0.1933 -- 88200 / 235102 in 23.06s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1758 | Validation loss: 0.2299 -- 90200 / 235102 in 4.59s\n",
            "Training loss: 0.1282 | Validation loss: 0.1956 -- 92200 / 235102 in 9.21s\n",
            "Training loss: 0.1277 | Validation loss: 0.2389 -- 94200 / 235102 in 13.88s\n",
            "Training loss: 0.0966 | Validation loss: 0.1969 -- 96200 / 235102 in 18.53s\n",
            "Training loss: 0.1373 | Validation loss: 0.2079 -- 98200 / 235102 in 23.05s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1145 | Validation loss: 0.2052 -- 100200 / 235102 in 4.58s\n",
            "Training loss: 0.1370 | Validation loss: 0.2573 -- 102200 / 235102 in 9.17s\n",
            "Training loss: 0.1274 | Validation loss: 0.1872 -- 104200 / 235102 in 13.70s\n",
            "Training loss: 0.1230 | Validation loss: 0.2089 -- 106200 / 235102 in 18.26s\n",
            "Training loss: 0.1132 | Validation loss: 0.2069 -- 108200 / 235102 in 22.89s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.1004 | Validation loss: 0.1958 -- 110200 / 235102 in 4.62s\n",
            "Training loss: 0.1274 | Validation loss: 0.2110 -- 112200 / 235102 in 9.27s\n",
            "Training loss: 0.1016 | Validation loss: 0.1796 -- 114200 / 235102 in 13.95s\n",
            "Training loss: 0.1408 | Validation loss: 0.2458 -- 116200 / 235102 in 18.60s\n",
            "Training loss: 0.1142 | Validation loss: 0.2255 -- 118200 / 235102 in 23.12s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1903 | Validation loss: 0.2318 -- 120200 / 235102 in 4.62s\n",
            "Training loss: 0.2148 | Validation loss: 0.1964 -- 122200 / 235102 in 9.23s\n",
            "Training loss: 0.1247 | Validation loss: 0.2031 -- 124200 / 235102 in 13.81s\n",
            "Training loss: 0.1365 | Validation loss: 0.2510 -- 126200 / 235102 in 18.47s\n",
            "Training loss: 0.1445 | Validation loss: 0.1884 -- 128200 / 235102 in 23.09s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.0992 | Validation loss: 0.2163 -- 130200 / 235102 in 4.53s\n",
            "Training loss: 0.1228 | Validation loss: 0.2154 -- 132200 / 235102 in 9.12s\n",
            "Training loss: 0.1036 | Validation loss: 0.1859 -- 134200 / 235102 in 13.73s\n",
            "Training loss: 0.1127 | Validation loss: 0.2014 -- 136200 / 235102 in 18.45s\n",
            "Training loss: 0.1531 | Validation loss: 0.2177 -- 138200 / 235102 in 23.11s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1562 | Validation loss: 0.1580 -- 140200 / 235102 in 4.56s\n",
            "Training loss: 0.1360 | Validation loss: 0.2227 -- 142200 / 235102 in 9.20s\n",
            "Training loss: 0.1111 | Validation loss: 0.1754 -- 144200 / 235102 in 13.83s\n",
            "Training loss: 0.1461 | Validation loss: 0.1850 -- 146200 / 235102 in 18.40s\n",
            "Training loss: 0.1005 | Validation loss: 0.2080 -- 148200 / 235102 in 23.01s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1673 | Validation loss: 0.2135 -- 150200 / 235102 in 4.54s\n",
            "Training loss: 0.1438 | Validation loss: 0.1744 -- 152200 / 235102 in 9.16s\n",
            "Training loss: 0.1341 | Validation loss: 0.2189 -- 154200 / 235102 in 13.75s\n",
            "Training loss: 0.1316 | Validation loss: 0.2012 -- 156200 / 235102 in 18.37s\n",
            "Training loss: 0.1273 | Validation loss: 0.2035 -- 158200 / 235102 in 22.95s\n",
            "Preparing 10000 data...Done 2.85s.\n",
            "Training...\n",
            "Training loss: 0.1281 | Validation loss: 0.1772 -- 160200 / 235102 in 4.60s\n",
            "Training loss: 0.0909 | Validation loss: 0.1916 -- 162200 / 235102 in 9.24s\n",
            "Training loss: 0.1239 | Validation loss: 0.1815 -- 164200 / 235102 in 13.88s\n",
            "Training loss: 0.1355 | Validation loss: 0.1685 -- 166200 / 235102 in 18.51s\n",
            "Training loss: 0.1152 | Validation loss: 0.2043 -- 168200 / 235102 in 23.13s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.0999 | Validation loss: 0.1802 -- 170200 / 235102 in 4.55s\n",
            "Training loss: 0.1354 | Validation loss: 0.2213 -- 172200 / 235102 in 9.14s\n",
            "Training loss: 0.1378 | Validation loss: 0.1891 -- 174200 / 235102 in 13.75s\n",
            "Training loss: 0.1257 | Validation loss: 0.1804 -- 176200 / 235102 in 18.43s\n",
            "Training loss: 0.1310 | Validation loss: 0.2271 -- 178200 / 235102 in 23.06s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1654 | Validation loss: 0.1906 -- 180200 / 235102 in 4.56s\n",
            "Training loss: 0.1132 | Validation loss: 0.1954 -- 182200 / 235102 in 9.20s\n",
            "Training loss: 0.1049 | Validation loss: 0.2252 -- 184200 / 235102 in 13.70s\n",
            "Training loss: 0.1505 | Validation loss: 0.2055 -- 186200 / 235102 in 18.25s\n",
            "Training loss: 0.1069 | Validation loss: 0.1827 -- 188200 / 235102 in 22.89s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.1282 | Validation loss: 0.2201 -- 190200 / 235102 in 4.61s\n",
            "Training loss: 0.1530 | Validation loss: 0.1829 -- 192200 / 235102 in 9.24s\n",
            "Training loss: 0.0694 | Validation loss: 0.2028 -- 194200 / 235102 in 13.80s\n",
            "Training loss: 0.1168 | Validation loss: 0.2461 -- 196200 / 235102 in 18.31s\n",
            "Training loss: 0.1054 | Validation loss: 0.1801 -- 198200 / 235102 in 22.96s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.0847 | Validation loss: 0.2048 -- 200200 / 235102 in 4.54s\n",
            "Training loss: 0.0906 | Validation loss: 0.2050 -- 202200 / 235102 in 9.15s\n",
            "Training loss: 0.2244 | Validation loss: 0.1891 -- 204200 / 235102 in 13.78s\n",
            "Training loss: 0.1651 | Validation loss: 0.1644 -- 206200 / 235102 in 18.48s\n",
            "Training loss: 0.1486 | Validation loss: 0.2300 -- 208200 / 235102 in 23.11s\n",
            "Preparing 10000 data...Done 2.85s.\n",
            "Training...\n",
            "Training loss: 0.1118 | Validation loss: 0.1882 -- 210200 / 235102 in 4.46s\n",
            "Training loss: 0.1183 | Validation loss: 0.2337 -- 212200 / 235102 in 9.01s\n",
            "Training loss: 0.1403 | Validation loss: 0.1838 -- 214200 / 235102 in 13.61s\n",
            "Training loss: 0.1570 | Validation loss: 0.1779 -- 216200 / 235102 in 18.25s\n",
            "Training loss: 0.0898 | Validation loss: 0.1796 -- 218200 / 235102 in 22.94s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1972 | Validation loss: 0.2005 -- 220200 / 235102 in 4.58s\n",
            "Training loss: 0.1264 | Validation loss: 0.1759 -- 222200 / 235102 in 9.05s\n",
            "Training loss: 0.1265 | Validation loss: 0.2149 -- 224200 / 235102 in 13.65s\n",
            "Training loss: 0.0843 | Validation loss: 0.1675 -- 226200 / 235102 in 18.28s\n",
            "Training loss: 0.1218 | Validation loss: 0.2041 -- 228200 / 235102 in 22.92s\n",
            "Preparing 5102 data...Done 2.90s.\n",
            "Training...Done.\n",
            "Epoch n°2: TRAIN LOSS 0.1375 | VAL LOSS 0.2084 TOTAL TIME: 617.45s.\n",
            "Testing on 10000...Done. 8.13s\n",
            "acc avg: 0.952, acc 0: 0.982, acc 1: 0.445, recall: 0.445,  precision: 0.609\n",
            "F1 Score: 0.5145145145145145\n",
            "______________________________________________________________________\n",
            "Epoch : 3\n",
            "Preparing 10000 data...Done 2.96s.\n",
            "Training...\n",
            "Training loss: 0.1047 | Validation loss: 0.1787 -- 200 / 235102 in 4.66s\n",
            "Training loss: 0.1347 | Validation loss: 0.1831 -- 2200 / 235102 in 9.34s\n",
            "Training loss: 0.1078 | Validation loss: 0.1971 -- 4200 / 235102 in 14.01s\n",
            "Training loss: 0.1313 | Validation loss: 0.2089 -- 6200 / 235102 in 18.61s\n",
            "Training loss: 0.0983 | Validation loss: 0.1837 -- 8200 / 235102 in 23.02s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.1368 | Validation loss: 0.2091 -- 10200 / 235102 in 4.43s\n",
            "Training loss: 0.1349 | Validation loss: 0.1667 -- 12200 / 235102 in 8.98s\n",
            "Training loss: 0.1644 | Validation loss: 0.2130 -- 14200 / 235102 in 13.55s\n",
            "Training loss: 0.1463 | Validation loss: 0.1673 -- 16200 / 235102 in 18.09s\n",
            "Training loss: 0.1339 | Validation loss: 0.1890 -- 18200 / 235102 in 22.70s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1284 | Validation loss: 0.1784 -- 20200 / 235102 in 4.52s\n",
            "Training loss: 0.0601 | Validation loss: 0.1839 -- 22200 / 235102 in 9.04s\n",
            "Training loss: 0.1264 | Validation loss: 0.1577 -- 24200 / 235102 in 13.64s\n",
            "Training loss: 0.1677 | Validation loss: 0.1809 -- 26200 / 235102 in 18.22s\n",
            "Training loss: 0.1292 | Validation loss: 0.1850 -- 28200 / 235102 in 22.80s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1146 | Validation loss: 0.1527 -- 30200 / 235102 in 4.50s\n",
            "Training loss: 0.1208 | Validation loss: 0.2052 -- 32200 / 235102 in 9.15s\n",
            "Training loss: 0.1606 | Validation loss: 0.1677 -- 34200 / 235102 in 13.74s\n",
            "Training loss: 0.1453 | Validation loss: 0.1693 -- 36200 / 235102 in 18.37s\n",
            "Training loss: 0.1771 | Validation loss: 0.1955 -- 38200 / 235102 in 23.05s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1609 | Validation loss: 0.2075 -- 40200 / 235102 in 4.54s\n",
            "Training loss: 0.1922 | Validation loss: 0.1876 -- 42200 / 235102 in 9.09s\n",
            "Training loss: 0.1767 | Validation loss: 0.2096 -- 44200 / 235102 in 13.68s\n",
            "Training loss: 0.1033 | Validation loss: 0.1814 -- 46200 / 235102 in 18.31s\n",
            "Training loss: 0.1056 | Validation loss: 0.1822 -- 48200 / 235102 in 22.96s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.0933 | Validation loss: 0.1854 -- 50200 / 235102 in 4.59s\n",
            "Training loss: 0.1408 | Validation loss: 0.1828 -- 52200 / 235102 in 9.25s\n",
            "Training loss: 0.1352 | Validation loss: 0.1772 -- 54200 / 235102 in 13.88s\n",
            "Training loss: 0.0929 | Validation loss: 0.1759 -- 56200 / 235102 in 18.50s\n",
            "Training loss: 0.1241 | Validation loss: 0.2282 -- 58200 / 235102 in 23.15s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1351 | Validation loss: 0.1634 -- 60200 / 235102 in 4.58s\n",
            "Training loss: 0.1360 | Validation loss: 0.2065 -- 62200 / 235102 in 9.28s\n",
            "Training loss: 0.0987 | Validation loss: 0.1860 -- 64200 / 235102 in 13.81s\n",
            "Training loss: 0.1842 | Validation loss: 0.2359 -- 66200 / 235102 in 18.39s\n",
            "Training loss: 0.1393 | Validation loss: 0.1763 -- 68200 / 235102 in 23.08s\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.1854 | Validation loss: 0.1886 -- 70200 / 235102 in 4.59s\n",
            "Training loss: 0.1711 | Validation loss: 0.2162 -- 72200 / 235102 in 9.29s\n",
            "Training loss: 0.1250 | Validation loss: 0.1694 -- 74200 / 235102 in 14.04s\n",
            "Training loss: 0.1290 | Validation loss: 0.1936 -- 76200 / 235102 in 18.74s\n",
            "Training loss: 0.1157 | Validation loss: 0.1750 -- 78200 / 235102 in 23.34s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1784 | Validation loss: 0.1679 -- 80200 / 235102 in 4.53s\n",
            "Training loss: 0.1180 | Validation loss: 0.2162 -- 82200 / 235102 in 9.17s\n",
            "Training loss: 0.0756 | Validation loss: 0.1661 -- 84200 / 235102 in 13.87s\n",
            "Training loss: 0.1539 | Validation loss: 0.1706 -- 86200 / 235102 in 18.50s\n",
            "Training loss: 0.1083 | Validation loss: 0.1851 -- 88200 / 235102 in 23.05s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1076 | Validation loss: 0.1697 -- 90200 / 235102 in 4.60s\n",
            "Training loss: 0.1447 | Validation loss: 0.1854 -- 92200 / 235102 in 9.26s\n",
            "Training loss: 0.1941 | Validation loss: 0.1988 -- 94200 / 235102 in 13.86s\n",
            "Training loss: 0.1732 | Validation loss: 0.1688 -- 96200 / 235102 in 18.59s\n",
            "Training loss: 0.1065 | Validation loss: 0.1839 -- 98200 / 235102 in 23.23s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.0945 | Validation loss: 0.1954 -- 100200 / 235102 in 4.54s\n",
            "Training loss: 0.1403 | Validation loss: 0.1737 -- 102200 / 235102 in 9.05s\n",
            "Training loss: 0.1196 | Validation loss: 0.1855 -- 104200 / 235102 in 13.60s\n",
            "Training loss: 0.1326 | Validation loss: 0.1889 -- 106200 / 235102 in 18.19s\n",
            "Training loss: 0.1235 | Validation loss: 0.1910 -- 108200 / 235102 in 22.81s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.0969 | Validation loss: 0.1849 -- 110200 / 235102 in 4.68s\n",
            "Training loss: 0.1190 | Validation loss: 0.1814 -- 112200 / 235102 in 9.33s\n",
            "Training loss: 0.1392 | Validation loss: 0.1767 -- 114200 / 235102 in 14.01s\n",
            "Training loss: 0.1492 | Validation loss: 0.1768 -- 116200 / 235102 in 18.67s\n",
            "Training loss: 0.1552 | Validation loss: 0.1769 -- 118200 / 235102 in 23.33s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.0951 | Validation loss: 0.1755 -- 120200 / 235102 in 4.64s\n",
            "Training loss: 0.1315 | Validation loss: 0.1599 -- 122200 / 235102 in 9.25s\n",
            "Training loss: 0.1773 | Validation loss: 0.2092 -- 124200 / 235102 in 13.79s\n",
            "Training loss: 0.1752 | Validation loss: 0.1679 -- 126200 / 235102 in 18.41s\n",
            "Training loss: 0.1571 | Validation loss: 0.2232 -- 128200 / 235102 in 23.09s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.0980 | Validation loss: 0.1637 -- 130200 / 235102 in 4.56s\n",
            "Training loss: 0.1416 | Validation loss: 0.1900 -- 132200 / 235102 in 9.19s\n",
            "Training loss: 0.0587 | Validation loss: 0.1628 -- 134200 / 235102 in 13.71s\n",
            "Training loss: 0.0959 | Validation loss: 0.1861 -- 136200 / 235102 in 18.35s\n",
            "Training loss: 0.0703 | Validation loss: 0.1707 -- 138200 / 235102 in 22.94s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1204 | Validation loss: 0.1517 -- 140200 / 235102 in 4.58s\n",
            "Training loss: 0.1245 | Validation loss: 0.1681 -- 142200 / 235102 in 9.21s\n",
            "Training loss: 0.0842 | Validation loss: 0.1613 -- 144200 / 235102 in 13.81s\n",
            "Training loss: 0.1252 | Validation loss: 0.1844 -- 146200 / 235102 in 18.43s\n",
            "Training loss: 0.1605 | Validation loss: 0.1641 -- 148200 / 235102 in 23.16s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.2186 | Validation loss: 0.1926 -- 150200 / 235102 in 4.55s\n",
            "Training loss: 0.1126 | Validation loss: 0.1650 -- 152200 / 235102 in 9.21s\n",
            "Training loss: 0.1031 | Validation loss: 0.2188 -- 154200 / 235102 in 13.89s\n",
            "Training loss: 0.1426 | Validation loss: 0.1557 -- 156200 / 235102 in 18.57s\n",
            "Training loss: 0.1872 | Validation loss: 0.2023 -- 158200 / 235102 in 23.24s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1805 | Validation loss: 0.1404 -- 160200 / 235102 in 4.58s\n",
            "Training loss: 0.0882 | Validation loss: 0.1752 -- 162200 / 235102 in 9.24s\n",
            "Training loss: 0.1559 | Validation loss: 0.1509 -- 164200 / 235102 in 13.96s\n",
            "Training loss: 0.1053 | Validation loss: 0.1666 -- 166200 / 235102 in 18.66s\n",
            "Training loss: 0.1201 | Validation loss: 0.1785 -- 168200 / 235102 in 23.31s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1930 | Validation loss: 0.1662 -- 170200 / 235102 in 4.57s\n",
            "Training loss: 0.1544 | Validation loss: 0.1732 -- 172200 / 235102 in 9.31s\n",
            "Training loss: 0.0890 | Validation loss: 0.1486 -- 174200 / 235102 in 13.92s\n",
            "Training loss: 0.1095 | Validation loss: 0.1642 -- 176200 / 235102 in 18.54s\n",
            "Training loss: 0.0822 | Validation loss: 0.1539 -- 178200 / 235102 in 23.14s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.1498 | Validation loss: 0.2043 -- 180200 / 235102 in 4.56s\n",
            "Training loss: 0.1277 | Validation loss: 0.1763 -- 182200 / 235102 in 9.19s\n",
            "Training loss: 0.1570 | Validation loss: 0.1754 -- 184200 / 235102 in 13.90s\n",
            "Training loss: 0.1588 | Validation loss: 0.1952 -- 186200 / 235102 in 18.56s\n",
            "Training loss: 0.1085 | Validation loss: 0.1756 -- 188200 / 235102 in 23.12s\n",
            "Preparing 10000 data...Done 2.95s.\n",
            "Training...\n",
            "Training loss: 0.1644 | Validation loss: 0.1690 -- 190200 / 235102 in 4.64s\n",
            "Training loss: 0.0841 | Validation loss: 0.1805 -- 192200 / 235102 in 9.33s\n",
            "Training loss: 0.1523 | Validation loss: 0.1812 -- 194200 / 235102 in 13.96s\n",
            "Training loss: 0.0679 | Validation loss: 0.1700 -- 196200 / 235102 in 18.66s\n",
            "Training loss: 0.1298 | Validation loss: 0.1664 -- 198200 / 235102 in 23.31s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1693 | Validation loss: 0.1723 -- 200200 / 235102 in 4.57s\n",
            "Training loss: 0.0708 | Validation loss: 0.1673 -- 202200 / 235102 in 9.24s\n",
            "Training loss: 0.0835 | Validation loss: 0.1504 -- 204200 / 235102 in 13.83s\n",
            "Training loss: 0.1314 | Validation loss: 0.1788 -- 206200 / 235102 in 18.50s\n",
            "Training loss: 0.1403 | Validation loss: 0.1695 -- 208200 / 235102 in 23.14s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1348 | Validation loss: 0.1977 -- 210200 / 235102 in 4.55s\n",
            "Training loss: 0.1281 | Validation loss: 0.1637 -- 212200 / 235102 in 9.19s\n",
            "Training loss: 0.1358 | Validation loss: 0.1710 -- 214200 / 235102 in 13.76s\n",
            "Training loss: 0.0996 | Validation loss: 0.1838 -- 216200 / 235102 in 18.31s\n",
            "Training loss: 0.0878 | Validation loss: 0.1693 -- 218200 / 235102 in 22.87s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1079 | Validation loss: 0.1609 -- 220200 / 235102 in 4.65s\n",
            "Training loss: 0.1538 | Validation loss: 0.1661 -- 222200 / 235102 in 9.32s\n",
            "Training loss: 0.1947 | Validation loss: 0.1644 -- 224200 / 235102 in 13.96s\n",
            "Training loss: 0.1132 | Validation loss: 0.1688 -- 226200 / 235102 in 18.63s\n",
            "Training loss: 0.1320 | Validation loss: 0.1624 -- 228200 / 235102 in 23.16s\n",
            "Preparing 5102 data...Done 2.87s.\n",
            "Training...Done.\n",
            "Epoch n°3: TRAIN LOSS 0.1316 | VAL LOSS 0.1798 TOTAL TIME: 618.37s.\n",
            "Testing on 10000...Done. 8.30s\n",
            "acc avg: 0.951, acc 0: 0.984, acc 1: 0.442, recall: 0.442,  precision: 0.631\n",
            "F1 Score: 0.5197628458498023\n",
            "______________________________________________________________________\n",
            "Epoch : 4\n",
            "Preparing 10000 data...Done 2.94s.\n",
            "Training...\n",
            "Training loss: 0.1112 | Validation loss: 0.1690 -- 200 / 235102 in 4.54s\n",
            "Training loss: 0.1112 | Validation loss: 0.1910 -- 2200 / 235102 in 9.06s\n",
            "Training loss: 0.1465 | Validation loss: 0.1608 -- 4200 / 235102 in 13.61s\n",
            "Training loss: 0.1360 | Validation loss: 0.2009 -- 6200 / 235102 in 18.25s\n",
            "Training loss: 0.1206 | Validation loss: 0.1639 -- 8200 / 235102 in 22.91s\n",
            "Preparing 10000 data...Done 2.97s.\n",
            "Training...\n",
            "Training loss: 0.1550 | Validation loss: 0.1743 -- 10200 / 235102 in 4.66s\n",
            "Training loss: 0.1251 | Validation loss: 0.1468 -- 12200 / 235102 in 9.30s\n",
            "Training loss: 0.1696 | Validation loss: 0.1717 -- 14200 / 235102 in 13.96s\n",
            "Training loss: 0.1113 | Validation loss: 0.1629 -- 16200 / 235102 in 18.58s\n",
            "Training loss: 0.1030 | Validation loss: 0.1879 -- 18200 / 235102 in 23.15s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.0772 | Validation loss: 0.1550 -- 20200 / 235102 in 4.59s\n",
            "Training loss: 0.1299 | Validation loss: 0.1751 -- 22200 / 235102 in 9.26s\n",
            "Training loss: 0.1608 | Validation loss: 0.1614 -- 24200 / 235102 in 13.99s\n",
            "Training loss: 0.1048 | Validation loss: 0.1396 -- 26200 / 235102 in 18.69s\n",
            "Training loss: 0.1311 | Validation loss: 0.1549 -- 28200 / 235102 in 23.29s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.0835 | Validation loss: 0.1497 -- 30200 / 235102 in 4.53s\n",
            "Training loss: 0.1078 | Validation loss: 0.1597 -- 32200 / 235102 in 9.22s\n",
            "Training loss: 0.1479 | Validation loss: 0.1851 -- 34200 / 235102 in 13.88s\n",
            "Training loss: 0.1094 | Validation loss: 0.1750 -- 36200 / 235102 in 18.51s\n",
            "Training loss: 0.1320 | Validation loss: 0.1602 -- 38200 / 235102 in 23.25s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1850 | Validation loss: 0.2092 -- 40200 / 235102 in 4.58s\n",
            "Training loss: 0.1013 | Validation loss: 0.1537 -- 42200 / 235102 in 9.15s\n",
            "Training loss: 0.1691 | Validation loss: 0.2066 -- 44200 / 235102 in 13.79s\n",
            "Training loss: 0.0817 | Validation loss: 0.1587 -- 46200 / 235102 in 18.50s\n",
            "Training loss: 0.1135 | Validation loss: 0.1656 -- 48200 / 235102 in 23.18s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.2150 | Validation loss: 0.1690 -- 50200 / 235102 in 4.61s\n",
            "Training loss: 0.1515 | Validation loss: 0.1638 -- 52200 / 235102 in 9.42s\n",
            "Training loss: 0.1340 | Validation loss: 0.1805 -- 54200 / 235102 in 14.07s\n",
            "Training loss: 0.0796 | Validation loss: 0.1745 -- 56200 / 235102 in 18.57s\n",
            "Training loss: 0.0852 | Validation loss: 0.1641 -- 58200 / 235102 in 23.07s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1024 | Validation loss: 0.1789 -- 60200 / 235102 in 4.60s\n",
            "Training loss: 0.1045 | Validation loss: 0.1626 -- 62200 / 235102 in 9.32s\n",
            "Training loss: 0.1333 | Validation loss: 0.1859 -- 64200 / 235102 in 14.03s\n",
            "Training loss: 0.0862 | Validation loss: 0.1677 -- 66200 / 235102 in 18.60s\n",
            "Training loss: 0.1357 | Validation loss: 0.2109 -- 68200 / 235102 in 23.17s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1163 | Validation loss: 0.1530 -- 70200 / 235102 in 4.59s\n",
            "Training loss: 0.1694 | Validation loss: 0.1522 -- 72200 / 235102 in 9.27s\n",
            "Training loss: 0.1186 | Validation loss: 0.1618 -- 74200 / 235102 in 14.04s\n",
            "Training loss: 0.1292 | Validation loss: 0.1554 -- 76200 / 235102 in 18.85s\n",
            "Training loss: 0.1297 | Validation loss: 0.1876 -- 78200 / 235102 in 23.51s\n",
            "Preparing 10000 data...Done 2.82s.\n",
            "Training...\n",
            "Training loss: 0.1834 | Validation loss: 0.1632 -- 80200 / 235102 in 4.45s\n",
            "Training loss: 0.0939 | Validation loss: 0.1572 -- 82200 / 235102 in 9.06s\n",
            "Training loss: 0.1111 | Validation loss: 0.1558 -- 84200 / 235102 in 13.79s\n",
            "Training loss: 0.1597 | Validation loss: 0.1858 -- 86200 / 235102 in 18.51s\n",
            "Training loss: 0.1856 | Validation loss: 0.1459 -- 88200 / 235102 in 23.22s\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.1031 | Validation loss: 0.1708 -- 90200 / 235102 in 4.61s\n",
            "Training loss: 0.0995 | Validation loss: 0.1430 -- 92200 / 235102 in 9.33s\n",
            "Training loss: 0.1457 | Validation loss: 0.2042 -- 94200 / 235102 in 13.87s\n",
            "Training loss: 0.1528 | Validation loss: 0.1506 -- 96200 / 235102 in 18.46s\n",
            "Training loss: 0.1437 | Validation loss: 0.1887 -- 98200 / 235102 in 23.16s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1220 | Validation loss: 0.1613 -- 100200 / 235102 in 4.54s\n",
            "Training loss: 0.1263 | Validation loss: 0.1716 -- 102200 / 235102 in 9.09s\n",
            "Training loss: 0.1238 | Validation loss: 0.1653 -- 104200 / 235102 in 13.69s\n",
            "Training loss: 0.2053 | Validation loss: 0.1644 -- 106200 / 235102 in 18.37s\n",
            "Training loss: 0.1273 | Validation loss: 0.1767 -- 108200 / 235102 in 22.97s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1180 | Validation loss: 0.1709 -- 110200 / 235102 in 4.64s\n",
            "Training loss: 0.1249 | Validation loss: 0.1546 -- 112200 / 235102 in 9.20s\n",
            "Training loss: 0.1298 | Validation loss: 0.1695 -- 114200 / 235102 in 13.78s\n",
            "Training loss: 0.1005 | Validation loss: 0.1629 -- 116200 / 235102 in 18.39s\n",
            "Training loss: 0.1286 | Validation loss: 0.1723 -- 118200 / 235102 in 22.93s\n",
            "Preparing 10000 data...Done 2.85s.\n",
            "Training...\n",
            "Training loss: 0.1342 | Validation loss: 0.1606 -- 120200 / 235102 in 4.63s\n",
            "Training loss: 0.1764 | Validation loss: 0.1661 -- 122200 / 235102 in 9.31s\n",
            "Training loss: 0.1150 | Validation loss: 0.1779 -- 124200 / 235102 in 13.96s\n",
            "Training loss: 0.1693 | Validation loss: 0.1734 -- 126200 / 235102 in 18.65s\n",
            "Training loss: 0.1408 | Validation loss: 0.1599 -- 128200 / 235102 in 23.26s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.0854 | Validation loss: 0.1704 -- 130200 / 235102 in 4.61s\n",
            "Training loss: 0.0965 | Validation loss: 0.1574 -- 132200 / 235102 in 9.19s\n",
            "Training loss: 0.1162 | Validation loss: 0.1647 -- 134200 / 235102 in 13.65s\n",
            "Training loss: 0.1177 | Validation loss: 0.1540 -- 136200 / 235102 in 18.32s\n",
            "Training loss: 0.1330 | Validation loss: 0.1722 -- 138200 / 235102 in 22.82s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1344 | Validation loss: 0.1546 -- 140200 / 235102 in 4.58s\n",
            "Training loss: 0.0996 | Validation loss: 0.1478 -- 142200 / 235102 in 9.28s\n",
            "Training loss: 0.1323 | Validation loss: 0.1554 -- 144200 / 235102 in 13.89s\n",
            "Training loss: 0.1055 | Validation loss: 0.1357 -- 146200 / 235102 in 18.43s\n",
            "Training loss: 0.1283 | Validation loss: 0.1686 -- 148200 / 235102 in 23.14s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.2135 | Validation loss: 0.1577 -- 150200 / 235102 in 4.57s\n",
            "Training loss: 0.1386 | Validation loss: 0.1680 -- 152200 / 235102 in 9.28s\n",
            "Training loss: 0.1352 | Validation loss: 0.1537 -- 154200 / 235102 in 14.00s\n",
            "Training loss: 0.1037 | Validation loss: 0.1769 -- 156200 / 235102 in 18.70s\n",
            "Training loss: 0.1092 | Validation loss: 0.1699 -- 158200 / 235102 in 23.40s\n",
            "Preparing 10000 data...Done 2.85s.\n",
            "Training...\n",
            "Training loss: 0.0905 | Validation loss: 0.1558 -- 160200 / 235102 in 4.58s\n",
            "Training loss: 0.0916 | Validation loss: 0.1584 -- 162200 / 235102 in 9.13s\n",
            "Training loss: 0.0957 | Validation loss: 0.1459 -- 164200 / 235102 in 13.79s\n",
            "Training loss: 0.0862 | Validation loss: 0.1584 -- 166200 / 235102 in 18.47s\n",
            "Training loss: 0.1365 | Validation loss: 0.1455 -- 168200 / 235102 in 23.13s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1208 | Validation loss: 0.1812 -- 170200 / 235102 in 4.55s\n",
            "Training loss: 0.1531 | Validation loss: 0.1456 -- 172200 / 235102 in 9.27s\n",
            "Training loss: 0.1222 | Validation loss: 0.1651 -- 174200 / 235102 in 13.96s\n",
            "Training loss: 0.1300 | Validation loss: 0.1385 -- 176200 / 235102 in 18.73s\n",
            "Training loss: 0.1196 | Validation loss: 0.1534 -- 178200 / 235102 in 23.41s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1434 | Validation loss: 0.1795 -- 180200 / 235102 in 4.57s\n",
            "Training loss: 0.0848 | Validation loss: 0.1806 -- 182200 / 235102 in 9.28s\n",
            "Training loss: 0.1634 | Validation loss: 0.1588 -- 184200 / 235102 in 13.99s\n",
            "Training loss: 0.1003 | Validation loss: 0.1781 -- 186200 / 235102 in 18.67s\n",
            "Training loss: 0.1151 | Validation loss: 0.1699 -- 188200 / 235102 in 23.39s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.1076 | Validation loss: 0.1532 -- 190200 / 235102 in 4.62s\n",
            "Training loss: 0.1079 | Validation loss: 0.1702 -- 192200 / 235102 in 9.27s\n",
            "Training loss: 0.1058 | Validation loss: 0.1700 -- 194200 / 235102 in 13.99s\n",
            "Training loss: 0.1364 | Validation loss: 0.1679 -- 196200 / 235102 in 18.73s\n",
            "Training loss: 0.1882 | Validation loss: 0.1615 -- 198200 / 235102 in 23.43s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1223 | Validation loss: 0.1500 -- 200200 / 235102 in 4.62s\n",
            "Training loss: 0.1405 | Validation loss: 0.1603 -- 202200 / 235102 in 9.33s\n",
            "Training loss: 0.1215 | Validation loss: 0.1660 -- 204200 / 235102 in 14.04s\n",
            "Training loss: 0.0960 | Validation loss: 0.1679 -- 206200 / 235102 in 18.66s\n",
            "Training loss: 0.1058 | Validation loss: 0.1546 -- 208200 / 235102 in 23.26s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1989 | Validation loss: 0.1647 -- 210200 / 235102 in 4.60s\n",
            "Training loss: 0.1091 | Validation loss: 0.1549 -- 212200 / 235102 in 9.34s\n",
            "Training loss: 0.1326 | Validation loss: 0.1518 -- 214200 / 235102 in 14.05s\n",
            "Training loss: 0.1479 | Validation loss: 0.1749 -- 216200 / 235102 in 18.71s\n",
            "Training loss: 0.0815 | Validation loss: 0.1660 -- 218200 / 235102 in 23.35s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1645 | Validation loss: 0.1571 -- 220200 / 235102 in 4.67s\n",
            "Training loss: 0.0790 | Validation loss: 0.1644 -- 222200 / 235102 in 9.34s\n",
            "Training loss: 0.0819 | Validation loss: 0.1504 -- 224200 / 235102 in 14.09s\n",
            "Training loss: 0.1746 | Validation loss: 0.1718 -- 226200 / 235102 in 18.82s\n",
            "Training loss: 0.0841 | Validation loss: 0.1554 -- 228200 / 235102 in 23.59s\n",
            "Preparing 5102 data...Done 2.88s.\n",
            "Training...Done.\n",
            "Epoch n°4: TRAIN LOSS 0.1283 | VAL LOSS 0.1655 TOTAL TIME: 621.53s.\n",
            "Testing on 10000...Done. 8.50s\n",
            "acc avg: 0.951, acc 0: 0.985, acc 1: 0.436, recall: 0.436,  precision: 0.655\n",
            "F1 Score: 0.5233463035019454\n",
            "______________________________________________________________________\n",
            "Epoch : 5\n",
            "Preparing 10000 data...Done 2.97s.\n",
            "Training...\n",
            "Training loss: 0.1722 | Validation loss: 0.1740 -- 200 / 235102 in 4.67s\n",
            "Training loss: 0.1400 | Validation loss: 0.1494 -- 2200 / 235102 in 9.35s\n",
            "Training loss: 0.0682 | Validation loss: 0.1713 -- 4200 / 235102 in 13.94s\n",
            "Training loss: 0.1291 | Validation loss: 0.1734 -- 6200 / 235102 in 18.54s\n",
            "Training loss: 0.1708 | Validation loss: 0.1574 -- 8200 / 235102 in 23.24s\n",
            "Preparing 10000 data...Done 2.98s.\n",
            "Training...\n",
            "Training loss: 0.1004 | Validation loss: 0.1677 -- 10200 / 235102 in 4.69s\n",
            "Training loss: 0.1280 | Validation loss: 0.1293 -- 12200 / 235102 in 9.40s\n",
            "Training loss: 0.1355 | Validation loss: 0.1865 -- 14200 / 235102 in 14.12s\n",
            "Training loss: 0.1519 | Validation loss: 0.1799 -- 16200 / 235102 in 18.84s\n",
            "Training loss: 0.1502 | Validation loss: 0.1416 -- 18200 / 235102 in 23.57s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1256 | Validation loss: 0.1566 -- 20200 / 235102 in 4.62s\n",
            "Training loss: 0.1615 | Validation loss: 0.1436 -- 22200 / 235102 in 9.30s\n",
            "Training loss: 0.1036 | Validation loss: 0.1595 -- 24200 / 235102 in 14.14s\n",
            "Training loss: 0.2122 | Validation loss: 0.1334 -- 26200 / 235102 in 18.88s\n",
            "Training loss: 0.1295 | Validation loss: 0.1466 -- 28200 / 235102 in 23.47s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1227 | Validation loss: 0.1651 -- 30200 / 235102 in 4.66s\n",
            "Training loss: 0.0767 | Validation loss: 0.1561 -- 32200 / 235102 in 9.37s\n",
            "Training loss: 0.0793 | Validation loss: 0.1531 -- 34200 / 235102 in 14.14s\n",
            "Training loss: 0.1609 | Validation loss: 0.1806 -- 36200 / 235102 in 19.09s\n",
            "Training loss: 0.1337 | Validation loss: 0.1487 -- 38200 / 235102 in 23.85s\n",
            "Preparing 10000 data...Done 2.83s.\n",
            "Training...\n",
            "Training loss: 0.1311 | Validation loss: 0.1702 -- 40200 / 235102 in 4.52s\n",
            "Training loss: 0.1923 | Validation loss: 0.1500 -- 42200 / 235102 in 9.24s\n",
            "Training loss: 0.1425 | Validation loss: 0.1692 -- 44200 / 235102 in 13.88s\n",
            "Training loss: 0.0822 | Validation loss: 0.1715 -- 46200 / 235102 in 18.50s\n",
            "Training loss: 0.1236 | Validation loss: 0.1607 -- 48200 / 235102 in 23.22s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1479 | Validation loss: 0.1714 -- 50200 / 235102 in 4.60s\n",
            "Training loss: 0.1769 | Validation loss: 0.1588 -- 52200 / 235102 in 9.31s\n",
            "Training loss: 0.1854 | Validation loss: 0.1584 -- 54200 / 235102 in 13.90s\n",
            "Training loss: 0.1348 | Validation loss: 0.1724 -- 56200 / 235102 in 18.56s\n",
            "Training loss: 0.1500 | Validation loss: 0.1649 -- 58200 / 235102 in 23.20s\n",
            "Preparing 10000 data...Done 2.81s.\n",
            "Training...\n",
            "Training loss: 0.1006 | Validation loss: 0.1595 -- 60200 / 235102 in 4.49s\n",
            "Training loss: 0.1246 | Validation loss: 0.1673 -- 62200 / 235102 in 9.18s\n",
            "Training loss: 0.1220 | Validation loss: 0.1666 -- 64200 / 235102 in 13.93s\n",
            "Training loss: 0.1047 | Validation loss: 0.1754 -- 66200 / 235102 in 18.70s\n",
            "Training loss: 0.1434 | Validation loss: 0.1635 -- 68200 / 235102 in 23.37s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1150 | Validation loss: 0.1543 -- 70200 / 235102 in 4.64s\n",
            "Training loss: 0.1492 | Validation loss: 0.1710 -- 72200 / 235102 in 9.50s\n",
            "Training loss: 0.1393 | Validation loss: 0.1473 -- 74200 / 235102 in 14.37s\n",
            "Training loss: 0.1435 | Validation loss: 0.1711 -- 76200 / 235102 in 19.16s\n",
            "Training loss: 0.1597 | Validation loss: 0.1333 -- 78200 / 235102 in 23.95s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1780 | Validation loss: 0.1842 -- 80200 / 235102 in 4.69s\n",
            "Training loss: 0.1663 | Validation loss: 0.1546 -- 82200 / 235102 in 9.33s\n",
            "Training loss: 0.1354 | Validation loss: 0.1450 -- 84200 / 235102 in 14.06s\n",
            "Training loss: 0.1147 | Validation loss: 0.1628 -- 86200 / 235102 in 18.81s\n",
            "Training loss: 0.0663 | Validation loss: 0.1412 -- 88200 / 235102 in 23.61s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1141 | Validation loss: 0.1614 -- 90200 / 235102 in 4.65s\n",
            "Training loss: 0.1851 | Validation loss: 0.1517 -- 92200 / 235102 in 9.45s\n",
            "Training loss: 0.1496 | Validation loss: 0.1668 -- 94200 / 235102 in 14.24s\n",
            "Training loss: 0.1437 | Validation loss: 0.1561 -- 96200 / 235102 in 19.10s\n",
            "Training loss: 0.1006 | Validation loss: 0.1559 -- 98200 / 235102 in 23.76s\n",
            "Preparing 10000 data...Done 2.85s.\n",
            "Training...\n",
            "Training loss: 0.1629 | Validation loss: 0.1841 -- 100200 / 235102 in 4.64s\n",
            "Training loss: 0.0917 | Validation loss: 0.1480 -- 102200 / 235102 in 9.43s\n",
            "Training loss: 0.1665 | Validation loss: 0.1800 -- 104200 / 235102 in 14.22s\n",
            "Training loss: 0.1269 | Validation loss: 0.1421 -- 106200 / 235102 in 19.00s\n",
            "Training loss: 0.1451 | Validation loss: 0.1887 -- 108200 / 235102 in 23.77s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1317 | Validation loss: 0.1331 -- 110200 / 235102 in 4.67s\n",
            "Training loss: 0.1241 | Validation loss: 0.1673 -- 112200 / 235102 in 9.38s\n",
            "Training loss: 0.1691 | Validation loss: 0.1508 -- 114200 / 235102 in 14.20s\n",
            "Training loss: 0.1154 | Validation loss: 0.1543 -- 116200 / 235102 in 18.97s\n",
            "Training loss: 0.0960 | Validation loss: 0.1507 -- 118200 / 235102 in 23.72s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1254 | Validation loss: 0.1743 -- 120200 / 235102 in 4.78s\n",
            "Training loss: 0.1780 | Validation loss: 0.1625 -- 122200 / 235102 in 9.59s\n",
            "Training loss: 0.0976 | Validation loss: 0.1639 -- 124200 / 235102 in 14.37s\n",
            "Training loss: 0.1158 | Validation loss: 0.1646 -- 126200 / 235102 in 19.09s\n",
            "Training loss: 0.0946 | Validation loss: 0.1473 -- 128200 / 235102 in 23.86s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1417 | Validation loss: 0.1656 -- 130200 / 235102 in 4.68s\n",
            "Training loss: 0.1008 | Validation loss: 0.1578 -- 132200 / 235102 in 9.51s\n",
            "Training loss: 0.1721 | Validation loss: 0.1498 -- 134200 / 235102 in 14.34s\n",
            "Training loss: 0.1063 | Validation loss: 0.1436 -- 136200 / 235102 in 19.13s\n",
            "Training loss: 0.1160 | Validation loss: 0.1430 -- 138200 / 235102 in 23.82s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1208 | Validation loss: 0.1416 -- 140200 / 235102 in 4.67s\n",
            "Training loss: 0.1326 | Validation loss: 0.1511 -- 142200 / 235102 in 9.58s\n",
            "Training loss: 0.0823 | Validation loss: 0.1427 -- 144200 / 235102 in 14.44s\n",
            "Training loss: 0.1375 | Validation loss: 0.1496 -- 146200 / 235102 in 19.24s\n",
            "Training loss: 0.1219 | Validation loss: 0.1432 -- 148200 / 235102 in 24.13s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1305 | Validation loss: 0.1538 -- 150200 / 235102 in 4.62s\n",
            "Training loss: 0.1164 | Validation loss: 0.1483 -- 152200 / 235102 in 9.37s\n",
            "Training loss: 0.1400 | Validation loss: 0.1554 -- 154200 / 235102 in 14.17s\n",
            "Training loss: 0.1040 | Validation loss: 0.1546 -- 156200 / 235102 in 19.04s\n",
            "Training loss: 0.1492 | Validation loss: 0.1513 -- 158200 / 235102 in 23.83s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1133 | Validation loss: 0.1576 -- 160200 / 235102 in 4.68s\n",
            "Training loss: 0.0640 | Validation loss: 0.1419 -- 162200 / 235102 in 9.40s\n",
            "Training loss: 0.1385 | Validation loss: 0.1360 -- 164200 / 235102 in 14.23s\n",
            "Training loss: 0.0944 | Validation loss: 0.1537 -- 166200 / 235102 in 19.02s\n",
            "Training loss: 0.1793 | Validation loss: 0.1368 -- 168200 / 235102 in 23.86s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1313 | Validation loss: 0.1507 -- 170200 / 235102 in 4.70s\n",
            "Training loss: 0.0817 | Validation loss: 0.1528 -- 172200 / 235102 in 9.53s\n",
            "Training loss: 0.1362 | Validation loss: 0.1451 -- 174200 / 235102 in 14.38s\n",
            "Training loss: 0.1492 | Validation loss: 0.1537 -- 176200 / 235102 in 19.14s\n",
            "Training loss: 0.1191 | Validation loss: 0.1421 -- 178200 / 235102 in 23.78s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1429 | Validation loss: 0.1692 -- 180200 / 235102 in 4.64s\n",
            "Training loss: 0.0760 | Validation loss: 0.1669 -- 182200 / 235102 in 9.38s\n",
            "Training loss: 0.1545 | Validation loss: 0.1572 -- 184200 / 235102 in 14.14s\n",
            "Training loss: 0.0927 | Validation loss: 0.1573 -- 186200 / 235102 in 18.84s\n",
            "Training loss: 0.1288 | Validation loss: 0.1545 -- 188200 / 235102 in 23.49s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.1517 | Validation loss: 0.1686 -- 190200 / 235102 in 4.69s\n",
            "Training loss: 0.0996 | Validation loss: 0.1493 -- 192200 / 235102 in 9.53s\n",
            "Training loss: 0.1164 | Validation loss: 0.1547 -- 194200 / 235102 in 14.28s\n",
            "Training loss: 0.1204 | Validation loss: 0.1612 -- 196200 / 235102 in 19.09s\n",
            "Training loss: 0.1189 | Validation loss: 0.1450 -- 198200 / 235102 in 23.84s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1339 | Validation loss: 0.1752 -- 200200 / 235102 in 4.61s\n",
            "Training loss: 0.0908 | Validation loss: 0.1347 -- 202200 / 235102 in 9.34s\n",
            "Training loss: 0.1398 | Validation loss: 0.1638 -- 204200 / 235102 in 14.00s\n",
            "Training loss: 0.1295 | Validation loss: 0.1330 -- 206200 / 235102 in 18.91s\n",
            "Training loss: 0.1119 | Validation loss: 0.1800 -- 208200 / 235102 in 23.63s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1399 | Validation loss: 0.1509 -- 210200 / 235102 in 4.67s\n",
            "Training loss: 0.1481 | Validation loss: 0.1600 -- 212200 / 235102 in 9.48s\n",
            "Training loss: 0.0912 | Validation loss: 0.1377 -- 214200 / 235102 in 14.28s\n",
            "Training loss: 0.0944 | Validation loss: 0.1751 -- 216200 / 235102 in 19.11s\n",
            "Training loss: 0.0903 | Validation loss: 0.1426 -- 218200 / 235102 in 23.89s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1021 | Validation loss: 0.1515 -- 220200 / 235102 in 4.68s\n",
            "Training loss: 0.1047 | Validation loss: 0.1436 -- 222200 / 235102 in 9.50s\n",
            "Training loss: 0.1274 | Validation loss: 0.1566 -- 224200 / 235102 in 14.34s\n",
            "Training loss: 0.1526 | Validation loss: 0.1627 -- 226200 / 235102 in 19.14s\n",
            "Training loss: 0.1138 | Validation loss: 0.1489 -- 228200 / 235102 in 23.99s\n",
            "Preparing 5102 data...Done 2.85s.\n",
            "Training...Done.\n",
            "Epoch n°5: TRAIN LOSS 0.1260 | VAL LOSS 0.1569 TOTAL TIME: 633.09s.\n",
            "Testing on 10000...Done. 8.69s\n",
            "acc avg: 0.957, acc 0: 0.979, acc 1: 0.558, recall: 0.558,  precision: 0.600\n",
            "F1 Score: 0.5780795344325897\n",
            "______________________________________________________________________\n",
            "Epoch : 6\n",
            "Preparing 10000 data...Done 2.96s.\n",
            "Training...\n",
            "Training loss: 0.1192 | Validation loss: 0.1614 -- 200 / 235102 in 4.77s\n",
            "Training loss: 0.1141 | Validation loss: 0.1617 -- 2200 / 235102 in 9.64s\n",
            "Training loss: 0.1038 | Validation loss: 0.1565 -- 4200 / 235102 in 14.46s\n",
            "Training loss: 0.1129 | Validation loss: 0.1687 -- 6200 / 235102 in 19.33s\n",
            "Training loss: 0.1226 | Validation loss: 0.1516 -- 8200 / 235102 in 24.20s\n",
            "Preparing 10000 data...Done 2.96s.\n",
            "Training...\n",
            "Training loss: 0.1226 | Validation loss: 0.1580 -- 10200 / 235102 in 4.70s\n",
            "Training loss: 0.1633 | Validation loss: 0.1496 -- 12200 / 235102 in 9.41s\n",
            "Training loss: 0.1414 | Validation loss: 0.1519 -- 14200 / 235102 in 14.30s\n",
            "Training loss: 0.1568 | Validation loss: 0.1454 -- 16200 / 235102 in 19.13s\n",
            "Training loss: 0.0847 | Validation loss: 0.1522 -- 18200 / 235102 in 23.96s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.1035 | Validation loss: 0.1273 -- 20200 / 235102 in 4.63s\n",
            "Training loss: 0.1456 | Validation loss: 0.1486 -- 22200 / 235102 in 9.45s\n",
            "Training loss: 0.1232 | Validation loss: 0.1452 -- 24200 / 235102 in 14.37s\n",
            "Training loss: 0.0983 | Validation loss: 0.1277 -- 26200 / 235102 in 19.21s\n",
            "Training loss: 0.1227 | Validation loss: 0.1412 -- 28200 / 235102 in 24.06s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.2042 | Validation loss: 0.1522 -- 30200 / 235102 in 4.71s\n",
            "Training loss: 0.0976 | Validation loss: 0.1504 -- 32200 / 235102 in 9.57s\n",
            "Training loss: 0.1364 | Validation loss: 0.1522 -- 34200 / 235102 in 14.33s\n",
            "Training loss: 0.1789 | Validation loss: 0.1524 -- 36200 / 235102 in 19.00s\n",
            "Training loss: 0.1402 | Validation loss: 0.1520 -- 38200 / 235102 in 23.96s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.0901 | Validation loss: 0.1445 -- 40200 / 235102 in 4.74s\n",
            "Training loss: 0.1053 | Validation loss: 0.1615 -- 42200 / 235102 in 9.65s\n",
            "Training loss: 0.1543 | Validation loss: 0.1614 -- 44200 / 235102 in 14.57s\n",
            "Training loss: 0.0892 | Validation loss: 0.1443 -- 46200 / 235102 in 19.34s\n",
            "Training loss: 0.1392 | Validation loss: 0.1622 -- 48200 / 235102 in 24.10s\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.1292 | Validation loss: 0.1652 -- 50200 / 235102 in 4.77s\n",
            "Training loss: 0.1301 | Validation loss: 0.1470 -- 52200 / 235102 in 9.57s\n",
            "Training loss: 0.1482 | Validation loss: 0.1486 -- 54200 / 235102 in 14.37s\n",
            "Training loss: 0.0931 | Validation loss: 0.1566 -- 56200 / 235102 in 19.28s\n",
            "Training loss: 0.1036 | Validation loss: 0.1646 -- 58200 / 235102 in 24.21s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.0900 | Validation loss: 0.1486 -- 60200 / 235102 in 4.75s\n",
            "Training loss: 0.1276 | Validation loss: 0.1769 -- 62200 / 235102 in 9.70s\n",
            "Training loss: 0.1351 | Validation loss: 0.1430 -- 64200 / 235102 in 14.51s\n",
            "Training loss: 0.1598 | Validation loss: 0.1714 -- 66200 / 235102 in 19.37s\n",
            "Training loss: 0.0999 | Validation loss: 0.1574 -- 68200 / 235102 in 24.29s\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.0883 | Validation loss: 0.1561 -- 70200 / 235102 in 4.77s\n",
            "Training loss: 0.1197 | Validation loss: 0.1525 -- 72200 / 235102 in 9.68s\n",
            "Training loss: 0.2021 | Validation loss: 0.1516 -- 74200 / 235102 in 14.73s\n",
            "Training loss: 0.1124 | Validation loss: 0.1517 -- 76200 / 235102 in 19.64s\n",
            "Training loss: 0.0526 | Validation loss: 0.1384 -- 78200 / 235102 in 24.50s\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.1265 | Validation loss: 0.1473 -- 80200 / 235102 in 4.73s\n",
            "Training loss: 0.0963 | Validation loss: 0.1675 -- 82200 / 235102 in 9.63s\n",
            "Training loss: 0.1413 | Validation loss: 0.1377 -- 84200 / 235102 in 14.60s\n",
            "Training loss: 0.1424 | Validation loss: 0.1592 -- 86200 / 235102 in 19.53s\n",
            "Training loss: 0.1096 | Validation loss: 0.1487 -- 88200 / 235102 in 24.48s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.0752 | Validation loss: 0.1361 -- 90200 / 235102 in 4.72s\n",
            "Training loss: 0.0838 | Validation loss: 0.1653 -- 92200 / 235102 in 9.61s\n",
            "Training loss: 0.1441 | Validation loss: 0.1435 -- 94200 / 235102 in 14.59s\n",
            "Training loss: 0.0783 | Validation loss: 0.1507 -- 96200 / 235102 in 19.62s\n",
            "Training loss: 0.1724 | Validation loss: 0.1568 -- 98200 / 235102 in 24.61s\n",
            "Preparing 10000 data...Done 2.82s.\n",
            "Training...\n",
            "Training loss: 0.1088 | Validation loss: 0.1440 -- 100200 / 235102 in 4.63s\n",
            "Training loss: 0.1442 | Validation loss: 0.1445 -- 102200 / 235102 in 9.36s\n",
            "Training loss: 0.1121 | Validation loss: 0.1462 -- 104200 / 235102 in 14.15s\n",
            "Training loss: 0.1237 | Validation loss: 0.1579 -- 106200 / 235102 in 19.07s\n",
            "Training loss: 0.1342 | Validation loss: 0.1549 -- 108200 / 235102 in 23.98s\n",
            "Preparing 10000 data...Done 2.83s.\n",
            "Training...\n",
            "Training loss: 0.1299 | Validation loss: 0.1527 -- 110200 / 235102 in 4.74s\n",
            "Training loss: 0.1316 | Validation loss: 0.1457 -- 112200 / 235102 in 9.69s\n",
            "Training loss: 0.1281 | Validation loss: 0.1456 -- 114200 / 235102 in 14.43s\n",
            "Training loss: 0.0826 | Validation loss: 0.1449 -- 116200 / 235102 in 19.19s\n",
            "Training loss: 0.1073 | Validation loss: 0.1484 -- 118200 / 235102 in 24.03s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1542 | Validation loss: 0.1544 -- 120200 / 235102 in 4.84s\n",
            "Training loss: 0.1389 | Validation loss: 0.1610 -- 122200 / 235102 in 9.67s\n",
            "Training loss: 0.1008 | Validation loss: 0.1422 -- 124200 / 235102 in 14.56s\n",
            "Training loss: 0.0836 | Validation loss: 0.1655 -- 126200 / 235102 in 19.46s\n",
            "Training loss: 0.1165 | Validation loss: 0.1448 -- 128200 / 235102 in 24.45s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.0802 | Validation loss: 0.1570 -- 130200 / 235102 in 4.77s\n",
            "Training loss: 0.1459 | Validation loss: 0.1507 -- 132200 / 235102 in 9.68s\n",
            "Training loss: 0.1514 | Validation loss: 0.1454 -- 134200 / 235102 in 14.49s\n",
            "Training loss: 0.1693 | Validation loss: 0.1362 -- 136200 / 235102 in 19.38s\n",
            "Training loss: 0.0881 | Validation loss: 0.1407 -- 138200 / 235102 in 24.21s\n",
            "Preparing 10000 data...Done 2.79s.\n",
            "Training...\n",
            "Training loss: 0.0963 | Validation loss: 0.1407 -- 140200 / 235102 in 4.65s\n",
            "Training loss: 0.1274 | Validation loss: 0.1345 -- 142200 / 235102 in 9.47s\n",
            "Training loss: 0.1383 | Validation loss: 0.1289 -- 144200 / 235102 in 14.32s\n",
            "Training loss: 0.1313 | Validation loss: 0.1368 -- 146200 / 235102 in 19.20s\n",
            "Training loss: 0.1116 | Validation loss: 0.1329 -- 148200 / 235102 in 24.10s\n",
            "Preparing 10000 data...Done 2.81s.\n",
            "Training...\n",
            "Training loss: 0.1420 | Validation loss: 0.1663 -- 150200 / 235102 in 4.67s\n",
            "Training loss: 0.1396 | Validation loss: 0.1376 -- 152200 / 235102 in 9.61s\n",
            "Training loss: 0.1406 | Validation loss: 0.1581 -- 154200 / 235102 in 14.45s\n",
            "Training loss: 0.1511 | Validation loss: 0.1438 -- 156200 / 235102 in 19.26s\n",
            "Training loss: 0.1723 | Validation loss: 0.1493 -- 158200 / 235102 in 24.10s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.2221 | Validation loss: 0.1316 -- 160200 / 235102 in 4.86s\n",
            "Training loss: 0.1144 | Validation loss: 0.1472 -- 162200 / 235102 in 9.73s\n",
            "Training loss: 0.1292 | Validation loss: 0.1308 -- 164200 / 235102 in 14.73s\n",
            "Training loss: 0.1213 | Validation loss: 0.1314 -- 166200 / 235102 in 19.78s\n",
            "Training loss: 0.1400 | Validation loss: 0.1503 -- 168200 / 235102 in 24.75s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1186 | Validation loss: 0.1384 -- 170200 / 235102 in 4.86s\n",
            "Training loss: 0.1416 | Validation loss: 0.1511 -- 172200 / 235102 in 9.77s\n",
            "Training loss: 0.0822 | Validation loss: 0.1345 -- 174200 / 235102 in 14.67s\n",
            "Training loss: 0.1195 | Validation loss: 0.1373 -- 176200 / 235102 in 19.75s\n",
            "Training loss: 0.1378 | Validation loss: 0.1483 -- 178200 / 235102 in 24.74s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1397 | Validation loss: 0.1532 -- 180200 / 235102 in 4.79s\n",
            "Training loss: 0.1429 | Validation loss: 0.1548 -- 182200 / 235102 in 9.72s\n",
            "Training loss: 0.0895 | Validation loss: 0.1582 -- 184200 / 235102 in 14.80s\n",
            "Training loss: 0.0857 | Validation loss: 0.1570 -- 186200 / 235102 in 19.87s\n",
            "Training loss: 0.0696 | Validation loss: 0.1524 -- 188200 / 235102 in 24.88s\n",
            "Preparing 10000 data...Done 2.99s.\n",
            "Training...\n",
            "Training loss: 0.1056 | Validation loss: 0.1796 -- 190200 / 235102 in 4.92s\n",
            "Training loss: 0.1083 | Validation loss: 0.1398 -- 192200 / 235102 in 9.84s\n",
            "Training loss: 0.1360 | Validation loss: 0.1604 -- 194200 / 235102 in 14.84s\n",
            "Training loss: 0.1348 | Validation loss: 0.1426 -- 196200 / 235102 in 19.93s\n",
            "Training loss: 0.1087 | Validation loss: 0.1630 -- 198200 / 235102 in 24.98s\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.1289 | Validation loss: 0.1288 -- 200200 / 235102 in 4.89s\n",
            "Training loss: 0.1612 | Validation loss: 0.1538 -- 202200 / 235102 in 9.97s\n",
            "Training loss: 0.1282 | Validation loss: 0.1451 -- 204200 / 235102 in 15.04s\n",
            "Training loss: 0.1088 | Validation loss: 0.1621 -- 206200 / 235102 in 20.10s\n",
            "Training loss: 0.1458 | Validation loss: 0.1416 -- 208200 / 235102 in 25.06s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1713 | Validation loss: 0.1447 -- 210200 / 235102 in 4.85s\n",
            "Training loss: 0.0897 | Validation loss: 0.1521 -- 212200 / 235102 in 9.92s\n",
            "Training loss: 0.0987 | Validation loss: 0.1347 -- 214200 / 235102 in 14.87s\n",
            "Training loss: 0.1374 | Validation loss: 0.1483 -- 216200 / 235102 in 19.80s\n",
            "Training loss: 0.1082 | Validation loss: 0.1417 -- 218200 / 235102 in 24.86s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1020 | Validation loss: 0.1392 -- 220200 / 235102 in 4.88s\n",
            "Training loss: 0.1311 | Validation loss: 0.1418 -- 222200 / 235102 in 9.87s\n",
            "Training loss: 0.0971 | Validation loss: 0.1457 -- 224200 / 235102 in 14.84s\n",
            "Training loss: 0.1536 | Validation loss: 0.1354 -- 226200 / 235102 in 19.80s\n",
            "Training loss: 0.1327 | Validation loss: 0.1547 -- 228200 / 235102 in 24.86s\n",
            "Preparing 5102 data...Done 2.84s.\n",
            "Training...Done.\n",
            "Epoch n°6: TRAIN LOSS 0.1234 | VAL LOSS 0.1493 TOTAL TIME: 651.92s.\n",
            "Testing on 10000...Done. 9.35s\n",
            "acc avg: 0.955, acc 0: 0.978, acc 1: 0.565, recall: 0.565,  precision: 0.612\n",
            "F1 Score: 0.5872727272727273\n",
            "______________________________________________________________________\n",
            "Epoch : 7\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.1213 | Validation loss: 0.1472 -- 200 / 235102 in 4.85s\n",
            "Training loss: 0.1362 | Validation loss: 0.1455 -- 2200 / 235102 in 10.01s\n",
            "Training loss: 0.0887 | Validation loss: 0.1521 -- 4200 / 235102 in 15.20s\n",
            "Training loss: 0.1027 | Validation loss: 0.1533 -- 6200 / 235102 in 20.39s\n",
            "Training loss: 0.1539 | Validation loss: 0.1604 -- 8200 / 235102 in 25.56s\n",
            "Preparing 10000 data...Done 3.04s.\n",
            "Training...\n",
            "Training loss: 0.1167 | Validation loss: 0.1396 -- 10200 / 235102 in 5.05s\n",
            "Training loss: 0.1675 | Validation loss: 0.1444 -- 12200 / 235102 in 10.10s\n",
            "Training loss: 0.1259 | Validation loss: 0.1466 -- 14200 / 235102 in 15.14s\n",
            "Training loss: 0.1334 | Validation loss: 0.1475 -- 16200 / 235102 in 20.13s\n",
            "Training loss: 0.1347 | Validation loss: 0.1363 -- 18200 / 235102 in 25.29s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.0815 | Validation loss: 0.1346 -- 20200 / 235102 in 4.93s\n",
            "Training loss: 0.0936 | Validation loss: 0.1374 -- 22200 / 235102 in 10.13s\n",
            "Training loss: 0.1547 | Validation loss: 0.1285 -- 24200 / 235102 in 15.24s\n",
            "Training loss: 0.1227 | Validation loss: 0.1362 -- 26200 / 235102 in 20.39s\n",
            "Training loss: 0.1192 | Validation loss: 0.1273 -- 28200 / 235102 in 25.57s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.1156 | Validation loss: 0.1487 -- 30200 / 235102 in 4.92s\n",
            "Training loss: 0.1051 | Validation loss: 0.1374 -- 32200 / 235102 in 10.11s\n",
            "Training loss: 0.1477 | Validation loss: 0.1422 -- 34200 / 235102 in 15.27s\n",
            "Training loss: 0.1485 | Validation loss: 0.1529 -- 36200 / 235102 in 20.39s\n",
            "Training loss: 0.1529 | Validation loss: 0.1363 -- 38200 / 235102 in 25.48s\n",
            "Preparing 10000 data...Done 2.94s.\n",
            "Training...\n",
            "Training loss: 0.0851 | Validation loss: 0.1479 -- 40200 / 235102 in 4.94s\n",
            "Training loss: 0.1325 | Validation loss: 0.1605 -- 42200 / 235102 in 10.11s\n",
            "Training loss: 0.1400 | Validation loss: 0.1531 -- 44200 / 235102 in 15.25s\n",
            "Training loss: 0.1499 | Validation loss: 0.1488 -- 46200 / 235102 in 20.39s\n",
            "Training loss: 0.1204 | Validation loss: 0.1465 -- 48200 / 235102 in 25.46s\n",
            "Preparing 10000 data...Done 2.94s.\n",
            "Training...\n",
            "Training loss: 0.1455 | Validation loss: 0.1481 -- 50200 / 235102 in 4.94s\n",
            "Training loss: 0.0811 | Validation loss: 0.1585 -- 52200 / 235102 in 10.15s\n",
            "Training loss: 0.1352 | Validation loss: 0.1437 -- 54200 / 235102 in 15.32s\n",
            "Training loss: 0.1235 | Validation loss: 0.1535 -- 56200 / 235102 in 20.47s\n",
            "Training loss: 0.1177 | Validation loss: 0.1438 -- 58200 / 235102 in 25.60s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1043 | Validation loss: 0.1593 -- 60200 / 235102 in 4.89s\n",
            "Training loss: 0.0817 | Validation loss: 0.1612 -- 62200 / 235102 in 10.11s\n",
            "Training loss: 0.1681 | Validation loss: 0.1577 -- 64200 / 235102 in 15.20s\n",
            "Training loss: 0.1494 | Validation loss: 0.1550 -- 66200 / 235102 in 20.30s\n",
            "Training loss: 0.1480 | Validation loss: 0.1479 -- 68200 / 235102 in 25.45s\n",
            "Preparing 10000 data...Done 2.96s.\n",
            "Training...\n",
            "Training loss: 0.1077 | Validation loss: 0.1475 -- 70200 / 235102 in 4.97s\n",
            "Training loss: 0.1302 | Validation loss: 0.1391 -- 72200 / 235102 in 10.12s\n",
            "Training loss: 0.1544 | Validation loss: 0.1485 -- 74200 / 235102 in 15.27s\n",
            "Training loss: 0.1716 | Validation loss: 0.1536 -- 76200 / 235102 in 20.35s\n",
            "Training loss: 0.0963 | Validation loss: 0.1367 -- 78200 / 235102 in 25.55s\n",
            "Preparing 10000 data...Done 2.95s.\n",
            "Training...\n",
            "Training loss: 0.1420 | Validation loss: 0.1528 -- 80200 / 235102 in 4.97s\n",
            "Training loss: 0.1356 | Validation loss: 0.1379 -- 82200 / 235102 in 10.19s\n",
            "Training loss: 0.0844 | Validation loss: 0.1459 -- 84200 / 235102 in 15.37s\n",
            "Training loss: 0.1069 | Validation loss: 0.1477 -- 86200 / 235102 in 20.49s\n",
            "Training loss: 0.1983 | Validation loss: 0.1402 -- 88200 / 235102 in 25.71s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.1551 | Validation loss: 0.1293 -- 90200 / 235102 in 4.96s\n",
            "Training loss: 0.1736 | Validation loss: 0.1509 -- 92200 / 235102 in 10.19s\n",
            "Training loss: 0.0644 | Validation loss: 0.1372 -- 94200 / 235102 in 15.40s\n",
            "Training loss: 0.1019 | Validation loss: 0.1541 -- 96200 / 235102 in 20.64s\n",
            "Training loss: 0.1532 | Validation loss: 0.1394 -- 98200 / 235102 in 25.79s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.0635 | Validation loss: 0.1517 -- 100200 / 235102 in 4.94s\n",
            "Training loss: 0.1416 | Validation loss: 0.1458 -- 102200 / 235102 in 10.15s\n",
            "Training loss: 0.0717 | Validation loss: 0.1443 -- 104200 / 235102 in 15.36s\n",
            "Training loss: 0.1461 | Validation loss: 0.1520 -- 106200 / 235102 in 20.52s\n",
            "Training loss: 0.1064 | Validation loss: 0.1507 -- 108200 / 235102 in 25.64s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.0784 | Validation loss: 0.1481 -- 110200 / 235102 in 4.98s\n",
            "Training loss: 0.1670 | Validation loss: 0.1433 -- 112200 / 235102 in 10.12s\n",
            "Training loss: 0.1394 | Validation loss: 0.1425 -- 114200 / 235102 in 15.25s\n",
            "Training loss: 0.0968 | Validation loss: 0.1414 -- 116200 / 235102 in 20.36s\n",
            "Training loss: 0.0944 | Validation loss: 0.1569 -- 118200 / 235102 in 25.53s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1064 | Validation loss: 0.1395 -- 120200 / 235102 in 4.96s\n",
            "Training loss: 0.1339 | Validation loss: 0.1590 -- 122200 / 235102 in 10.16s\n",
            "Training loss: 0.1110 | Validation loss: 0.1417 -- 124200 / 235102 in 15.37s\n",
            "Training loss: 0.1040 | Validation loss: 0.1556 -- 126200 / 235102 in 20.58s\n",
            "Training loss: 0.1064 | Validation loss: 0.1366 -- 128200 / 235102 in 25.67s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.0991 | Validation loss: 0.1556 -- 130200 / 235102 in 4.97s\n",
            "Training loss: 0.1477 | Validation loss: 0.1343 -- 132200 / 235102 in 10.23s\n",
            "Training loss: 0.1221 | Validation loss: 0.1474 -- 134200 / 235102 in 15.45s\n",
            "Training loss: 0.1508 | Validation loss: 0.1296 -- 136200 / 235102 in 20.68s\n",
            "Training loss: 0.0706 | Validation loss: 0.1371 -- 138200 / 235102 in 25.75s\n",
            "Preparing 10000 data...Done 2.83s.\n",
            "Training...\n",
            "Training loss: 0.1308 | Validation loss: 0.1338 -- 140200 / 235102 in 4.87s\n",
            "Training loss: 0.1039 | Validation loss: 0.1305 -- 142200 / 235102 in 10.06s\n",
            "Training loss: 0.0933 | Validation loss: 0.1340 -- 144200 / 235102 in 15.27s\n",
            "Training loss: 0.0878 | Validation loss: 0.1190 -- 146200 / 235102 in 20.50s\n",
            "Training loss: 0.1333 | Validation loss: 0.1542 -- 148200 / 235102 in 25.76s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.1122 | Validation loss: 0.1426 -- 150200 / 235102 in 4.88s\n",
            "Training loss: 0.1154 | Validation loss: 0.1370 -- 152200 / 235102 in 10.08s\n",
            "Training loss: 0.1407 | Validation loss: 0.1418 -- 154200 / 235102 in 15.36s\n",
            "Training loss: 0.1171 | Validation loss: 0.1422 -- 156200 / 235102 in 20.50s\n",
            "Training loss: 0.1663 | Validation loss: 0.1392 -- 158200 / 235102 in 25.72s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.0868 | Validation loss: 0.1488 -- 160200 / 235102 in 4.98s\n",
            "Training loss: 0.1008 | Validation loss: 0.1201 -- 162200 / 235102 in 10.18s\n",
            "Training loss: 0.1235 | Validation loss: 0.1499 -- 164200 / 235102 in 15.35s\n",
            "Training loss: 0.1102 | Validation loss: 0.1287 -- 166200 / 235102 in 20.42s\n",
            "Training loss: 0.1186 | Validation loss: 0.1253 -- 168200 / 235102 in 25.50s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1912 | Validation loss: 0.1515 -- 170200 / 235102 in 4.88s\n",
            "Training loss: 0.1297 | Validation loss: 0.1374 -- 172200 / 235102 in 10.10s\n",
            "Training loss: 0.1328 | Validation loss: 0.1375 -- 174200 / 235102 in 15.21s\n",
            "Training loss: 0.0907 | Validation loss: 0.1386 -- 176200 / 235102 in 20.37s\n",
            "Training loss: 0.1561 | Validation loss: 0.1443 -- 178200 / 235102 in 25.50s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1267 | Validation loss: 0.1488 -- 180200 / 235102 in 4.86s\n",
            "Training loss: 0.1073 | Validation loss: 0.1554 -- 182200 / 235102 in 10.00s\n",
            "Training loss: 0.1013 | Validation loss: 0.1515 -- 184200 / 235102 in 15.12s\n",
            "Training loss: 0.0769 | Validation loss: 0.1434 -- 186200 / 235102 in 20.24s\n",
            "Training loss: 0.1085 | Validation loss: 0.1662 -- 188200 / 235102 in 25.40s\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.0854 | Validation loss: 0.1317 -- 190200 / 235102 in 4.90s\n",
            "Training loss: 0.0917 | Validation loss: 0.1555 -- 192200 / 235102 in 10.03s\n",
            "Training loss: 0.1790 | Validation loss: 0.1412 -- 194200 / 235102 in 15.14s\n",
            "Training loss: 0.0926 | Validation loss: 0.1611 -- 196200 / 235102 in 20.28s\n",
            "Training loss: 0.1352 | Validation loss: 0.1423 -- 198200 / 235102 in 25.40s\n",
            "Preparing 10000 data...Done 2.80s.\n",
            "Training...\n",
            "Training loss: 0.1589 | Validation loss: 0.1389 -- 200200 / 235102 in 4.75s\n",
            "Training loss: 0.1547 | Validation loss: 0.1328 -- 202200 / 235102 in 9.86s\n",
            "Training loss: 0.1319 | Validation loss: 0.1539 -- 204200 / 235102 in 15.10s\n",
            "Training loss: 0.1826 | Validation loss: 0.1352 -- 206200 / 235102 in 20.26s\n",
            "Training loss: 0.1320 | Validation loss: 0.1373 -- 208200 / 235102 in 25.44s\n",
            "Preparing 10000 data...Done 2.81s.\n",
            "Training...\n",
            "Training loss: 0.0801 | Validation loss: 0.1434 -- 210200 / 235102 in 4.81s\n",
            "Training loss: 0.1202 | Validation loss: 0.1498 -- 212200 / 235102 in 10.03s\n",
            "Training loss: 0.0843 | Validation loss: 0.1432 -- 214200 / 235102 in 15.13s\n",
            "Training loss: 0.1428 | Validation loss: 0.1338 -- 216200 / 235102 in 20.22s\n",
            "Training loss: 0.1769 | Validation loss: 0.1426 -- 218200 / 235102 in 25.36s\n",
            "Preparing 10000 data...Done 2.82s.\n",
            "Training...\n",
            "Training loss: 0.1127 | Validation loss: 0.1365 -- 220200 / 235102 in 4.87s\n",
            "Training loss: 0.1079 | Validation loss: 0.1460 -- 222200 / 235102 in 9.96s\n",
            "Training loss: 0.1389 | Validation loss: 0.1369 -- 224200 / 235102 in 15.06s\n",
            "Training loss: 0.0778 | Validation loss: 0.1468 -- 226200 / 235102 in 20.23s\n",
            "Training loss: 0.1113 | Validation loss: 0.1461 -- 228200 / 235102 in 25.36s\n",
            "Preparing 5102 data...Done 2.90s.\n",
            "Training...Done.\n",
            "Epoch n°7: TRAIN LOSS 0.1216 | VAL LOSS 0.1443 TOTAL TIME: 680.41s.\n",
            "Testing on 10000...Done. 9.76s\n",
            "acc avg: 0.951, acc 0: 0.976, acc 1: 0.548, recall: 0.548,  precision: 0.592\n",
            "F1 Score: 0.5691768826619965\n",
            "______________________________________________________________________\n",
            "Epoch : 8\n",
            "Preparing 10000 data...Done 2.95s.\n",
            "Training...\n",
            "Training loss: 0.1112 | Validation loss: 0.1432 -- 200 / 235102 in 4.92s\n",
            "Training loss: 0.1075 | Validation loss: 0.1468 -- 2200 / 235102 in 9.98s\n",
            "Training loss: 0.1330 | Validation loss: 0.1685 -- 4200 / 235102 in 15.10s\n",
            "Training loss: 0.1467 | Validation loss: 0.1433 -- 6200 / 235102 in 20.24s\n",
            "Training loss: 0.1720 | Validation loss: 0.1489 -- 8200 / 235102 in 25.52s\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.1275 | Validation loss: 0.1530 -- 10200 / 235102 in 4.97s\n",
            "Training loss: 0.1549 | Validation loss: 0.1310 -- 12200 / 235102 in 10.21s\n",
            "Training loss: 0.1631 | Validation loss: 0.1338 -- 14200 / 235102 in 15.38s\n",
            "Training loss: 0.1746 | Validation loss: 0.1516 -- 16200 / 235102 in 20.59s\n",
            "Training loss: 0.1066 | Validation loss: 0.1352 -- 18200 / 235102 in 25.78s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.1174 | Validation loss: 0.1491 -- 20200 / 235102 in 4.95s\n",
            "Training loss: 0.1625 | Validation loss: 0.1243 -- 22200 / 235102 in 10.07s\n",
            "Training loss: 0.1170 | Validation loss: 0.1439 -- 24200 / 235102 in 15.23s\n",
            "Training loss: 0.0801 | Validation loss: 0.1329 -- 26200 / 235102 in 20.38s\n",
            "Training loss: 0.0899 | Validation loss: 0.1186 -- 28200 / 235102 in 25.58s\n",
            "Preparing 10000 data...Done 2.92s.\n",
            "Training...\n",
            "Training loss: 0.1251 | Validation loss: 0.1379 -- 30200 / 235102 in 4.98s\n",
            "Training loss: 0.1004 | Validation loss: 0.1423 -- 32200 / 235102 in 10.07s\n",
            "Training loss: 0.0635 | Validation loss: 0.1381 -- 34200 / 235102 in 15.22s\n",
            "Training loss: 0.1125 | Validation loss: 0.1473 -- 36200 / 235102 in 20.38s\n",
            "Training loss: 0.0860 | Validation loss: 0.1333 -- 38200 / 235102 in 25.57s\n",
            "Preparing 10000 data...Done 2.84s.\n",
            "Training...\n",
            "Training loss: 0.1179 | Validation loss: 0.1458 -- 40200 / 235102 in 4.88s\n",
            "Training loss: 0.1449 | Validation loss: 0.1413 -- 42200 / 235102 in 10.13s\n",
            "Training loss: 0.1144 | Validation loss: 0.1509 -- 44200 / 235102 in 15.33s\n",
            "Training loss: 0.0671 | Validation loss: 0.1559 -- 46200 / 235102 in 20.52s\n",
            "Training loss: 0.1027 | Validation loss: 0.1396 -- 48200 / 235102 in 25.67s\n",
            "Preparing 10000 data...Done 2.81s.\n",
            "Training...\n",
            "Training loss: 0.0896 | Validation loss: 0.1508 -- 50200 / 235102 in 4.84s\n",
            "Training loss: 0.1516 | Validation loss: 0.1387 -- 52200 / 235102 in 10.14s\n",
            "Training loss: 0.1748 | Validation loss: 0.1532 -- 54200 / 235102 in 15.32s\n",
            "Training loss: 0.1226 | Validation loss: 0.1541 -- 56200 / 235102 in 20.50s\n",
            "Training loss: 0.1136 | Validation loss: 0.1426 -- 58200 / 235102 in 25.64s\n",
            "Preparing 10000 data...Done 2.83s.\n",
            "Training...\n",
            "Training loss: 0.0894 | Validation loss: 0.1441 -- 60200 / 235102 in 4.89s\n",
            "Training loss: 0.1011 | Validation loss: 0.1768 -- 62200 / 235102 in 10.21s\n",
            "Training loss: 0.1057 | Validation loss: 0.1458 -- 64200 / 235102 in 15.52s\n",
            "Training loss: 0.1279 | Validation loss: 0.1558 -- 66200 / 235102 in 20.79s\n",
            "Training loss: 0.1120 | Validation loss: 0.1459 -- 68200 / 235102 in 26.02s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1405 | Validation loss: 0.1462 -- 70200 / 235102 in 4.99s\n",
            "Training loss: 0.1730 | Validation loss: 0.1442 -- 72200 / 235102 in 10.23s\n",
            "Training loss: 0.1168 | Validation loss: 0.1478 -- 74200 / 235102 in 15.50s\n",
            "Training loss: 0.2135 | Validation loss: 0.1274 -- 76200 / 235102 in 20.69s\n",
            "Training loss: 0.1080 | Validation loss: 0.1363 -- 78200 / 235102 in 25.84s\n",
            "Preparing 10000 data...Done 2.82s.\n",
            "Training...\n",
            "Training loss: 0.0961 | Validation loss: 0.1390 -- 80200 / 235102 in 4.88s\n",
            "Training loss: 0.0791 | Validation loss: 0.1424 -- 82200 / 235102 in 10.11s\n",
            "Training loss: 0.1615 | Validation loss: 0.1421 -- 84200 / 235102 in 15.40s\n",
            "Training loss: 0.1093 | Validation loss: 0.1341 -- 86200 / 235102 in 20.67s\n",
            "Training loss: 0.1083 | Validation loss: 0.1524 -- 88200 / 235102 in 25.92s\n",
            "Preparing 10000 data...Done 2.83s.\n",
            "Training...\n",
            "Training loss: 0.1111 | Validation loss: 0.1281 -- 90200 / 235102 in 4.89s\n",
            "Training loss: 0.1754 | Validation loss: 0.1662 -- 92200 / 235102 in 10.14s\n",
            "Training loss: 0.1554 | Validation loss: 0.1371 -- 94200 / 235102 in 15.39s\n",
            "Training loss: 0.1555 | Validation loss: 0.1433 -- 96200 / 235102 in 20.71s\n",
            "Training loss: 0.1392 | Validation loss: 0.1538 -- 98200 / 235102 in 25.97s\n",
            "Preparing 10000 data...Done 2.78s.\n",
            "Training...\n",
            "Training loss: 0.1390 | Validation loss: 0.1414 -- 100200 / 235102 in 4.86s\n",
            "Training loss: 0.1739 | Validation loss: 0.1476 -- 102200 / 235102 in 10.10s\n",
            "Training loss: 0.1584 | Validation loss: 0.1355 -- 104200 / 235102 in 15.38s\n",
            "Training loss: 0.1167 | Validation loss: 0.1447 -- 106200 / 235102 in 20.72s\n",
            "Training loss: 0.0937 | Validation loss: 0.1453 -- 108200 / 235102 in 26.02s\n",
            "Preparing 10000 data...Done 2.84s.\n",
            "Training...\n",
            "Training loss: 0.0995 | Validation loss: 0.1434 -- 110200 / 235102 in 4.98s\n",
            "Training loss: 0.1682 | Validation loss: 0.1312 -- 112200 / 235102 in 10.25s\n",
            "Training loss: 0.1428 | Validation loss: 0.1470 -- 114200 / 235102 in 15.57s\n",
            "Training loss: 0.1364 | Validation loss: 0.1505 -- 116200 / 235102 in 20.79s\n",
            "Training loss: 0.1622 | Validation loss: 0.1463 -- 118200 / 235102 in 26.04s\n",
            "Preparing 10000 data...Done 2.83s.\n",
            "Training...\n",
            "Training loss: 0.1128 | Validation loss: 0.1548 -- 120200 / 235102 in 5.01s\n",
            "Training loss: 0.0871 | Validation loss: 0.1397 -- 122200 / 235102 in 10.32s\n",
            "Training loss: 0.1093 | Validation loss: 0.1432 -- 124200 / 235102 in 15.53s\n",
            "Training loss: 0.0995 | Validation loss: 0.1465 -- 126200 / 235102 in 20.70s\n",
            "Training loss: 0.1279 | Validation loss: 0.1396 -- 128200 / 235102 in 25.80s\n",
            "Preparing 10000 data...Done 2.82s.\n",
            "Training...\n",
            "Training loss: 0.0878 | Validation loss: 0.1395 -- 130200 / 235102 in 4.91s\n",
            "Training loss: 0.1131 | Validation loss: 0.1348 -- 132200 / 235102 in 10.11s\n",
            "Training loss: 0.1117 | Validation loss: 0.1328 -- 134200 / 235102 in 15.39s\n",
            "Training loss: 0.1420 | Validation loss: 0.1285 -- 136200 / 235102 in 20.60s\n",
            "Training loss: 0.1096 | Validation loss: 0.1447 -- 138200 / 235102 in 25.84s\n",
            "Preparing 10000 data...Done 2.85s.\n",
            "Training...\n",
            "Training loss: 0.1370 | Validation loss: 0.1297 -- 140200 / 235102 in 4.95s\n",
            "Training loss: 0.1130 | Validation loss: 0.1230 -- 142200 / 235102 in 10.28s\n",
            "Training loss: 0.0931 | Validation loss: 0.1413 -- 144200 / 235102 in 15.50s\n",
            "Training loss: 0.1898 | Validation loss: 0.1262 -- 146200 / 235102 in 20.72s\n",
            "Training loss: 0.0684 | Validation loss: 0.1220 -- 148200 / 235102 in 26.03s\n",
            "Preparing 10000 data...Done 2.85s.\n",
            "Training...\n",
            "Training loss: 0.0961 | Validation loss: 0.1440 -- 150200 / 235102 in 4.95s\n",
            "Training loss: 0.1049 | Validation loss: 0.1375 -- 152200 / 235102 in 10.17s\n",
            "Training loss: 0.1224 | Validation loss: 0.1407 -- 154200 / 235102 in 15.42s\n",
            "Training loss: 0.1589 | Validation loss: 0.1426 -- 156200 / 235102 in 20.66s\n",
            "Training loss: 0.1335 | Validation loss: 0.1400 -- 158200 / 235102 in 25.84s\n",
            "Preparing 10000 data...Done 2.81s.\n",
            "Training...\n",
            "Training loss: 0.1532 | Validation loss: 0.1287 -- 160200 / 235102 in 4.96s\n",
            "Training loss: 0.0851 | Validation loss: 0.1307 -- 162200 / 235102 in 10.24s\n",
            "Training loss: 0.0788 | Validation loss: 0.1222 -- 164200 / 235102 in 15.54s\n",
            "Training loss: 0.1127 | Validation loss: 0.1372 -- 166200 / 235102 in 20.80s\n",
            "Training loss: 0.1730 | Validation loss: 0.1260 -- 168200 / 235102 in 26.04s\n",
            "Preparing 10000 data...Done 2.79s.\n",
            "Training...\n",
            "Training loss: 0.1385 | Validation loss: 0.1351 -- 170200 / 235102 in 4.87s\n",
            "Training loss: 0.1265 | Validation loss: 0.1393 -- 172200 / 235102 in 10.06s\n",
            "Training loss: 0.0941 | Validation loss: 0.1361 -- 174200 / 235102 in 15.28s\n",
            "Training loss: 0.1643 | Validation loss: 0.1343 -- 176200 / 235102 in 20.61s\n",
            "Training loss: 0.1068 | Validation loss: 0.1375 -- 178200 / 235102 in 25.97s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1342 | Validation loss: 0.1444 -- 180200 / 235102 in 5.01s\n",
            "Training loss: 0.1675 | Validation loss: 0.1486 -- 182200 / 235102 in 10.31s\n",
            "Training loss: 0.1076 | Validation loss: 0.1524 -- 184200 / 235102 in 15.60s\n",
            "Training loss: 0.0806 | Validation loss: 0.1403 -- 186200 / 235102 in 20.83s\n",
            "Training loss: 0.1380 | Validation loss: 0.1632 -- 188200 / 235102 in 26.09s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.0687 | Validation loss: 0.1290 -- 190200 / 235102 in 5.03s\n",
            "Training loss: 0.1213 | Validation loss: 0.1607 -- 192200 / 235102 in 10.42s\n",
            "Training loss: 0.0899 | Validation loss: 0.1288 -- 194200 / 235102 in 15.71s\n",
            "Training loss: 0.0877 | Validation loss: 0.1565 -- 196200 / 235102 in 21.00s\n",
            "Training loss: 0.0621 | Validation loss: 0.1404 -- 198200 / 235102 in 26.38s\n",
            "Preparing 10000 data...Done 2.85s.\n",
            "Training...\n",
            "Training loss: 0.1060 | Validation loss: 0.1414 -- 200200 / 235102 in 4.99s\n",
            "Training loss: 0.1007 | Validation loss: 0.1435 -- 202200 / 235102 in 10.27s\n",
            "Training loss: 0.0801 | Validation loss: 0.1421 -- 204200 / 235102 in 15.56s\n",
            "Training loss: 0.0957 | Validation loss: 0.1379 -- 206200 / 235102 in 20.87s\n",
            "Training loss: 0.0934 | Validation loss: 0.1438 -- 208200 / 235102 in 26.15s\n",
            "Preparing 10000 data...Done 2.78s.\n",
            "Training...\n",
            "Training loss: 0.1630 | Validation loss: 0.1380 -- 210200 / 235102 in 5.00s\n",
            "Training loss: 0.1294 | Validation loss: 0.1371 -- 212200 / 235102 in 10.48s\n",
            "Training loss: 0.0679 | Validation loss: 0.1319 -- 214200 / 235102 in 15.81s\n",
            "Training loss: 0.1187 | Validation loss: 0.1415 -- 216200 / 235102 in 21.10s\n",
            "Training loss: 0.1363 | Validation loss: 0.1334 -- 218200 / 235102 in 26.35s\n",
            "Preparing 10000 data...Done 2.78s.\n",
            "Training...\n",
            "Training loss: 0.0858 | Validation loss: 0.1431 -- 220200 / 235102 in 4.99s\n",
            "Training loss: 0.1474 | Validation loss: 0.1329 -- 222200 / 235102 in 10.25s\n",
            "Training loss: 0.1024 | Validation loss: 0.1413 -- 224200 / 235102 in 15.56s\n",
            "Training loss: 0.1276 | Validation loss: 0.1476 -- 226200 / 235102 in 20.99s\n",
            "Training loss: 0.1093 | Validation loss: 0.1339 -- 228200 / 235102 in 26.31s\n",
            "Preparing 5102 data...Done 2.75s.\n",
            "Training...Done.\n",
            "Epoch n°8: TRAIN LOSS 0.1199 | VAL LOSS 0.1415 TOTAL TIME: 689.16s.\n",
            "Testing on 10000...Done. 10.14s\n",
            "acc avg: 0.949, acc 0: 0.971, acc 1: 0.617, recall: 0.617,  precision: 0.585\n",
            "F1 Score: 0.60062893081761\n",
            "______________________________________________________________________\n",
            "Epoch : 9\n",
            "Preparing 10000 data...Done 2.84s.\n",
            "Training...\n",
            "Training loss: 0.1615 | Validation loss: 0.1408 -- 200 / 235102 in 4.96s\n",
            "Training loss: 0.0892 | Validation loss: 0.1481 -- 2200 / 235102 in 10.13s\n",
            "Training loss: 0.1599 | Validation loss: 0.1505 -- 4200 / 235102 in 15.38s\n",
            "Training loss: 0.1845 | Validation loss: 0.1525 -- 6200 / 235102 in 20.59s\n",
            "Training loss: 0.1108 | Validation loss: 0.1521 -- 8200 / 235102 in 25.83s\n",
            "Preparing 10000 data...Done 2.83s.\n",
            "Training...\n",
            "Training loss: 0.1004 | Validation loss: 0.1251 -- 10200 / 235102 in 4.97s\n",
            "Training loss: 0.2286 | Validation loss: 0.1197 -- 12200 / 235102 in 10.35s\n",
            "Training loss: 0.0887 | Validation loss: 0.1555 -- 14200 / 235102 in 15.63s\n",
            "Training loss: 0.1577 | Validation loss: 0.1382 -- 16200 / 235102 in 20.88s\n",
            "Training loss: 0.1466 | Validation loss: 0.1443 -- 18200 / 235102 in 26.11s\n",
            "Preparing 10000 data...Done 2.76s.\n",
            "Training...\n",
            "Training loss: 0.0870 | Validation loss: 0.1172 -- 20200 / 235102 in 4.81s\n",
            "Training loss: 0.1406 | Validation loss: 0.1532 -- 22200 / 235102 in 10.06s\n",
            "Training loss: 0.0704 | Validation loss: 0.1157 -- 24200 / 235102 in 15.36s\n",
            "Training loss: 0.1582 | Validation loss: 0.1315 -- 26200 / 235102 in 20.65s\n",
            "Training loss: 0.0977 | Validation loss: 0.1175 -- 28200 / 235102 in 25.93s\n",
            "Preparing 10000 data...Done 2.76s.\n",
            "Training...\n",
            "Training loss: 0.0895 | Validation loss: 0.1355 -- 30200 / 235102 in 4.84s\n",
            "Training loss: 0.1328 | Validation loss: 0.1387 -- 32200 / 235102 in 10.08s\n",
            "Training loss: 0.1070 | Validation loss: 0.1358 -- 34200 / 235102 in 15.31s\n",
            "Training loss: 0.1314 | Validation loss: 0.1369 -- 36200 / 235102 in 20.58s\n",
            "Training loss: 0.1302 | Validation loss: 0.1357 -- 38200 / 235102 in 25.89s\n",
            "Preparing 10000 data...Done 2.83s.\n",
            "Training...\n",
            "Training loss: 0.1052 | Validation loss: 0.1493 -- 40200 / 235102 in 4.98s\n",
            "Training loss: 0.1727 | Validation loss: 0.1456 -- 42200 / 235102 in 10.35s\n",
            "Training loss: 0.1552 | Validation loss: 0.1450 -- 44200 / 235102 in 15.70s\n",
            "Training loss: 0.1849 | Validation loss: 0.1409 -- 46200 / 235102 in 21.06s\n",
            "Training loss: 0.1203 | Validation loss: 0.1523 -- 48200 / 235102 in 26.42s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.1283 | Validation loss: 0.1488 -- 50200 / 235102 in 5.39s\n",
            "Training loss: 0.0815 | Validation loss: 0.1505 -- 52200 / 235102 in 10.79s\n",
            "Training loss: 0.1102 | Validation loss: 0.1493 -- 54200 / 235102 in 16.18s\n",
            "Training loss: 0.1185 | Validation loss: 0.1377 -- 56200 / 235102 in 21.50s\n",
            "Training loss: 0.1560 | Validation loss: 0.1584 -- 58200 / 235102 in 26.84s\n",
            "Preparing 10000 data...Done 2.82s.\n",
            "Training...\n",
            "Training loss: 0.1546 | Validation loss: 0.1324 -- 60200 / 235102 in 4.95s\n",
            "Training loss: 0.1269 | Validation loss: 0.1878 -- 62200 / 235102 in 10.39s\n",
            "Training loss: 0.1038 | Validation loss: 0.1402 -- 64200 / 235102 in 15.76s\n",
            "Training loss: 0.1416 | Validation loss: 0.1541 -- 66200 / 235102 in 21.18s\n",
            "Training loss: 0.1525 | Validation loss: 0.1435 -- 68200 / 235102 in 26.60s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1236 | Validation loss: 0.1356 -- 70200 / 235102 in 5.07s\n",
            "Training loss: 0.1007 | Validation loss: 0.1428 -- 72200 / 235102 in 10.48s\n",
            "Training loss: 0.1062 | Validation loss: 0.1322 -- 74200 / 235102 in 15.94s\n",
            "Training loss: 0.0924 | Validation loss: 0.1530 -- 76200 / 235102 in 21.41s\n",
            "Training loss: 0.1213 | Validation loss: 0.1261 -- 78200 / 235102 in 26.89s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.0983 | Validation loss: 0.1383 -- 80200 / 235102 in 5.07s\n",
            "Training loss: 0.1264 | Validation loss: 0.1408 -- 82200 / 235102 in 10.55s\n",
            "Training loss: 0.0954 | Validation loss: 0.1399 -- 84200 / 235102 in 16.06s\n",
            "Training loss: 0.0598 | Validation loss: 0.1437 -- 86200 / 235102 in 21.56s\n",
            "Training loss: 0.1020 | Validation loss: 0.1245 -- 88200 / 235102 in 26.92s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.1524 | Validation loss: 0.1299 -- 90200 / 235102 in 5.04s\n",
            "Training loss: 0.0914 | Validation loss: 0.1337 -- 92200 / 235102 in 10.43s\n",
            "Training loss: 0.1749 | Validation loss: 0.1273 -- 94200 / 235102 in 15.75s\n",
            "Training loss: 0.1004 | Validation loss: 0.1432 -- 96200 / 235102 in 21.18s\n",
            "Training loss: 0.1367 | Validation loss: 0.1398 -- 98200 / 235102 in 26.62s\n",
            "Preparing 10000 data...Done 2.80s.\n",
            "Training...\n",
            "Training loss: 0.1500 | Validation loss: 0.1581 -- 100200 / 235102 in 5.02s\n",
            "Training loss: 0.1044 | Validation loss: 0.1426 -- 102200 / 235102 in 10.47s\n",
            "Training loss: 0.1086 | Validation loss: 0.1299 -- 104200 / 235102 in 16.00s\n",
            "Training loss: 0.0661 | Validation loss: 0.1451 -- 106200 / 235102 in 21.45s\n",
            "Training loss: 0.0998 | Validation loss: 0.1390 -- 108200 / 235102 in 26.92s\n",
            "Preparing 10000 data...Done 2.85s.\n",
            "Training...\n",
            "Training loss: 0.1524 | Validation loss: 0.1509 -- 110200 / 235102 in 5.13s\n",
            "Training loss: 0.1297 | Validation loss: 0.1400 -- 112200 / 235102 in 10.54s\n",
            "Training loss: 0.1087 | Validation loss: 0.1394 -- 114200 / 235102 in 15.97s\n",
            "Training loss: 0.1094 | Validation loss: 0.1392 -- 116200 / 235102 in 21.35s\n",
            "Training loss: 0.0671 | Validation loss: 0.1393 -- 118200 / 235102 in 26.81s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.0849 | Validation loss: 0.1410 -- 120200 / 235102 in 5.10s\n",
            "Training loss: 0.0912 | Validation loss: 0.1456 -- 122200 / 235102 in 10.45s\n",
            "Training loss: 0.1388 | Validation loss: 0.1469 -- 124200 / 235102 in 15.77s\n",
            "Training loss: 0.0685 | Validation loss: 0.1393 -- 126200 / 235102 in 21.02s\n",
            "Training loss: 0.0980 | Validation loss: 0.1460 -- 128200 / 235102 in 26.34s\n",
            "Preparing 10000 data...Done 2.84s.\n",
            "Training...\n",
            "Training loss: 0.0931 | Validation loss: 0.1438 -- 130200 / 235102 in 5.04s\n",
            "Training loss: 0.0778 | Validation loss: 0.1227 -- 132200 / 235102 in 10.51s\n",
            "Training loss: 0.1052 | Validation loss: 0.1407 -- 134200 / 235102 in 15.85s\n",
            "Training loss: 0.0944 | Validation loss: 0.1280 -- 136200 / 235102 in 21.24s\n",
            "Training loss: 0.1137 | Validation loss: 0.1309 -- 138200 / 235102 in 26.60s\n",
            "Preparing 10000 data...Done 2.83s.\n",
            "Training...\n",
            "Training loss: 0.1287 | Validation loss: 0.1455 -- 140200 / 235102 in 5.06s\n",
            "Training loss: 0.1062 | Validation loss: 0.1190 -- 142200 / 235102 in 10.45s\n",
            "Training loss: 0.1194 | Validation loss: 0.1307 -- 144200 / 235102 in 15.91s\n",
            "Training loss: 0.1025 | Validation loss: 0.1241 -- 146200 / 235102 in 21.38s\n",
            "Training loss: 0.1383 | Validation loss: 0.1299 -- 148200 / 235102 in 26.82s\n",
            "Preparing 10000 data...Done 2.79s.\n",
            "Training...\n",
            "Training loss: 0.1158 | Validation loss: 0.1412 -- 150200 / 235102 in 4.95s\n",
            "Training loss: 0.1966 | Validation loss: 0.1376 -- 152200 / 235102 in 10.32s\n",
            "Training loss: 0.1145 | Validation loss: 0.1318 -- 154200 / 235102 in 15.63s\n",
            "Training loss: 0.1347 | Validation loss: 0.1515 -- 156200 / 235102 in 20.93s\n",
            "Training loss: 0.0912 | Validation loss: 0.1272 -- 158200 / 235102 in 26.29s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.0874 | Validation loss: 0.1477 -- 160200 / 235102 in 5.13s\n",
            "Training loss: 0.1272 | Validation loss: 0.1298 -- 162200 / 235102 in 10.47s\n",
            "Training loss: 0.0679 | Validation loss: 0.1268 -- 164200 / 235102 in 15.92s\n",
            "Training loss: 0.1005 | Validation loss: 0.1211 -- 166200 / 235102 in 21.29s\n",
            "Training loss: 0.0829 | Validation loss: 0.1285 -- 168200 / 235102 in 26.71s\n",
            "Preparing 10000 data...Done 2.84s.\n",
            "Training...\n",
            "Training loss: 0.0864 | Validation loss: 0.1307 -- 170200 / 235102 in 5.08s\n",
            "Training loss: 0.1491 | Validation loss: 0.1486 -- 172200 / 235102 in 10.61s\n",
            "Training loss: 0.1018 | Validation loss: 0.1281 -- 174200 / 235102 in 16.09s\n",
            "Training loss: 0.1042 | Validation loss: 0.1432 -- 176200 / 235102 in 21.64s\n",
            "Training loss: 0.0551 | Validation loss: 0.1259 -- 178200 / 235102 in 27.05s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1178 | Validation loss: 0.1725 -- 180200 / 235102 in 5.15s\n",
            "Training loss: 0.1403 | Validation loss: 0.1351 -- 182200 / 235102 in 10.73s\n",
            "Training loss: 0.1933 | Validation loss: 0.1513 -- 184200 / 235102 in 16.17s\n",
            "Training loss: 0.1084 | Validation loss: 0.1442 -- 186200 / 235102 in 21.61s\n",
            "Training loss: 0.1325 | Validation loss: 0.1521 -- 188200 / 235102 in 27.05s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.1388 | Validation loss: 0.1434 -- 190200 / 235102 in 5.12s\n",
            "Training loss: 0.0990 | Validation loss: 0.1406 -- 192200 / 235102 in 10.61s\n",
            "Training loss: 0.1021 | Validation loss: 0.1353 -- 194200 / 235102 in 16.21s\n",
            "Training loss: 0.1561 | Validation loss: 0.1545 -- 196200 / 235102 in 21.85s\n",
            "Training loss: 0.1066 | Validation loss: 0.1335 -- 198200 / 235102 in 27.41s\n",
            "Preparing 10000 data...Done 2.81s.\n",
            "Training...\n",
            "Training loss: 0.1367 | Validation loss: 0.1555 -- 200200 / 235102 in 5.08s\n",
            "Training loss: 0.1538 | Validation loss: 0.1391 -- 202200 / 235102 in 10.59s\n",
            "Training loss: 0.1258 | Validation loss: 0.1455 -- 204200 / 235102 in 16.17s\n",
            "Training loss: 0.0652 | Validation loss: 0.1366 -- 206200 / 235102 in 21.89s\n",
            "Training loss: 0.1071 | Validation loss: 0.1341 -- 208200 / 235102 in 27.39s\n",
            "Preparing 10000 data...Done 2.80s.\n",
            "Training...\n",
            "Training loss: 0.0872 | Validation loss: 0.1420 -- 210200 / 235102 in 5.12s\n",
            "Training loss: 0.1006 | Validation loss: 0.1326 -- 212200 / 235102 in 10.66s\n",
            "Training loss: 0.1373 | Validation loss: 0.1502 -- 214200 / 235102 in 16.23s\n",
            "Training loss: 0.0935 | Validation loss: 0.1297 -- 216200 / 235102 in 21.82s\n",
            "Training loss: 0.1422 | Validation loss: 0.1379 -- 218200 / 235102 in 27.44s\n",
            "Preparing 10000 data...Done 2.82s.\n",
            "Training...\n",
            "Training loss: 0.0642 | Validation loss: 0.1410 -- 220200 / 235102 in 5.23s\n",
            "Training loss: 0.2215 | Validation loss: 0.1317 -- 222200 / 235102 in 10.84s\n",
            "Training loss: 0.1334 | Validation loss: 0.1467 -- 224200 / 235102 in 16.53s\n",
            "Training loss: 0.1295 | Validation loss: 0.1363 -- 226200 / 235102 in 22.20s\n",
            "Training loss: 0.0783 | Validation loss: 0.1462 -- 228200 / 235102 in 27.90s\n",
            "Preparing 5102 data...Done 2.84s.\n",
            "Training...Done.\n",
            "Epoch n°9: TRAIN LOSS 0.1190 | VAL LOSS 0.1398 TOTAL TIME: 710.22s.\n",
            "Testing on 10000...Done. 10.82s\n",
            "acc avg: 0.956, acc 0: 0.981, acc 1: 0.557, recall: 0.557,  precision: 0.642\n",
            "F1 Score: 0.5964912280701754\n",
            "______________________________________________________________________\n",
            "Epoch : 10\n",
            "Preparing 10000 data...Done 2.91s.\n",
            "Training...\n",
            "Training loss: 0.0908 | Validation loss: 0.1408 -- 200 / 235102 in 5.35s\n",
            "Training loss: 0.1143 | Validation loss: 0.1573 -- 2200 / 235102 in 11.08s\n",
            "Training loss: 0.1293 | Validation loss: 0.1497 -- 4200 / 235102 in 16.71s\n",
            "Training loss: 0.1787 | Validation loss: 0.1551 -- 6200 / 235102 in 22.27s\n",
            "Training loss: 0.0822 | Validation loss: 0.1516 -- 8200 / 235102 in 27.83s\n",
            "Preparing 10000 data...Done 2.93s.\n",
            "Training...\n",
            "Training loss: 0.1526 | Validation loss: 0.1307 -- 10200 / 235102 in 5.36s\n",
            "Training loss: 0.1140 | Validation loss: 0.1472 -- 12200 / 235102 in 11.13s\n",
            "Training loss: 0.0939 | Validation loss: 0.1381 -- 14200 / 235102 in 16.81s\n",
            "Training loss: 0.1644 | Validation loss: 0.1338 -- 16200 / 235102 in 22.42s\n",
            "Training loss: 0.1333 | Validation loss: 0.1373 -- 18200 / 235102 in 28.48s\n",
            "Preparing 10000 data...Done 2.82s.\n",
            "Training...\n",
            "Training loss: 0.0893 | Validation loss: 0.1258 -- 20200 / 235102 in 5.28s\n",
            "Training loss: 0.1690 | Validation loss: 0.1276 -- 22200 / 235102 in 11.07s\n",
            "Training loss: 0.1238 | Validation loss: 0.1225 -- 24200 / 235102 in 16.85s\n",
            "Training loss: 0.1131 | Validation loss: 0.1340 -- 26200 / 235102 in 22.66s\n",
            "Training loss: 0.1044 | Validation loss: 0.1283 -- 28200 / 235102 in 28.40s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1439 | Validation loss: 0.1449 -- 30200 / 235102 in 5.35s\n",
            "Training loss: 0.0889 | Validation loss: 0.1298 -- 32200 / 235102 in 11.13s\n",
            "Training loss: 0.1204 | Validation loss: 0.1576 -- 34200 / 235102 in 16.96s\n",
            "Training loss: 0.1002 | Validation loss: 0.1301 -- 36200 / 235102 in 22.77s\n",
            "Training loss: 0.1592 | Validation loss: 0.1357 -- 38200 / 235102 in 28.59s\n",
            "Preparing 10000 data...Done 2.80s.\n",
            "Training...\n",
            "Training loss: 0.1150 | Validation loss: 0.1390 -- 40200 / 235102 in 5.25s\n",
            "Training loss: 0.1250 | Validation loss: 0.1622 -- 42200 / 235102 in 11.12s\n",
            "Training loss: 0.1391 | Validation loss: 0.1478 -- 44200 / 235102 in 16.91s\n",
            "Training loss: 0.0728 | Validation loss: 0.1518 -- 46200 / 235102 in 22.56s\n",
            "Training loss: 0.1739 | Validation loss: 0.1344 -- 48200 / 235102 in 28.27s\n",
            "Preparing 10000 data...Done 2.80s.\n",
            "Training...\n",
            "Training loss: 0.1294 | Validation loss: 0.1489 -- 50200 / 235102 in 5.23s\n",
            "Training loss: 0.0909 | Validation loss: 0.1312 -- 52200 / 235102 in 11.13s\n",
            "Training loss: 0.1644 | Validation loss: 0.1459 -- 54200 / 235102 in 16.90s\n",
            "Training loss: 0.1560 | Validation loss: 0.1524 -- 56200 / 235102 in 22.68s\n",
            "Training loss: 0.2035 | Validation loss: 0.1500 -- 58200 / 235102 in 28.47s\n",
            "Preparing 10000 data...Done 2.84s.\n",
            "Training...\n",
            "Training loss: 0.1412 | Validation loss: 0.1528 -- 60200 / 235102 in 5.26s\n",
            "Training loss: 0.0955 | Validation loss: 0.1602 -- 62200 / 235102 in 11.14s\n",
            "Training loss: 0.1272 | Validation loss: 0.1400 -- 64200 / 235102 in 16.93s\n",
            "Training loss: 0.1041 | Validation loss: 0.1471 -- 66200 / 235102 in 22.68s\n",
            "Training loss: 0.1447 | Validation loss: 0.1428 -- 68200 / 235102 in 28.53s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.0801 | Validation loss: 0.1435 -- 70200 / 235102 in 5.35s\n",
            "Training loss: 0.0996 | Validation loss: 0.1270 -- 72200 / 235102 in 11.18s\n",
            "Training loss: 0.1272 | Validation loss: 0.1368 -- 74200 / 235102 in 17.08s\n",
            "Training loss: 0.1554 | Validation loss: 0.1504 -- 76200 / 235102 in 22.96s\n",
            "Training loss: 0.1075 | Validation loss: 0.1401 -- 78200 / 235102 in 28.88s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1018 | Validation loss: 0.1397 -- 80200 / 235102 in 5.35s\n",
            "Training loss: 0.1018 | Validation loss: 0.1407 -- 82200 / 235102 in 11.26s\n",
            "Training loss: 0.1108 | Validation loss: 0.1468 -- 84200 / 235102 in 17.25s\n",
            "Training loss: 0.1789 | Validation loss: 0.1269 -- 86200 / 235102 in 23.11s\n",
            "Training loss: 0.1160 | Validation loss: 0.1357 -- 88200 / 235102 in 28.99s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1110 | Validation loss: 0.1276 -- 90200 / 235102 in 5.39s\n",
            "Training loss: 0.1257 | Validation loss: 0.1473 -- 92200 / 235102 in 11.35s\n",
            "Training loss: 0.1199 | Validation loss: 0.1277 -- 94200 / 235102 in 17.30s\n",
            "Training loss: 0.0919 | Validation loss: 0.1416 -- 96200 / 235102 in 23.31s\n",
            "Training loss: 0.1313 | Validation loss: 0.1350 -- 98200 / 235102 in 29.30s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.0982 | Validation loss: 0.1544 -- 100200 / 235102 in 5.43s\n",
            "Training loss: 0.0618 | Validation loss: 0.1350 -- 102200 / 235102 in 11.39s\n",
            "Training loss: 0.0746 | Validation loss: 0.1411 -- 104200 / 235102 in 17.41s\n",
            "Training loss: 0.1060 | Validation loss: 0.1462 -- 106200 / 235102 in 23.39s\n",
            "Training loss: 0.1059 | Validation loss: 0.1571 -- 108200 / 235102 in 29.46s\n",
            "Preparing 10000 data...Done 2.86s.\n",
            "Training...\n",
            "Training loss: 0.1309 | Validation loss: 0.1363 -- 110200 / 235102 in 5.48s\n",
            "Training loss: 0.0856 | Validation loss: 0.1554 -- 112200 / 235102 in 11.46s\n",
            "Training loss: 0.0690 | Validation loss: 0.1318 -- 114200 / 235102 in 17.41s\n",
            "Training loss: 0.1376 | Validation loss: 0.1435 -- 116200 / 235102 in 23.40s\n",
            "Training loss: 0.1892 | Validation loss: 0.1420 -- 118200 / 235102 in 29.44s\n",
            "Preparing 10000 data...Done 2.90s.\n",
            "Training...\n",
            "Training loss: 0.0758 | Validation loss: 0.1403 -- 120200 / 235102 in 5.54s\n",
            "Training loss: 0.1404 | Validation loss: 0.1560 -- 122200 / 235102 in 11.57s\n",
            "Training loss: 0.1280 | Validation loss: 0.1413 -- 124200 / 235102 in 17.62s\n",
            "Training loss: 0.1360 | Validation loss: 0.1462 -- 126200 / 235102 in 23.57s\n",
            "Training loss: 0.1139 | Validation loss: 0.1350 -- 128200 / 235102 in 29.52s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.0877 | Validation loss: 0.1368 -- 130200 / 235102 in 5.40s\n",
            "Training loss: 0.1625 | Validation loss: 0.1361 -- 132200 / 235102 in 11.37s\n",
            "Training loss: 0.0997 | Validation loss: 0.1160 -- 134200 / 235102 in 17.35s\n",
            "Training loss: 0.1664 | Validation loss: 0.1550 -- 136200 / 235102 in 23.30s\n",
            "Training loss: 0.1184 | Validation loss: 0.1243 -- 138200 / 235102 in 29.15s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.0991 | Validation loss: 0.1196 -- 140200 / 235102 in 5.33s\n",
            "Training loss: 0.1343 | Validation loss: 0.1219 -- 142200 / 235102 in 11.22s\n",
            "Training loss: 0.0825 | Validation loss: 0.1306 -- 144200 / 235102 in 17.05s\n",
            "Training loss: 0.1551 | Validation loss: 0.1290 -- 146200 / 235102 in 22.87s\n",
            "Training loss: 0.1275 | Validation loss: 0.1307 -- 148200 / 235102 in 28.83s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.1483 | Validation loss: 0.1330 -- 150200 / 235102 in 5.35s\n",
            "Training loss: 0.0899 | Validation loss: 0.1331 -- 152200 / 235102 in 11.17s\n",
            "Training loss: 0.0874 | Validation loss: 0.1354 -- 154200 / 235102 in 17.11s\n",
            "Training loss: 0.1500 | Validation loss: 0.1311 -- 156200 / 235102 in 23.05s\n",
            "Training loss: 0.1105 | Validation loss: 0.1419 -- 158200 / 235102 in 28.85s\n",
            "Preparing 10000 data...Done 2.87s.\n",
            "Training...\n",
            "Training loss: 0.0791 | Validation loss: 0.1302 -- 160200 / 235102 in 5.43s\n",
            "Training loss: 0.1071 | Validation loss: 0.1315 -- 162200 / 235102 in 11.32s\n",
            "Training loss: 0.1621 | Validation loss: 0.1313 -- 164200 / 235102 in 17.20s\n",
            "Training loss: 0.1106 | Validation loss: 0.1248 -- 166200 / 235102 in 23.10s\n",
            "Training loss: 0.1145 | Validation loss: 0.1259 -- 168200 / 235102 in 28.99s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.1771 | Validation loss: 0.1446 -- 170200 / 235102 in 5.43s\n",
            "Training loss: 0.0997 | Validation loss: 0.1264 -- 172200 / 235102 in 11.36s\n",
            "Training loss: 0.1784 | Validation loss: 0.1392 -- 174200 / 235102 in 17.35s\n",
            "Training loss: 0.1819 | Validation loss: 0.1331 -- 176200 / 235102 in 23.37s\n",
            "Training loss: 0.0617 | Validation loss: 0.1318 -- 178200 / 235102 in 29.37s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1167 | Validation loss: 0.1480 -- 180200 / 235102 in 5.44s\n",
            "Training loss: 0.1402 | Validation loss: 0.1501 -- 182200 / 235102 in 11.37s\n",
            "Training loss: 0.0917 | Validation loss: 0.1493 -- 184200 / 235102 in 17.25s\n",
            "Training loss: 0.0969 | Validation loss: 0.1365 -- 186200 / 235102 in 23.29s\n",
            "Training loss: 0.1017 | Validation loss: 0.1528 -- 188200 / 235102 in 29.31s\n",
            "Preparing 10000 data...Done 2.98s.\n",
            "Training...\n",
            "Training loss: 0.0899 | Validation loss: 0.1406 -- 190200 / 235102 in 5.51s\n",
            "Training loss: 0.1517 | Validation loss: 0.1319 -- 192200 / 235102 in 11.38s\n",
            "Training loss: 0.0912 | Validation loss: 0.1369 -- 194200 / 235102 in 17.38s\n",
            "Training loss: 0.1594 | Validation loss: 0.1408 -- 196200 / 235102 in 23.24s\n",
            "Training loss: 0.1438 | Validation loss: 0.1451 -- 198200 / 235102 in 29.21s\n",
            "Preparing 10000 data...Done 2.88s.\n",
            "Training...\n",
            "Training loss: 0.1095 | Validation loss: 0.1383 -- 200200 / 235102 in 5.43s\n",
            "Training loss: 0.1674 | Validation loss: 0.1380 -- 202200 / 235102 in 11.38s\n",
            "Training loss: 0.0573 | Validation loss: 0.1321 -- 204200 / 235102 in 17.28s\n",
            "Training loss: 0.1123 | Validation loss: 0.1483 -- 206200 / 235102 in 23.34s\n",
            "Training loss: 0.1340 | Validation loss: 0.1380 -- 208200 / 235102 in 29.28s\n",
            "Preparing 10000 data...Done 2.84s.\n",
            "Training...\n",
            "Training loss: 0.1198 | Validation loss: 0.1297 -- 210200 / 235102 in 5.45s\n",
            "Training loss: 0.1364 | Validation loss: 0.1327 -- 212200 / 235102 in 11.35s\n",
            "Training loss: 0.0734 | Validation loss: 0.1417 -- 214200 / 235102 in 17.33s\n",
            "Training loss: 0.1142 | Validation loss: 0.1364 -- 216200 / 235102 in 23.39s\n",
            "Training loss: 0.1007 | Validation loss: 0.1421 -- 218200 / 235102 in 29.42s\n",
            "Preparing 10000 data...Done 2.89s.\n",
            "Training...\n",
            "Training loss: 0.0549 | Validation loss: 0.1306 -- 220200 / 235102 in 5.53s\n",
            "Training loss: 0.1448 | Validation loss: 0.1401 -- 222200 / 235102 in 11.57s\n",
            "Training loss: 0.1108 | Validation loss: 0.1403 -- 224200 / 235102 in 17.50s\n",
            "Training loss: 0.1221 | Validation loss: 0.1478 -- 226200 / 235102 in 23.42s\n",
            "Training loss: 0.0848 | Validation loss: 0.1286 -- 228200 / 235102 in 29.50s\n",
            "Preparing 5102 data...Done 2.91s.\n",
            "Training...Done.\n",
            "Epoch n°10: TRAIN LOSS 0.1168 | VAL LOSS 0.1392 TOTAL TIME: 766.62s.\n",
            "Testing on 10000...Done. 11.74s\n",
            "acc avg: 0.954, acc 0: 0.979, acc 1: 0.561, recall: 0.561,  precision: 0.626\n",
            "F1 Score: 0.5918727915194346\n",
            "______________________________________________________________________\n",
            "Loading model with best performances: epoch 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omMSKZB1tsXB",
        "colab_type": "text"
      },
      "source": [
        "### Test of the model\n",
        "We test on validate_df which is in fact a test set, with a normal proportion of 0 and 1 (about 6% of 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fzc7-kEt_wJ",
        "colab_type": "code",
        "outputId": "b97943fc-0fe1-4294-9a9c-f30751b6df84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "#model = RNN_Conv(input_dim, hidden_dim, layer_dim, output_dim, sentence_size, \n",
        "#                 number_channels, conv_kernel, max_kernel, dropout_rate=dropout_rate)\n",
        "#model.load_state_dict(th.load(path+'model_RNNConv_U.pt'))\n",
        "model.eval()\n",
        "#model = th.load(path + 'model6.pt')\n",
        "#model.eval()\n",
        "print(model)\n",
        "print(test_model(model, validate_df.values, words_dict, 50000, sentence_size, neighboring_area))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN_Conv(\n",
            "  (dropout): Dropout(p=0.4, inplace=False)\n",
            "  (rnn): RNN(300, 400, batch_first=True, bidirectional=True)\n",
            "  (conv): Conv1d(30, 16, kernel_size=(3,), stride=(1,))\n",
            "  (maxp): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flatten): Flatten()\n",
            "  (linear_1): Linear(in_features=3184, out_features=200, bias=True)\n",
            "  (batch_norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (linear_2): Linear(in_features=200, out_features=1, bias=True)\n",
            ")\n",
            "Testing on 50000...Done. 58.21s\n",
            "acc avg: 0.956, acc 0: 0.981, acc 1: 0.567, recall: 0.567,  precision: 0.661\n",
            "F1 Score: 0.6101574942488055\n",
            "(0.95594, 0.9811533710975766, 0.5667324128862591, 0.5667324128862591, 0.6607895745496358, 0.6101574942488055)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6VXsc9jFr5L",
        "colab_type": "text"
      },
      "source": [
        "## BERT finetuning\n",
        "\n",
        "In this part, we will fine-tune a model imported from Hugging's Face Library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBn2_tsnG81M",
        "colab_type": "code",
        "outputId": "404d3dfd-9a42-48d5-c282-f9b98da72481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 13.0MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 41.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=ec9cc05237dbd6d8b2aaa300b71bfa19f4e501cb529c0142329b3fa3b060bf1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.7.0 transformers-2.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPZxYk-yJkYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import os\n",
        "import tqdm\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset # used load data\n",
        "import torch.optim as optim\n",
        "\n",
        "from transformers.data.processors.utils import InputExample, InputFeatures\n",
        "from transformers import (AdamW,\n",
        "                          get_linear_schedule_with_warmup,\n",
        "                          get_cosine_with_hard_restarts_schedule_with_warmup)\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
        "\n",
        "# Functions to process data\n",
        "\n",
        "class InputExample(object):\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "    \"\"\" \n",
        "    INPUTS:\n",
        "    - guid: string, describing the example\n",
        "    - text_a: string, text of the example\n",
        "    - text_b: string, optional\n",
        "    - label: int, optional\n",
        "    \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.label = label\n",
        "\n",
        "    \n",
        "class DataProcessor():\n",
        "    \n",
        "  def get_data_examples(self, sentences, labels=None):\n",
        "    \"\"\"\n",
        "    This function transforms a list of sentences into a list of \n",
        "    InputExample.\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    guid = \"data\"\n",
        "    if labels is None:\n",
        "      for i in range(len(sentences)):\n",
        "        examples.append(InputExample(guid=guid, text_a=sentences[i], text_b=None, label=\"0\"))\n",
        "    else:\n",
        "      for i in range(len(sentences)):\n",
        "        examples.append(InputExample(guid=guid, text_a=sentences[i], text_b=None, label=str(labels[i])))\n",
        "        \n",
        "    return examples\n",
        "\n",
        "  def get_labels(self):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    return [str(j) for j in range(2)]\n",
        "\n",
        "    \n",
        "def convert_examples_to_features(examples, tokenizer, max_length=50, label_list=None,\n",
        "                                 pad_token=0, pad_token_segment_id=0,\n",
        "                                 mask_padding_with_zero=True):\n",
        "  \"\"\"\n",
        "  This function encodes string sentences using the tokenizer applying max_length,\n",
        "  padding and mask.\n",
        "  INPUTS:\n",
        "  - examples: list of InputExample objects, examples to tokenize.\n",
        "  - tokenizer: tokenizer object.\n",
        "  - max_length: int, max_length of sequences.\n",
        "  - label_list: list of all possible labels.\n",
        "  - pad_token: int, default token for padding.\n",
        "  - pad_token_segment_id: int, default id for padding.\n",
        "  - mask_padding_with_zero: bool, saying if we mask padding with zero or 1.\n",
        "  OUTPUTS:\n",
        "  - features: list of InputFeatures object.\n",
        "  \"\"\"\n",
        "  processor = DataProcessor()\n",
        "\n",
        "  label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "  features = []\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "\n",
        "    inputs = tokenizer.encode_plus(example.text_a, example.text_b,\n",
        "                                    add_special_tokens=True,\n",
        "                                    max_length=max_length)\n",
        "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    padding_length = max_length - len(input_ids)\n",
        "    input_ids = input_ids + ([pad_token] * padding_length)\n",
        "    attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "    token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "    assert len(input_ids) == max_length, \"Error with input length {} vs {}\".format(len(input_ids), max_length)\n",
        "    assert len(attention_mask) == max_length, \"Error with input length {} vs {}\".format(len(attention_mask), max_length)\n",
        "    assert len(token_type_ids) == max_length, \"Error with input length {} vs {}\".format(len(token_type_ids), max_length)\n",
        "    \n",
        "    label = label_map[example.label]\n",
        "    \n",
        "    features.append(InputFeatures(input_ids=input_ids, attention_mask=attention_mask,\n",
        "                                  token_type_ids=token_type_ids, label=label))\n",
        "\n",
        "  return features\n",
        "\n",
        "def load_examples(sentences, labels, tokenizer, max_seq_length, label_list):\n",
        "  \"\"\"\n",
        "  This function converts a group of sentences and labels into a proper \n",
        "  TensorDataset using the tokenizer.\n",
        "  INPUTS:\n",
        "  - sentences: pandas dataframe, sentences that we want to analyze.\n",
        "  - labels: pandas dataframe, labels of sentences\n",
        "  - tokenizer: tokenizer object\n",
        "  - max_seq_length: int, max length of a sentence.\n",
        "  - label_list: list of strings, reprensenting all the different labels.\n",
        "  OUTPUTS:\n",
        "  - dataset: TensorDataset torch, organized dataset for training and testing.\n",
        "  \"\"\"\n",
        "  processor = DataProcessor()\n",
        "  examples = processor.get_data_examples(sentences, labels)\n",
        "\n",
        "  features = convert_examples_to_features(examples, tokenizer, max_length=max_seq_length,\n",
        "                                          label_list = label_list,\n",
        "                                          pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "                                          pad_token_segment_id=0)\n",
        "\n",
        "  # Convert to Tensors and build dataset\n",
        "  all_input_ids = th.tensor([f.input_ids for f in features], dtype=th.long)\n",
        "  all_attention_mask = th.tensor([f.attention_mask for f in features], dtype=th.long)\n",
        "  all_token_type_ids = th.tensor([f.token_type_ids for f in features], dtype=th.long)\n",
        "  all_labels = th.tensor([f.label for f in features], dtype=th.long)\n",
        "\n",
        "  dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0PMfFAibrOF",
        "colab_type": "text"
      },
      "source": [
        "### Training BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFEpGzyxN7UB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, eval_dataset, tokenizer, eval_batch_size=8):\n",
        "  \"\"\"\n",
        "  This function evaluates the performance of the model on the eval dataset.\n",
        "  INPUTS:\n",
        "  - model: Torch model, model to evaluate\n",
        "  - eval_datastet: Torch Dataset, evaluation set\n",
        "  - tokenizer: tokenizer object\n",
        "  - eval_batch_size: int, size of batch size for evaluation.\n",
        "  OUTPUTS:\n",
        "  - result: dictionary, contains loss, accuracy and f1_score\n",
        "  \"\"\"      \n",
        "  eval_batch_size = eval_batch_size\n",
        "  eval_sampler = SequentialSampler(eval_dataset)\n",
        "  eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n",
        "  \n",
        "  t0 = time.time()\n",
        "  print(\"Running evaluation...\", end= '', flush=True)\n",
        "      \n",
        "  eval_loss = 0.0\n",
        "  nb_eval_steps = 0\n",
        "  out_label_ids = None\n",
        "  iterator = eval_dataloader\n",
        "  \n",
        "  preds = None\n",
        "\n",
        "  for batch in iterator:\n",
        "    model.eval()\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    with th.no_grad():\n",
        "      inputs = {'input_ids':      batch[0],\n",
        "                'attention_mask': batch[1],\n",
        "                'labels':         batch[3]}\n",
        "      inputs['token_type_ids'] = batch[2] #or None\n",
        "      outputs = model(**inputs)\n",
        "      tmp_eval_loss, logits = outputs[:2]\n",
        "      eval_loss += tmp_eval_loss.mean().item()\n",
        "    nb_eval_steps += 1\n",
        "    if preds is None:\n",
        "      preds = logits.detach().cpu().numpy()\n",
        "      out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "    else:\n",
        "      preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "      out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "  eval_loss = eval_loss / nb_eval_steps\n",
        "  preds_class = np.argmax(preds, axis=1)\n",
        "  # print(out_label_ids)\n",
        "  # print(preds_class)\n",
        "  acc = accuracy_score(out_label_ids, preds_class)\n",
        "  f1 = f1_score(out_label_ids, preds_class)\n",
        "  \n",
        "  result = {\"val_loss\": eval_loss, \"val_acc\" : acc, \"f1\": f1}\n",
        "  t1 = time.time()\n",
        "  print(\"Done. {:.2f}\".format(t1-t0))\n",
        "  return result\n",
        "\n",
        "def train_BERT_model(model, train, val, test, tokenizer, learning_rate, \n",
        "                     batch_size, num_epochs, warmup_steps=0, num_cycles=1.0,\n",
        "                     gradient_accumulation_steps=1):\n",
        "  '''\n",
        "  Train the model on the train set.\n",
        "\n",
        "  INPUTS:\n",
        "  - model: Net in pytorch.\n",
        "  - train: whole training dataframe (pandas)\n",
        "  - val: The whole validation dataframe (pandas).\n",
        "  - test: pandas dataframe, test dataset\n",
        "  - tokenizer: tokenizer object, pretrained\n",
        "  - learning_rate: float, learning rate\n",
        "  - batch_size: int, size of mini-batches\n",
        "  - num_epochs: int, number of epochs\n",
        "  - warmup_steps: int, parameter of AdamW\n",
        "  - num_cycles: float, parameter of AdamW\n",
        "  OUTPUTS:\n",
        "  - NONE\n",
        "  COMPUTATION TIME:\n",
        "  Extremely long and costly in RAM\n",
        "  '''\n",
        "  model.train()\n",
        "  model.zero_grad()\n",
        "  list_perfs = []\n",
        "\n",
        "  train_sampler = RandomSampler(train)\n",
        "  train_dataloader = DataLoader(train, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "  t_total = len(train_dataloader) // gradient_accumulation_steps * num_epochs\n",
        "\n",
        "  optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
        "  scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer,\n",
        "                                                                 num_warmup_steps=warmup_steps,\n",
        "                                                                 num_training_steps=t_total,\n",
        "                                                                 num_cycles=num_cycles)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    ticks0 = time.time()\n",
        "    print(\"Epoch :\", epoch + 1)\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    epoch_iterator = train_dataloader\n",
        "    for step, batch in enumerate(epoch_iterator): \n",
        "      model.train()\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      inputs = {'input_ids':      batch[0],\n",
        "                'attention_mask': batch[1],\n",
        "                'labels':         batch[3]}\n",
        "      inputs['token_type_ids'] = batch[2] \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(**inputs)\n",
        "      loss = outputs[0]  \n",
        "      losses.append(loss.item())\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()  # Update learning rate schedule\n",
        "      model.zero_grad()\n",
        "      if step%10 == 0:\n",
        "        print(\"Lr: {:.6f}\".format(scheduler.get_lr()[0]), \" | Train Loss: {:.4f}\", loss.item())\n",
        "\n",
        "      if step%100 == 0 : #and step > 0\n",
        "        model.eval()\n",
        "        results = evaluate(model=model, eval_dataset=val, tokenizer=tokenizer)\n",
        "        val_losses += [results[\"val_loss\"]]\n",
        "        model.train()\n",
        "        print(\"Lr: {:.6f} | Train loss: {:.4f} | Val loss: {:.4f}, Val acc: {:.4f} | {}\".format(scheduler.get_lr()[0],\n",
        "                                                                                       loss.item(),\n",
        "                                                                                       results[\"val_loss\"], \n",
        "                                                                                       results[\"val_acc\"],\n",
        "                                                                                       step*batch_size))\n",
        "\n",
        "    ticks3 = time.time()\n",
        "    print(\"Done.\")\n",
        "    print(\"Epoch n°{}: TRAIN LOSS {:.4f} | VAL LOSS {:.4f} | TOTAL TIME: {:.2f}s.\".format(epoch+1, \n",
        "                                                                        sum(losses)/len(losses),\n",
        "                                                                        sum(val_losses)/len(val_losses),\n",
        "                                                                        ticks3-ticks0))\n",
        "    model.eval()\n",
        "    results = evaluate(model=model, eval_dataset=test, tokenizer=tokenizer)\n",
        "    th.save(model.state_dict(), \"/content/epoch_{}.pt\".format(epoch+1))\n",
        "    list_perfs.append(results[\"f1\"])\n",
        "    print(\"F1 score: {:.4f}\".format(results[\"f1\"]))\n",
        "    model.train()\n",
        "    print(\"______________________________________________________________________\")\n",
        "  index_best = np.argmax(np.array(list_perfs))\n",
        "  print(\"Loading model with best performances: epoch {}\".format(index_best+1))\n",
        "  model = th.load(\"/content/epoch_{}.pt\".format(index_best+1))\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOWRCwWD9JNf",
        "colab_type": "code",
        "outputId": "bfbbe77b-7125-4898-8749-cc7312b00b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "print(validate_df)\n",
        "print(validate_df_bis)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          qid  ... target\n",
            "738975   90b952c8cf6e77763868  ...      0\n",
            "616701   78c48243c367915c6071  ...      0\n",
            "893661   af185eb18ed4da49b8bc  ...      1\n",
            "872857   ab036f055ee94694d16c  ...      0\n",
            "25791    050ed8ac80d55a03b15a  ...      0\n",
            "...                       ...  ...    ...\n",
            "1081783  d3fdfb65b0da453858da  ...      0\n",
            "1075448  d2bc3b3a2112f57efa99  ...      0\n",
            "1020524  c7fb6feff7eda31015f7  ...      0\n",
            "1252564  f5765e8a05893e5e5eba  ...      1\n",
            "339675   428b9d0d5562f9e2633f  ...      0\n",
            "\n",
            "[130613 rows x 3 columns]\n",
            "                          qid  ... target\n",
            "656282   808c9dcd10446bcb9b6d  ...      0\n",
            "779414   98ae700637a9d6b74820  ...      0\n",
            "553376   6c6d5ac4b0c3d1c738a6  ...      0\n",
            "414991   5151b208697733125dc8  ...      0\n",
            "32719    06678cfbd1af160b45e3  ...      0\n",
            "...                       ...  ...    ...\n",
            "850561   a6a8c3dafa8eaf533851  ...      0\n",
            "1177724  e6c86e61d7b8897221e2  ...      0\n",
            "436921   559e29acce0383fcec7b  ...      1\n",
            "1032852  ca64b751a178414b447a  ...      0\n",
            "287700   385727d951e9df8dd60e  ...      0\n",
            "\n",
            "[58776 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoO1leJ0P1xS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_size = 30\n",
        "nb_labels = 2\n",
        "learning_rate = 3e-5\n",
        "\n",
        "batch_size = 200\n",
        "num_epochs = 2\n",
        "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# input_ids = th.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "# labels = th.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "# outputs = model(input_ids, labels=labels)\n",
        "\n",
        "# loss, logits = outputs[:2]\n",
        "try:\n",
        "  sentences_tr = train_df_bis[1].values\n",
        "  labels_tr = train_df_bis[2].values\n",
        "  sentences_val = validate_df_bis[1].values\n",
        "  labels_val = validate_df_bis[2].values\n",
        "except:\n",
        "  sentences_tr = train_df_bis[\"question_text\"].values\n",
        "  labels_tr = train_df_bis[\"target\"].values\n",
        "  sentences_val = validate_df_bis[\"question_text\"].values\n",
        "  labels_val = validate_df_bis[\"target\"].values\n",
        "  pass\n",
        "sentences_test = validate_df[\"question_text\"].values\n",
        "labels_test = validate_df[\"target\"].values\n",
        "\n",
        "dataset_tr = load_examples(sentences=sentences_tr, labels=labels_tr,\n",
        "                            tokenizer=tokenizer, max_seq_length = sentence_size,\n",
        "                            label_list = [str(j) for j in range(nb_labels)])\n",
        "\n",
        "dataset_val = load_examples(sentences=sentences_val, labels=labels_val,\n",
        "                            tokenizer=tokenizer, max_seq_length = sentence_size,\n",
        "                            label_list = [str(j) for j in range(nb_labels)])\n",
        "\n",
        "dataset_test = load_examples(sentences=sentences_test, labels=labels_test,\n",
        "                            tokenizer=tokenizer, max_seq_length = sentence_size,\n",
        "                            label_list = [str(j) for j in range(nb_labels)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_GOqY6Ti_QM",
        "colab_type": "code",
        "outputId": "0b920b65-53f2-48ca-fd37-6efd8783ced7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3bea31bb078a485eb08b0074737043c6",
            "ff32d71ae94548aca61ff7f8e4b5327a",
            "09f2180111514472b403b5657a6cc50b",
            "a853bf9042b84642b37b6604023bc77b",
            "8723b0ef292a43f9822fc373e62d3e06",
            "6c5dd20967564e87af104490ad74f888",
            "30704839d7484b7787452ad282f67ce5",
            "b187ce138ae34618807d99e2f4539451",
            "52288c16a8204513b8118eb143bc2a43",
            "724ae28444bc4c03b79a9387e5183d69",
            "b30f34f70f914b8892023d6f2b718b45",
            "2d2f13e83e2e463e9a2510ae76a7d41e",
            "ff85d0ae23d74e0dabda0c01eca1965a",
            "6854144dd0b848bda4e8f81968070549",
            "6bd0ec4d43ff4932a78a85fd3fed8c4a",
            "e9afd75522b447719ac8a8d1d0c3c5a2"
          ]
        }
      },
      "source": [
        "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, \n",
        "                                      finetuning_task='classification')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
        "                                                      config=config)\n",
        "model = model.to(device)\n",
        "train_BERT_model(model, dataset_tr, dataset_val, dataset_test, tokenizer,\n",
        "                 learning_rate, batch_size, num_epochs)\n",
        "#path = \"/content/drive/My Drive/Cours/Kaggle/\"\n",
        "#th.save(model.state_dict(), path+'modelBERT.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bea31bb078a485eb08b0074737043c6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52288c16a8204513b8118eb143bc2a43",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch : 1\n",
            "Lr: 2.9999994052919367e-05  | Train Loss: 0.9654431939125061\n",
            "Running evaluation..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done. 321.87\n",
            "Lr: 2.9999994052919367e-05 | Train loss: 0.9654 | Val loss: 0.6866, Val acc: 0.5330 | 0\n",
            "Lr: 2.9999280408948928e-05  | Train Loss: 0.3273201286792755\n",
            "Lr: 2.99973774136912e-05  | Train Loss: 0.23106227815151215\n",
            "Lr: 2.9994285218042063e-05  | Train Loss: 0.10356522351503372\n",
            "Lr: 2.9990004067193748e-05  | Train Loss: 0.20537562668323517\n",
            "Lr: 2.9984534300615354e-05  | Train Loss: 0.11596480011940002\n",
            "Lr: 2.9977876352025952e-05  | Train Loss: 0.1523970365524292\n",
            "Lr: 2.997003074936018e-05  | Train Loss: 0.1660945564508438\n",
            "Lr: 2.9960998114726396e-05  | Train Loss: 0.07099092751741409\n",
            "Lr: 2.9950779164357328e-05  | Train Loss: 0.11807925254106522\n",
            "Lr: 2.99393747085533e-05  | Train Loss: 0.1300225704908371\n",
            "Running evaluation...Done. 324.04\n",
            "Lr: 2.99393747085533e-05 | Train loss: 0.1300 | Val loss: 0.1247, Val acc: 0.9462 | 20000\n",
            "Lr: 2.9926785651617986e-05  | Train Loss: 0.18724480271339417\n",
            "Lr: 2.9913012991786666e-05  | Train Loss: 0.17069974541664124\n",
            "Lr: 2.9898057821147116e-05  | Train Loss: 0.1487574279308319\n",
            "Lr: 2.9881921325552987e-05  | Train Loss: 0.09664477407932281\n",
            "Lr: 2.986460478452978e-05  | Train Loss: 0.0849054604768753\n",
            "Lr: 2.984610957117339e-05  | Train Loss: 0.1053105890750885\n",
            "Lr: 2.982643715204123e-05  | Train Loss: 0.1809905469417572\n",
            "Lr: 2.980558908703593e-05  | Train Loss: 0.11759242415428162\n",
            "Lr: 2.9783567029281658e-05  | Train Loss: 0.05883125960826874\n",
            "Lr: 2.9760372724993044e-05  | Train Loss: 0.1729576736688614\n",
            "Running evaluation...Done. 324.95\n",
            "Lr: 2.9760372724993044e-05 | Train loss: 0.1730 | Val loss: 0.1110, Val acc: 0.9567 | 40000\n",
            "Lr: 2.9736008013336676e-05  | Train Loss: 0.12781637907028198\n",
            "Lr: 2.9710474826285338e-05  | Train Loss: 0.08722461760044098\n",
            "Lr: 2.968377518846473e-05  | Train Loss: 0.11531233787536621\n",
            "Lr: 2.9655911216992994e-05  | Train Loss: 0.09096476435661316\n",
            "Lr: 2.962688512131282e-05  | Train Loss: 0.10788930207490921\n",
            "Lr: 2.959669920301623e-05  | Train Loss: 0.1172654926776886\n",
            "Lr: 2.9565355855662112e-05  | Train Loss: 0.09658979624509811\n",
            "Lr: 2.9532857564586388e-05  | Train Loss: 0.11570184677839279\n",
            "Lr: 2.9499206906704972e-05  | Train Loss: 0.1565917581319809\n",
            "Lr: 2.9464406550309417e-05  | Train Loss: 0.08197721838951111\n",
            "Running evaluation...Done. 324.77\n",
            "Lr: 2.9464406550309417e-05 | Train loss: 0.0820 | Val loss: 0.1101, Val acc: 0.9562 | 60000\n",
            "Lr: 2.942845925485535e-05  | Train Loss: 0.13928073644638062\n",
            "Lr: 2.9391367870743647e-05  | Train Loss: 0.07340149581432343\n",
            "Lr: 2.9353135339094442e-05  | Train Loss: 0.1532749980688095\n",
            "Lr: 2.9313764691513882e-05  | Train Loss: 0.1056828424334526\n",
            "Lr: 2.9273259049853748e-05  | Train Loss: 0.1193021610379219\n",
            "Lr: 2.9231621625963935e-05  | Train Loss: 0.11141347140073776\n",
            "Lr: 2.918885572143774e-05  | Train Loss: 0.12588846683502197\n",
            "Lr: 2.914496472735008e-05  | Train Loss: 0.10505413264036179\n",
            "Lr: 2.9099952123988603e-05  | Train Loss: 0.056976303458213806\n",
            "Lr: 2.9053821480577712e-05  | Train Loss: 0.08256983757019043\n",
            "Running evaluation...Done. 324.11\n",
            "Lr: 2.9053821480577712e-05 | Train loss: 0.0826 | Val loss: 0.1047, Val acc: 0.9585 | 80000\n",
            "Lr: 2.9006576454995574e-05  | Train Loss: 0.12006998807191849\n",
            "Lr: 2.8958220793484027e-05  | Train Loss: 0.08062901347875595\n",
            "Lr: 2.8908758330351572e-05  | Train Loss: 0.08421852439641953\n",
            "Lr: 2.8858192987669303e-05  | Train Loss: 0.07859769463539124\n",
            "Lr: 2.880652877495993e-05  | Train Loss: 0.08434572070837021\n",
            "Lr: 2.8753769788879844e-05  | Train Loss: 0.09485968947410583\n",
            "Lr: 2.8699920212894265e-05  | Train Loss: 0.1547834873199463\n",
            "Lr: 2.8644984316945543e-05  | Train Loss: 0.0673222467303276\n",
            "Lr: 2.8588966457114544e-05  | Train Loss: 0.09527120739221573\n",
            "Lr: 2.853187107527528e-05  | Train Loss: 0.05650217831134796\n",
            "Running evaluation...Done. 324.15\n",
            "Lr: 2.853187107527528e-05 | Train loss: 0.0565 | Val loss: 0.1053, Val acc: 0.9592 | 100000\n",
            "Lr: 2.8473702698742662e-05  | Train Loss: 0.09124335646629333\n",
            "Lr: 2.8414465939913527e-05  | Train Loss: 0.1010364517569542\n",
            "Lr: 2.8354165495900902e-05  | Train Loss: 0.07939687371253967\n",
            "Lr: 2.829280614816154e-05  | Train Loss: 0.12926416099071503\n",
            "Lr: 2.8230392762116805e-05  | Train Loss: 0.10071421414613724\n",
            "Lr: 2.8166930286766845e-05  | Train Loss: 0.09263842552900314\n",
            "Lr: 2.810242375429818e-05  | Train Loss: 0.10641483962535858\n",
            "Lr: 2.8036878279684698e-05  | Train Loss: 0.15317696332931519\n",
            "Lr: 2.7970299060282026e-05  | Train Loss: 0.07961582392454147\n",
            "Lr: 2.790269137541545e-05  | Train Loss: 0.10043161362409592\n",
            "Running evaluation...Done. 323.50\n",
            "Lr: 2.790269137541545e-05 | Train loss: 0.1004 | Val loss: 0.1043, Val acc: 0.9589 | 120000\n",
            "Lr: 2.7834060585961288e-05  | Train Loss: 0.10968754440546036\n",
            "Lr: 2.7764412133921785e-05  | Train Loss: 0.04885563254356384\n",
            "Lr: 2.7693751541993623e-05  | Train Loss: 0.10072138905525208\n",
            "Lr: 2.762208441312999e-05  | Train Loss: 0.09384118020534515\n",
            "Lr: 2.7549416430096298e-05  | Train Loss: 0.10974948853254318\n",
            "Lr: 2.7475753355019586e-05  | Train Loss: 0.08660588413476944\n",
            "Lr: 2.7401101028931607e-05  | Train Loss: 0.09822279214859009\n",
            "Lr: 2.732546537130568e-05  | Train Loss: 0.10147719085216522\n",
            "Lr: 2.7248852379587303e-05  | Train Loss: 0.09250681102275848\n",
            "Lr: 2.717126812871859e-05  | Train Loss: 0.08712547272443771\n",
            "Running evaluation...Done. 321.71\n",
            "Lr: 2.717126812871859e-05 | Train loss: 0.0871 | Val loss: 0.1061, Val acc: 0.9589 | 140000\n",
            "Lr: 2.709271877065658e-05  | Train Loss: 0.11626486480236053\n",
            "Lr: 2.701321053388542e-05  | Train Loss: 0.07110416144132614\n",
            "Lr: 2.6932749722922462e-05  | Train Loss: 0.17150580883026123\n",
            "Lr: 2.685134271781838e-05  | Train Loss: 0.062475670129060745\n",
            "Lr: 2.6768995973651268e-05  | Train Loss: 0.07298672199249268\n",
            "Lr: 2.6685716020014787e-05  | Train Loss: 0.07455858588218689\n",
            "Lr: 2.6601509460500382e-05  | Train Loss: 0.09444145858287811\n",
            "Lr: 2.6516382972173713e-05  | Train Loss: 0.07585146278142929\n",
            "Lr: 2.643034330504516e-05  | Train Loss: 0.07922784984111786\n",
            "Lr: 2.6343397281534606e-05  | Train Loss: 0.11805160343647003\n",
            "Running evaluation...Done. 321.74\n",
            "Lr: 2.6343397281534606e-05 | Train loss: 0.1181 | Val loss: 0.1006, Val acc: 0.9607 | 160000\n",
            "Lr: 2.6255551795930453e-05  | Train Loss: 0.10268443077802658\n",
            "Lr: 2.6166813813842948e-05  | Train Loss: 0.09905552864074707\n",
            "Lr: 2.607719037165186e-05  | Train Loss: 0.10913404822349548\n",
            "Lr: 2.598668857594855e-05  | Train Loss: 0.1031111627817154\n",
            "Lr: 2.5895315602972408e-05  | Train Loss: 0.11937131732702255\n",
            "Lr: 2.58030786980419e-05  | Train Loss: 0.07881966233253479\n",
            "Lr: 2.5709985174979987e-05  | Train Loss: 0.11487239599227905\n",
            "Lr: 2.5616042415534223e-05  | Train Loss: 0.06479734182357788\n",
            "Lr: 2.5521257868791426e-05  | Train Loss: 0.06808003783226013\n",
            "Lr: 2.5425639050586992e-05  | Train Loss: 0.08929315209388733\n",
            "Running evaluation...Done. 321.60\n",
            "Lr: 2.5425639050586992e-05 | Train loss: 0.0893 | Val loss: 0.0996, Val acc: 0.9613 | 180000\n",
            "Lr: 2.5329193542908955e-05  | Train Loss: 0.0959266647696495\n",
            "Lr: 2.523192899329677e-05  | Train Loss: 0.10927076637744904\n",
            "Lr: 2.5133853114234907e-05  | Train Loss: 0.06451019644737244\n",
            "Lr: 2.503497368254131e-05  | Train Loss: 0.10788865387439728\n",
            "Lr: 2.493529853875074e-05  | Train Loss: 0.10295553505420685\n",
            "Lr: 2.4834835586493054e-05  | Train Loss: 0.07941915839910507\n",
            "Lr: 2.4733592791866523e-05  | Train Loss: 0.06283347308635712\n",
            "Lr: 2.4631578182806132e-05  | Train Loss: 0.10709211975336075\n",
            "Lr: 2.4528799848447057e-05  | Train Loss: 0.16759145259857178\n",
            "Lr: 2.442526593848321e-05  | Train Loss: 0.16394628584384918\n",
            "Running evaluation...Done. 322.00\n",
            "Lr: 2.442526593848321e-05 | Train loss: 0.1639 | Val loss: 0.0982, Val acc: 0.9619 | 200000\n",
            "Lr: 2.432098466252103e-05  | Train Loss: 0.12106838822364807\n",
            "Lr: 2.421596428942852e-05  | Train Loss: 0.06381839513778687\n",
            "Lr: 2.4110213146679568e-05  | Train Loss: 0.07206543534994125\n",
            "Lr: 2.4003739619693633e-05  | Train Loss: 0.08262739330530167\n",
            "Lr: 2.3896552151170823e-05  | Train Loss: 0.08281548321247101\n",
            "Lr: 2.378865924042245e-05  | Train Loss: 0.08194582909345627\n",
            "Lr: 2.368006944269709e-05  | Train Loss: 0.0670703575015068\n",
            "Lr: 2.3570791368502185e-05  | Train Loss: 0.11360087990760803\n",
            "Lr: 2.3460833682921307e-05  | Train Loss: 0.14028698205947876\n",
            "Lr: 2.3350205104927063e-05  | Train Loss: 0.1282411366701126\n",
            "Running evaluation...Done. 320.66\n",
            "Lr: 2.3350205104927063e-05 | Train loss: 0.1282 | Val loss: 0.1002, Val acc: 0.9606 | 220000\n",
            "Lr: 2.3238914406689714e-05  | Train Loss: 0.05354468524456024\n",
            "Lr: 2.312697041288162e-05  | Train Loss: 0.06483175605535507\n",
            "Lr: 2.3014381999977484e-05  | Train Loss: 0.09460578113794327\n",
            "Lr: 2.290115809555051e-05  | Train Loss: 0.10777470469474792\n",
            "Lr: 2.2787307677564494e-05  | Train Loss: 0.08686951547861099\n",
            "Lr: 2.2672839773661928e-05  | Train Loss: 0.12208812683820724\n",
            "Lr: 2.2557763460448162e-05  | Train Loss: 0.09393841028213501\n",
            "Done.\n",
            "Epoch n°1: TRAIN LOSS 0.1149 | VAL LOSS 0.1543 | TOTAL TIME: 5833.50s.\n",
            "Running evaluation...Done. 716.38\n",
            "F1 score: 0.6590\n",
            "______________________________________________________________________\n",
            "Epoch : 2\n",
            "Lr: 2.248842943000224e-05  | Train Loss: 0.1250523328781128\n",
            "Running evaluation...Done. 320.45\n",
            "Lr: 2.248842943000224e-05 | Train loss: 0.1251 | Val loss: 0.0968, Val acc: 0.9622 | 0\n",
            "Lr: 2.2372398661373795e-05  | Train Loss: 0.10139051079750061\n",
            "Lr: 2.2255783306578596e-05  | Train Loss: 0.04328086972236633\n",
            "Lr: 2.213859261250173e-05  | Train Loss: 0.06297464668750763\n",
            "Lr: 2.2020835871649168e-05  | Train Loss: 0.0750730112195015\n",
            "Lr: 2.190252242141095e-05  | Train Loss: 0.06606648117303848\n",
            "Lr: 2.1783661643320744e-05  | Train Loss: 0.09552592039108276\n",
            "Lr: 2.166426296231199e-05  | Train Loss: 0.05783635005354881\n",
            "Lr: 2.1544335845970546e-05  | Train Loss: 0.09512383490800858\n",
            "Lr: 2.142388980378394e-05  | Train Loss: 0.07454965263605118\n",
            "Lr: 2.1302934386387376e-05  | Train Loss: 0.09245657920837402\n",
            "Running evaluation...Done. 321.21\n",
            "Lr: 2.1302934386387376e-05 | Train loss: 0.0925 | Val loss: 0.1036, Val acc: 0.9615 | 20000\n",
            "Lr: 2.1181479184806383e-05  | Train Loss: 0.07457998394966125\n",
            "Lr: 2.1059533829696325e-05  | Train Loss: 0.09051547199487686\n",
            "Lr: 2.0937107990578752e-05  | Train Loss: 0.0708652213215828\n",
            "Lr: 2.081421137507465e-05  | Train Loss: 0.08991824835538864\n",
            "Lr: 2.06908537281347e-05  | Train Loss: 0.05389256030321121\n",
            "Lr: 2.056704483126657e-05  | Train Loss: 0.0781637653708458\n",
            "Lr: 2.0442794501759272e-05  | Train Loss: 0.11659963428974152\n",
            "Lr: 2.031811259190474e-05  | Train Loss: 0.050443802028894424\n",
            "Lr: 2.0193008988216585e-05  | Train Loss: 0.08559581637382507\n",
            "Lr: 2.0067493610646175e-05  | Train Loss: 0.08048251271247864\n",
            "Running evaluation...Done. 321.38\n",
            "Lr: 2.0067493610646175e-05 | Train loss: 0.0805 | Val loss: 0.1020, Val acc: 0.9613 | 40000\n",
            "Lr: 1.9941576411796018e-05  | Train Loss: 0.13142889738082886\n",
            "Lr: 1.98152673761306e-05  | Train Loss: 0.03424232453107834\n",
            "Lr: 1.968857651918467e-05  | Train Loss: 0.13231584429740906\n",
            "Lr: 1.9561513886769073e-05  | Train Loss: 0.09220180660486221\n",
            "Lr: 1.9434089554174183e-05  | Train Loss: 0.065158411860466\n",
            "Lr: 1.930631362537097e-05  | Train Loss: 0.07015189528465271\n",
            "Lr: 1.9178196232209855e-05  | Train Loss: 0.08336827158927917\n",
            "Lr: 1.9049747533617292e-05  | Train Loss: 0.06002604588866234\n",
            "Lr: 1.8920977714790224e-05  | Train Loss: 0.03681028261780739\n",
            "Lr: 1.8791896986388462e-05  | Train Loss: 0.06176125630736351\n",
            "Running evaluation...Done. 320.85\n",
            "Lr: 1.8791896986388462e-05 | Train loss: 0.0618 | Val loss: 0.1012, Val acc: 0.9614 | 60000\n",
            "Lr: 1.866251558372507e-05  | Train Loss: 0.08112785220146179\n",
            "Lr: 1.8532843765954717e-05  | Train Loss: 0.1242515966296196\n",
            "Lr: 1.8402891815260225e-05  | Train Loss: 0.11998763680458069\n",
            "Lr: 1.8272670036037264e-05  | Train Loss: 0.07418893277645111\n",
            "Lr: 1.8142188754077218e-05  | Train Loss: 0.10538680851459503\n",
            "Lr: 1.8011458315748492e-05  | Train Loss: 0.06110186129808426\n",
            "Lr: 1.7880489087176044e-05  | Train Loss: 0.0348430834710598\n",
            "Lr: 1.7749291453419445e-05  | Train Loss: 0.06120225787162781\n",
            "Lr: 1.7617875817649413e-05  | Train Loss: 0.07755760848522186\n",
            "Lr: 1.748625260032288e-05  | Train Loss: 0.04898793622851372\n",
            "Running evaluation...Done. 321.54\n",
            "Lr: 1.748625260032288e-05 | Train loss: 0.0490 | Val loss: 0.1087, Val acc: 0.9605 | 80000\n",
            "Lr: 1.735443223835672e-05  | Train Loss: 0.15599817037582397\n",
            "Lr: 1.7222425184300205e-05  | Train Loss: 0.08541581779718399\n",
            "Lr: 1.7090241905506105e-05  | Train Loss: 0.06915214657783508\n",
            "Lr: 1.6957892883300778e-05  | Train Loss: 0.07025500386953354\n",
            "Lr: 1.6825388612152985e-05  | Train Loss: 0.04210719093680382\n",
            "Lr: 1.66927395988418e-05  | Train Loss: 0.1077440157532692\n",
            "Lr: 1.6559956361623468e-05  | Train Loss: 0.09479963034391403\n",
            "Lr: 1.6427049429397357e-05  | Train Loss: 0.06154610961675644\n",
            "Lr: 1.629402934087111e-05  | Train Loss: 0.05745835602283478\n",
            "Lr: 1.6160906643724944e-05  | Train Loss: 0.04735134169459343\n",
            "Running evaluation...Done. 323.02\n",
            "Lr: 1.6160906643724944e-05 | Train loss: 0.0474 | Val loss: 0.1034, Val acc: 0.9611 | 100000\n",
            "Lr: 1.602769189377535e-05  | Train Loss: 0.09768465906381607\n",
            "Lr: 1.589439565413802e-05  | Train Loss: 0.10546201467514038\n",
            "Lr: 1.5761028494390282e-05  | Train Loss: 0.06381196528673172\n",
            "Lr: 1.5627600989733004e-05  | Train Loss: 0.05623288452625275\n",
            "Lr: 1.549412372015203e-05  | Train Loss: 0.043053437024354935\n",
            "Lr: 1.5360607269579253e-05  | Train Loss: 0.0735771507024765\n",
            "Lr: 1.5227062225053401e-05  | Train Loss: 0.06992271542549133\n",
            "Lr: 1.5093499175880504e-05  | Train Loss: 0.13128547370433807\n",
            "Lr: 1.4959928712794262e-05  | Train Loss: 0.08060472458600998\n",
            "Lr: 1.482636142711625e-05  | Train Loss: 0.11378009617328644\n",
            "Running evaluation...Done. 322.23\n",
            "Lr: 1.482636142711625e-05 | Train loss: 0.1138 | Val loss: 0.1095, Val acc: 0.9606 | 120000\n",
            "Lr: 1.4692807909916099e-05  | Train Loss: 0.0978490337729454\n",
            "Lr: 1.4559278751171668e-05  | Train Loss: 0.13514043390750885\n",
            "Lr: 1.4425784538929362e-05  | Train Loss: 0.07869069278240204\n",
            "Lr: 1.4292335858464529e-05  | Train Loss: 0.04015378654003143\n",
            "Lr: 1.4158943291442121e-05  | Train Loss: 0.04810161888599396\n",
            "Lr: 1.4025617415077655e-05  | Train Loss: 0.08299200236797333\n",
            "Lr: 1.3892368801298462e-05  | Train Loss: 0.04373672604560852\n",
            "Lr: 1.3759208015905438e-05  | Train Loss: 0.10351808369159698\n",
            "Lr: 1.3626145617735223e-05  | Train Loss: 0.0852251946926117\n",
            "Lr: 1.3493192157822946e-05  | Train Loss: 0.09922322630882263\n",
            "Running evaluation...Done. 322.31\n",
            "Lr: 1.3493192157822946e-05 | Train loss: 0.0992 | Val loss: 0.1021, Val acc: 0.9609 | 140000\n",
            "Lr: 1.336035817856561e-05  | Train Loss: 0.07303666323423386\n",
            "Lr: 1.3227654212886108e-05  | Train Loss: 0.06536149233579636\n",
            "Lr: 1.309509078339808e-05  | Train Loss: 0.060955971479415894\n",
            "Lr: 1.296267840157148e-05  | Train Loss: 0.06014062836766243\n",
            "Lr: 1.2830427566899127e-05  | Train Loss: 0.0515042245388031\n",
            "Lr: 1.2698348766064122e-05  | Train Loss: 0.0905279740691185\n",
            "Lr: 1.2566452472108332e-05  | Train Loss: 0.07188811898231506\n",
            "Lr: 1.243474914360196e-05  | Train Loss: 0.06979914754629135\n",
            "Lr: 1.2303249223814211e-05  | Train Loss: 0.07481284439563751\n",
            "Lr: 1.217196313988523e-05  | Train Loss: 0.08601151406764984\n",
            "Running evaluation...Done. 321.59\n",
            "Lr: 1.217196313988523e-05 | Train loss: 0.0860 | Val loss: 0.1034, Val acc: 0.9619 | 160000\n",
            "Lr: 1.2040901301999294e-05  | Train Loss: 0.0961264967918396\n",
            "Lr: 1.1910074102559317e-05  | Train Loss: 0.10211717337369919\n",
            "Lr: 1.1779491915362829e-05  | Train Loss: 0.08000347018241882\n",
            "Lr: 1.1649165094779393e-05  | Train Loss: 0.07976381480693817\n",
            "Lr: 1.151910397492955e-05  | Train Loss: 0.046765901148319244\n",
            "Lr: 1.1389318868865408e-05  | Train Loss: 0.08717130869626999\n",
            "Lr: 1.125982006775285e-05  | Train Loss: 0.06779975444078445\n",
            "Lr: 1.1130617840055537e-05  | Train Loss: 0.04809442535042763\n",
            "Lr: 1.1001722430720674e-05  | Train Loss: 0.02549971267580986\n",
            "Lr: 1.087314406036664e-05  | Train Loss: 0.05985014885663986\n",
            "Running evaluation...Done. 322.93\n",
            "Lr: 1.087314406036664e-05 | Train loss: 0.0599 | Val loss: 0.1020, Val acc: 0.9619 | 180000\n",
            "Lr: 1.0744892924472554e-05  | Train Loss: 0.09506024420261383\n",
            "Lr: 1.0616979192569862e-05  | Train Loss: 0.08985801041126251\n",
            "Lr: 1.0489413007435905e-05  | Train Loss: 0.05455266311764717\n",
            "Lr: 1.0362204484289708e-05  | Train Loss: 0.09428830444812775\n",
            "Lr: 1.0235363709989888e-05  | Train Loss: 0.09500611573457718\n",
            "Lr: 1.010890074223482e-05  | Train Loss: 0.050858136266469955\n",
            "Lr: 9.982825608765127e-06  | Train Loss: 0.061494652181863785\n",
            "Lr: 9.857148306568536e-06  | Train Loss: 0.08505483716726303\n",
            "Lr: 9.731878801087208e-06  | Train Loss: 0.05497270077466965\n",
            "Lr: 9.607027025427487e-06  | Train Loss: 0.0830179899930954\n",
            "Running evaluation...Done. 320.15\n",
            "Lr: 9.607027025427487e-06 | Train loss: 0.0830 | Val loss: 0.1056, Val acc: 0.9621 | 200000\n",
            "Lr: 9.48260287957232e-06  | Train Loss: 0.12165635079145432\n",
            "Lr: 9.358616229596213e-06  | Train Loss: 0.0976380929350853\n",
            "Lr: 9.235076906882911e-06  | Train Loss: 0.057814404368400574\n",
            "Lr: 9.111994707345856e-06  | Train Loss: 0.07888133823871613\n",
            "Lr: 8.989379390651414e-06  | Train Loss: 0.058931875973939896\n",
            "Lr: 8.867240679444986e-06  | Train Loss: 0.08262564986944199\n",
            "Lr: 8.745588258580084e-06  | Train Loss: 0.1104498878121376\n",
            "Lr: 8.62443177435034e-06  | Train Loss: 0.049333829432725906\n",
            "Lr: 8.50378083372467e-06  | Train Loss: 0.1575285792350769\n",
            "Lr: 8.383645003585459e-06  | Train Loss: 0.061836980283260345\n",
            "Running evaluation...Done. 322.17\n",
            "Lr: 8.383645003585459e-06 | Train loss: 0.0618 | Val loss: 0.0994, Val acc: 0.9620 | 220000\n",
            "Lr: 8.264033809969977e-06  | Train Loss: 0.04320834204554558\n",
            "Lr: 8.144956737315035e-06  | Train Loss: 0.0807182714343071\n",
            "Lr: 8.026423227704894e-06  | Train Loss: 0.06514770537614822\n",
            "Lr: 7.908442680122598e-06  | Train Loss: 0.10062055289745331\n",
            "Lr: 7.791024449704682e-06  | Train Loss: 0.06534679234027863\n",
            "Lr: 7.67417784699937e-06  | Train Loss: 0.0543753020465374\n",
            "Lr: 7.5579121372283e-06  | Train Loss: 0.09156277030706406\n",
            "Done.\n",
            "Epoch n°2: TRAIN LOSS 0.0763 | VAL LOSS 0.1031 | TOTAL TIME: 5807.56s.\n",
            "Running evaluation...Done. 712.21\n",
            "F1 score: 0.6772\n",
            "______________________________________________________________________\n",
            "Epoch : 3\n",
            "Lr: 7.488435377082877e-06  | Train Loss: 0.05719562992453575\n",
            "Running evaluation..."
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-3eb5e628927f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m train_BERT_model(model, dataset_tr, dataset_val, dataset_test, tokenizer,\n\u001b[0;32m----> 7\u001b[0;31m                  learning_rate, batch_size, num_epochs)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Cours/Kaggle/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'modelBERT.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-5d84552b241d>\u001b[0m in \u001b[0;36mtrain_BERT_model\u001b[0;34m(model, train, val, test, tokenizer, learning_rate, batch_size, num_epochs, warmup_steps, num_cycles, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m#and step > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mval_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-5d84552b241d>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, eval_dataset, tokenizer, eval_batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 'labels':         batch[3]}\n\u001b[1;32m     34\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#or None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0mtmp_eval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0meval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtmp_eval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m         )\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         )\n\u001b[1;32m    738\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     ):\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    312\u001b[0m     ):\n\u001b[1;32m    313\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1615\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY6o6NqScNiQ",
        "colab_type": "text"
      },
      "source": [
        "### Testing BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMGwUZWKTQhy",
        "colab_type": "code",
        "outputId": "081bfce0-39db-4cea-e223-8c55dc077e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.load_state_dict(th.load('epoch_1.pt'))\n",
        "model.eval()\n",
        "results = evaluate(model=model, eval_dataset=dataset_test, tokenizer=tokenizer)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running evaluation...Done. 711.63\n",
            "{'val_loss': 0.09551778247697096, 'val_acc': 0.9617419399294098, 'f1': 0.6590242238143977}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufuP_CXcdYlw",
        "colab_type": "code",
        "outputId": "407d3648-74c2-4f69-9159-2eafbab353a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.load_state_dict(th.load('epoch_2.pt'))\n",
        "model.eval()\n",
        "results = evaluate(model=model, eval_dataset=dataset_test, tokenizer=tokenizer)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running evaluation...Done. 709.14\n",
            "{'val_loss': 0.09709692623692368, 'val_acc': 0.9615275661687581, 'f1': 0.6772017729813067}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}